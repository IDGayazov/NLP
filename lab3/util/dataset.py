import json

import numpy as np
from sklearn.preprocessing import MultiLabelBinarizer

from util.decribe import get_dataset
from sklearn.model_selection import train_test_split
from collections import Counter

import pandas as pd

def make_dataset_for_classification():
    data = get_dataset()

    dataset = []
    for item in data:
        dataset.append({
            'title': item['title'],
            'text': item['text'],
            'sentiment': get_binary_class_from_category(item['rubric']),
            'category': item['rubric']
        })

    return dataset

def get_binary_class_from_category(category: str):
    if category in ('–Æ–≥–∞–ª—Ç—É', '–•”ô–≤–µ—Ñ-—Ö”ô—Ç”ô—Ä', '–ú–∞—Ö—Å—É—Å —Ö”ô—Ä–±–∏ –æ–ø–µ—Ä–∞—Ü–∏—è'):
        return 0
    else:
        return 1

def simple_train_val_test_split(data, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):
    """
    –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation/test —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Ç–æ–∫

    Args:
        data: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏
        train_size: –¥–æ–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (0.7 = 70%)
        val_size: –¥–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (0.15 = 15%)
        test_size: –¥–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (0.15 = 15%)
        random_state: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏

    Returns:
        —Å–ª–æ–≤–∞—Ä—å —Å train, val, test –Ω–∞–±–æ—Ä–∞–º–∏
    """

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—É–º–º–∞ –¥–æ–ª–µ–π = 1
    assert abs(train_size + val_size + test_size - 1.0) < 0.001, "–°—É–º–º–∞ –¥–æ–ª–µ–π –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 1.0"

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∫–∏ –¥–ª—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
    labels = [item['sentiment'] for item in data]

    print("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫ –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    label_counts = Counter(labels)
    for label, count in label_counts.items():
        print(f"   {label}: {count} ({count / len(data) * 100:.1f}%)")

    # 1. –°–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train –∏ temp (val + test)
    train_data, temp_data = train_test_split(
        data,
        test_size=val_size + test_size,
        stratify=labels,
        random_state=random_state
    )

    val_ratio = val_size / (val_size + test_size)

    val_data, test_data = train_test_split(
        temp_data,
        test_size=1 - val_ratio,
        stratify=[item['sentiment'] for item in temp_data],
        random_state=random_state
    )

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
    print("\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è:")
    for split_name, split_data in [("Train", train_data), ("Val", val_data), ("Test", test_data)]:
        split_labels = [item['sentiment'] for item in split_data]
        label_dist = Counter(split_labels)
        print(f"\n   {split_name} ({len(split_data)} samples):")
        for label in sorted(label_dist.keys()):
            count = label_dist[label]
            pct = count / len(split_data) * 100
            original_pct = label_counts[label] / len(data) * 100
            deviation = abs(pct - original_pct)
            print(f"      {label}: {count} ({pct:.1f}%) - –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {deviation:.1f}%")

    return {
        'train': train_data,
        'val': val_data,
        'test': test_data
    }

def train_val_test_split(data, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):
    """
    –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation/test —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Ç–æ–∫

    Args:
        data: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏
        train_size: –¥–æ–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (0.7 = 70%)
        val_size: –¥–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (0.15 = 15%)
        test_size: –¥–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (0.15 = 15%)
        random_state: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏

    Returns:
        —Å–ª–æ–≤–∞—Ä—å —Å train, val, test –Ω–∞–±–æ—Ä–∞–º–∏
    """

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—É–º–º–∞ –¥–æ–ª–µ–π = 1
    assert abs(train_size + val_size + test_size - 1.0) < 0.001, "–°—É–º–º–∞ –¥–æ–ª–µ–π –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 1.0"

    new_data = []
    for item in data:
        if item['category'] != '–Æ–≥–∞–ª—Ç—É':
            new_data.append(item)

    data = new_data

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∫–∏ –¥–ª—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
    labels = [item['category'] for item in data]

    print("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫ –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    label_counts = Counter(labels)
    for label, count in label_counts.items():
        print(f"   {label}: {count} ({count / len(data) * 100:.1f}%)")

    # 1. –°–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train –∏ temp (val + test)
    train_data, temp_data = train_test_split(
        data,
        test_size=val_size + test_size,
        stratify=labels,
        random_state=random_state
    )

    val_ratio = val_size / (val_size + test_size)

    val_data, test_data = train_test_split(
        temp_data,
        test_size=1 - val_ratio,
        stratify=[item['category'] for item in temp_data],
        random_state=random_state
    )

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
    print("\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è:")
    for split_name, split_data in [("Train", train_data), ("Val", val_data), ("Test", test_data)]:
        split_labels = [item['category'] for item in split_data]
        label_dist = Counter(split_labels)
        print(f"\n   {split_name} ({len(split_data)} samples):")
        for label in sorted(label_dist.keys()):
            count = label_dist[label]
            pct = count / len(split_data) * 100
            original_pct = label_counts[label] / len(data) * 100
            deviation = abs(pct - original_pct)
            print(f"      {label}: {count} ({pct:.1f}%) - –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {deviation:.1f}%")

    return {
        'train': train_data,
        'val': val_data,
        'test': test_data
    }



def save_splits(splits, filename_prefix):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–∞–π–ª—ã
    """
    for split_name, data in splits.items():
        filename = f"{filename_prefix}_{split_name}.jsonl"
        with open(filename, 'w', encoding='utf-8') as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        print(f"üíæ {split_name}: {len(data)} –∑–∞–ø–∏—Å–µ–π -> {filename}")


def create_multi_label_dataset(data, strategy='similarity', top_k=2):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ –æ–¥–Ω–æ–ª–∞–±–µ–ª—å–Ω–æ–≥–æ

    Args:
        data: –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        strategy: 'similarity' (–ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏)
                 'cooccurrence' (–ø–æ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–º—É –ø–æ—è–≤–ª–µ–Ω–∏—é)
                 'hierarchical' (–∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ)
        top_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫
    """
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity

    all_categories = sorted(list(set([item['category'] for item in data])))
    mlb = MultiLabelBinarizer(classes=all_categories)

    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    from collections import Counter
    cat_counter = Counter([item['category'] for item in data])
    print("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π:")
    for cat, count in cat_counter.most_common():
        print(f"  {cat}: {count} ({count / len(data) * 100:.1f}%)")

    if strategy == 'similarity':
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –±–ª–∏–∑–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤
        print("üîç –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏...")
        texts = [item['text'] for item in data]

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º SentenceTransformer –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        embeddings = model.encode(texts, show_progress_bar=True)

        # –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarity_matrix = cosine_similarity(embeddings)

        converted_data = []
        for i, item in enumerate(data):
            # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã
            similarities = similarity_matrix[i]
            similar_indices = np.argsort(similarities)[::-1][1:top_k + 1]  # –∏—Å–∫–ª—é—á–∞–µ–º —Å–µ–±—è

            # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç–∫–∏ –∏–∑ –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
            main_label = item['category']
            similar_labels = [data[idx]['category'] for idx in similar_indices]

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–µ—Ç–∫–∏ (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ)
            all_labels = list(set([main_label] + similar_labels))

            # –°–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
            binary_vector = mlb.fit_transform([all_labels])[0]

            converted_data.append({
                'text': item['text'],
                'labels': all_labels,
                'binary_labels': binary_vector.tolist(),
                'main_category': main_label,
                'similar_categories': similar_labels
            })

    elif strategy == 'cooccurrence':
        # –°–æ–∑–¥–∞–µ–º –ø—Å–µ–≤–¥–æ-—Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        print("üîç –°–æ–∑–¥–∞–Ω–∏–µ –ø—Å–µ–≤–¥–æ-—Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö –ø–æ—è–≤–ª–µ–Ω–∏–π...")

        # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è (–ø—Å–µ–≤–¥–æ)
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—Å—Ç–æ—è—â–∏–µ –¥–∞–Ω–Ω—ã–µ

        converted_data = []
        for item in data:
            main_label = item['category']

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            most_common_cats = [cat for cat, _ in cat_counter.most_common(top_k + 3)
                                if cat != main_label][:top_k]

            all_labels = [main_label] + most_common_cats[:top_k]
            binary_vector = mlb.fit_transform([all_labels])[0]

            converted_data.append({
                'text': item['text'],
                'labels': all_labels,
                'binary_labels': binary_vector.tolist(),
                'main_category': main_label
            })

    return converted_data, all_categories, mlb


def convert_to_multi_label_format(data, label_type='binary'):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç —Å –æ–¥–Ω–æ–π –º–µ—Ç–∫–æ–π –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

    Args:
        data: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π {'text': text, 'category': category}
        label_type: 'binary' (0/1) –∏–ª–∏ 'hierarchical' (–∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è)
    """
    all_categories = sorted(list(set([item['category'] for item in data])))
    num_classes = len(all_categories)

    print(f"üìä –ù–∞–π–¥–µ–Ω–æ {num_classes} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π:")
    for i, cat in enumerate(all_categories):
        print(f"  {i:3d}. {cat}")

    # –°–æ–∑–¥–∞–µ–º —ç–Ω–∫–æ–¥–µ—Ä
    mlb = MultiLabelBinarizer(classes=all_categories)

    converted_data = []
    for item in data:
        # –ö–∞–∂–¥–∞—è –∑–∞–ø–∏—Å—å –ø–æ–ª—É—á–∞–µ—Ç —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É –º–µ—Ç–∫—É
        # –î–ª—è multi-label –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –∏–∑ –æ–¥–Ω–æ–π –º–µ—Ç–∫–∏
        labels = [item['category']]

        # –°–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
        binary_vector = mlb.fit_transform([labels])[0]

        converted_item = {
            'text': item['text'],
            'labels': labels,  # –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏
            'binary_labels': binary_vector.tolist(),  # –±–∏–Ω–∞—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
            'category': item['category']  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        }
        converted_data.append(converted_item)

    return converted_data, all_categories, mlb

def main_binary():
    labeled_news = make_dataset_for_classification()

    splits = simple_train_val_test_split(
        labeled_news,
        train_size=0.7,
        val_size=0.15,
        test_size=0.15,
        random_state=42
    )

    save_splits(splits, "news_sentiment")

    print(f"\nüéØ –ò–¢–û–ì:")
    print(f"   Train: {len(splits['train'])} –∑–∞–ø–∏—Å–µ–π (70%)")
    print(f"   Val:   {len(splits['val'])} –∑–∞–ø–∏—Å–µ–π (15%)")
    print(f"   Test:  {len(splits['test'])} –∑–∞–ø–∏—Å–µ–π (15%)")
    print(f"   –í—Å–µ–≥–æ: {len(splits['train']) + len(splits['val']) + len(splits['test'])} –∑–∞–ø–∏—Å–µ–π")

def main_category():
    labeled_news = make_dataset_for_classification()

    splits = train_val_test_split(
        labeled_news,
        train_size=0.7,
        val_size=0.15,
        test_size=0.15,
        random_state=42
    )

    save_splits(splits, "news_category")

    print(f"\nüéØ –ò–¢–û–ì:")
    print(f"   Train: {len(splits['train'])} –∑–∞–ø–∏—Å–µ–π (70%)")
    print(f"   Val:   {len(splits['val'])} –∑–∞–ø–∏—Å–µ–π (15%)")
    print(f"   Test:  {len(splits['test'])} –∑–∞–ø–∏—Å–µ–π (15%)")
    print(f"   –í—Å–µ–≥–æ: {len(splits['train']) + len(splits['val']) + len(splits['test'])} –∑–∞–ø–∏—Å–µ–π")


def split_multi_label_data(conv_data, test_size=0.2, val_size=0.1, random_state=42):
    """
    –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

    –í–∞–∂–Ω–æ: –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫ –≤–æ –≤—Å–µ—Ö –≤—ã–±–æ—Ä–∫–∞—Ö!
    """

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏
    texts = [item['text'] for item in conv_data]
    binary_labels = np.array([item['binary_labels'] for item in conv_data])

    print(f"üìä –í—Å–µ–≥–æ –¥–∞–Ω–Ω—ã—Ö: {len(texts)}")
    print(f"üìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–µ—Ç–æ–∫: {binary_labels.shape}")

    # 1. –°–Ω–∞—á–∞–ª–∞ –æ—Ç–¥–µ–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É (20%)
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—É—é —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é –¥–ª—è –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    from sklearn.model_selection import train_test_split

    # –î–ª—è –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º train_test_split –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑
    # –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã

    # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ: —Ä–∞–∑–¥–µ–ª—è–µ–º —Å–ª—É—á–∞–π–Ω–æ, –Ω–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Ç–æ–∫
    # –î–ª—è production –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å IterativeStratification

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: train_temp (80%) –∏ test (20%)
    X_temp, X_test, y_temp, y_test = train_test_split(
        texts, binary_labels,
        test_size=test_size,
        random_state=random_state,
        # –î–ª—è –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω—ã—Ö –ø—Ä–æ—Å—Ç–æ–π random —á–∞—Å—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç
        stratify=None  # –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π stratify –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è multi-label
    )

    # 2. –ó–∞—Ç–µ–º —Ä–∞–∑–¥–µ–ª—è–µ–º train_temp –Ω–∞ train (70%) –∏ val (10%)
    # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    val_relative_size = val_size / (1 - test_size)

    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp,
        test_size=val_relative_size,
        random_state=random_state,
        stratify=None
    )

    print(f"\n‚úÖ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ:")
    print(f"   Train: {len(X_train)} ({len(X_train) / len(texts) * 100:.1f}%)")
    print(f"   Val:   {len(X_val)} ({len(X_val) / len(texts) * 100:.1f}%)")
    print(f"   Test:  {len(X_test)} ({len(X_test) / len(texts) * 100:.1f}%)")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫
    print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫ (—Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞ —Ç–µ–∫—Å—Ç):")
    print(f"   Train: {y_train.sum(axis=1).mean():.2f}")
    print(f"   Val:   {y_val.sum(axis=1).mean():.2f}")
    print(f"   Test:  {y_test.sum(axis=1).mean():.2f}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ –∫–ª–∞—Å—Å–æ–≤
    print(f"\nüìä –ü–æ–∫—Ä—ã—Ç–∏–µ –∫–ª–∞—Å—Å–æ–≤ (% —Ç–µ–∫—Å—Ç–æ–≤ —Å –¥–∞–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π):")
    num_classes = binary_labels.shape[1]

    for i in range(min(5, num_classes)):  # –ø–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–µ 5 –∫–ª–∞—Å—Å–æ–≤
        train_coverage = (y_train[:, i].sum() / len(y_train)) * 100
        val_coverage = (y_val[:, i].sum() / len(y_val)) * 100
        test_coverage = (y_test[:, i].sum() / len(y_test)) * 100

        print(f"   –ö–ª–∞—Å—Å {i}: Train={train_coverage:.1f}%, "
              f"Val={val_coverage:.1f}%, Test={test_coverage:.1f}%")

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ —É–¥–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
    train_data = [
        {'text': text, 'binary_labels': labels.tolist()}
        for text, labels in zip(X_train, y_train)
    ]

    val_data = [
        {'text': text, 'binary_labels': labels.tolist()}
        for text, labels in zip(X_val, y_val)
    ]

    test_data = [
        {'text': text, 'binary_labels': labels.tolist()}
        for text, labels in zip(X_test, y_test)
    ]

    # splits = {
    #     'train_data': train_data,
    #     'test_data': test_data,
    #     'val_data': val_data
    # }
    # save_splits(splits, "news_multilabel")

    return train_data, val_data, test_data, binary_labels.shape[1]

if __name__ == "__main__":
    # main_binary()
    # main_category()
    data = make_dataset_for_classification()

    conv_data, _, _ = create_multi_label_dataset(data, strategy='cooccurrence')

    train_data, val_data, test_data, b_labels = split_multi_label_data(conv_data)

    print(b_labels)

    # multi_label_data, categories, mlb = convert_to_multi_label_format(data)
    #
    # print(f"\nüìä –ü—Ä–∏–º–µ—Ä –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω–æ–π –∑–∞–ø–∏—Å–∏:")
    # print(f"–¢–µ–∫—Å—Ç: {multi_label_data[0]['text'][:50]}...")
    # print(f"–ò—Å—Ö–æ–¥–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {multi_label_data[0]['category']}")
    # print(f"–ë–∏–Ω–∞—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä ({len(categories)} –∫–ª–∞—Å—Å–æ–≤): {multi_label_data[0]['binary_labels']}")
