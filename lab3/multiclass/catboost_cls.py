from catboost import CatBoostClassifier, Pool
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import numpy as np
import joblib
import json


class CatBoostMultiClassClassifier:
    """
    –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ CatBoost
    """

    def __init__(self, iterations=1000, learning_rate=0.1, depth=6,
                 l2_leaf_reg=3, random_state=42, verbose=100):
        """
        Args:
            iterations: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π
            learning_rate: —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            depth: –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤
            l2_leaf_reg: L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
            random_state: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
            verbose: –≤—ã–≤–æ–¥ –ª–æ–≥–æ–≤
        """
        self.vectorizer = TfidfVectorizer(
            max_features=10000,
            min_df=2,
            max_df=0.9,
            ngram_range=(1, 2),
            stop_words=None
        )

        # –î–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫
        self.label_encoder = LabelEncoder()

        # CatBoost –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        self.model = CatBoostClassifier(
            iterations=iterations,
            learning_rate=learning_rate,
            depth=depth,
            l2_leaf_reg=l2_leaf_reg,
            random_seed=random_state,
            verbose=verbose,
            loss_function='MultiClass',  # –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
            eval_metric='Accuracy',
            early_stopping_rounds=50,
            use_best_model=True
        )

        self.is_trained = False
        self.class_names = None
        self.n_classes = None

    def prepare_data(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏
        """
        texts = [item['text'] for item in data]
        labels = [item['category'] for item in data]
        return texts, labels

    def train(self, train_data, val_data=None):
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ CatBoost –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        print("üéØ –û–ë–£–ß–ï–ù–ò–ï CATBOOST (–º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π)...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train = self.prepare_data(train_data)

        # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
        y_train_encoded = self.label_encoder.fit_transform(y_train)
        self.class_names = self.label_encoder.classes_
        self.n_classes = len(self.class_names)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
        unique_labels = set(y_train)
        print(f"üìä –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {self.n_classes} –∫–ª–∞—Å—Å–æ–≤: {list(self.class_names)}")

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
        print("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        X_train_vec = self.vectorizer.fit_transform(X_train)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–ª–æ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è CatBoost
        X_train_dense = X_train_vec.toarray()

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_dense.shape}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {self.n_classes}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(y_train)}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π: {self.model.get_param('iterations')}")
        print(f"   –ì–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤: {self.model.get_param('depth')}")
        print(f"   Learning rate: {self.model.get_param('learning_rate')}")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è CatBoost
        if val_data:
            X_val, y_val = self.prepare_data(val_data)
            X_val_vec = self.vectorizer.transform(X_val)
            X_val_dense = X_val_vec.toarray()
            y_val_encoded = self.label_encoder.transform(y_val)

            train_pool = Pool(X_train_dense, label=y_train_encoded)
            val_pool = Pool(X_val_dense, label=y_val_encoded)

            print("ü§ñ –û–±—É—á–µ–Ω–∏–µ CatBoost —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π...")
            self.model.fit(
                train_pool,
                eval_set=val_pool,
                plot=False,
                verbose=self.model.get_param('verbose')
            )
        else:
            train_pool = Pool(X_train_dense, label=y_train_encoded)
            print("ü§ñ –û–±—É—á–µ–Ω–∏–µ CatBoost...")
            self.model.fit(train_pool)

        self.is_trained = True

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        train_pred = self.model.predict(X_train_dense)
        train_pred_decoded = self.label_encoder.inverse_transform(train_pred.flatten())
        train_accuracy = accuracy_score(y_train, train_pred_decoded)
        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train: {train_accuracy:.3f}")

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        if val_data:
            val_pred = self.model.predict(X_val_dense)
            val_pred_decoded = self.label_encoder.inverse_transform(val_pred.flatten())
            val_accuracy = accuracy_score(y_val, val_pred_decoded)
            print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ val: {val_accuracy:.3f}")

        # –ü–æ–∫–∞–∂–µ–º –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        self._show_important_features(top_n=20)

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        self._show_model_info()

    def predict(self, texts):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)
        X_dense = X_vec.toarray()

        predictions_encoded = self.model.predict(X_dense)
        predictions = self.label_encoder.inverse_transform(predictions_encoded.flatten())
        probabilities = self.model.predict_proba(X_dense)

        return predictions, probabilities

    def predict_single(self, text):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        predictions, probabilities = self.predict([text])
        pred = predictions[0]
        pred_encoded = self.label_encoder.transform([pred])[0]
        prob = probabilities[0]

        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤
        class_probabilities = {}
        for class_name, prob_value in zip(self.class_names, prob):
            class_probabilities[class_name] = prob_value

        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã
        sorted_probs = sorted(class_probabilities.items(), key=lambda x: x[1], reverse=True)
        top_3 = sorted_probs[:3]

        return {
            'prediction': pred,
            'confidence': float(prob[pred_encoded]),
            'probabilities': class_probabilities,
            'top_3_predictions': top_3,
            'model_type': 'CatBoost',
            'n_classes': self.n_classes
        }

    def predict_proba(self, texts):
        """
        –¢–æ–ª—å–∫–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)
        X_dense = X_vec.toarray()

        return self.model.predict_proba(X_dense)

    def evaluate(self, test_data):
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        X_test, y_test = self.prepare_data(test_data)
        X_test_vec = self.vectorizer.transform(X_test)
        X_test_dense = X_test_vec.toarray()

        # –ö–æ–¥–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç–∫–∏
        y_test_encoded = self.label_encoder.transform(y_test)

        y_pred_encoded = self.model.predict(X_test_dense)
        y_pred = self.label_encoder.inverse_transform(y_pred_encoded.flatten())

        accuracy = accuracy_score(y_test_encoded, y_pred_encoded)

        print("\nüìä –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(classification_report(y_test, y_pred, digits=4))

        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        print("\nüìà –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö:")
        cm = confusion_matrix(y_test_encoded, y_pred_encoded)
        self._print_confusion_matrix(cm, self.class_names)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        print(f"\nüìä –û–ë–©–ò–ï –ú–ï–¢–†–ò–ö–ò:")
        print(f"   Accuracy: {accuracy:.4f}")

        # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        from sklearn.metrics import precision_score, recall_score, f1_score
        precision_macro = precision_score(y_test_encoded, y_pred_encoded, average='macro')
        recall_macro = recall_score(y_test_encoded, y_pred_encoded, average='macro')
        f1_macro = f1_score(y_test_encoded, y_pred_encoded, average='macro')

        precision_weighted = precision_score(y_test_encoded, y_pred_encoded, average='weighted')
        recall_weighted = recall_score(y_test_encoded, y_pred_encoded, average='weighted')
        f1_weighted = f1_score(y_test_encoded, y_pred_encoded, average='weighted')

        print(f"   Precision (macro): {precision_macro:.4f}")
        print(f"   Recall (macro): {recall_macro:.4f}")
        print(f"   F1-Score (macro): {f1_macro:.4f}")
        print(f"   Precision (weighted): {precision_weighted:.4f}")
        print(f"   Recall (weighted): {recall_weighted:.4f}")
        print(f"   F1-Score (weighted): {f1_weighted:.4f}")

        return accuracy

    def _print_confusion_matrix(self, cm, class_names):
        """
        –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
        """
        n_classes = len(class_names)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã
        if cm.shape[0] != n_classes:
            print(f"‚ö†Ô∏è  –†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã ({cm.shape[0]}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–ª–∞—Å—Å–æ–≤ ({n_classes})")
            # –û–±—Ä–µ–∑–∞–µ–º –∏–ª–∏ –¥–æ–ø–æ–ª–Ω—è–µ–º –∏–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–æ —Ä–∞–∑–º–µ—Ä–∞ –º–∞—Ç—Ä–∏—Ü—ã
            if cm.shape[0] < n_classes:
                class_names = class_names[:cm.shape[0]]
            n_classes = cm.shape[0]

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        header = " " * 15 + "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ ‚Üí"
        print(header)

        # –ò–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        pred_header = " " * 10
        for name in class_names[:n_classes]:
            pred_header += f"{str(name)[:8]:^8} "
        print(pred_header)

        # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
        separator = " " * 10 + "‚îÄ" * (n_classes * 9)
        print(separator)

        # –°—Ç—Ä–æ–∫–∏ –º–∞—Ç—Ä–∏—Ü—ã
        for i, true_name in enumerate(class_names[:n_classes]):
            row = f"–ò—Å—Ç–∏–Ω–Ω–æ {str(true_name)[:8]:<8}‚îÇ"
            for j in range(n_classes):
                row += f"{cm[i][j]:^8} "
            print(row)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        diagonal = cm.diagonal()
        total = cm.sum()
        correct = diagonal.sum()

        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: {correct}/{total} ({correct / total:.1%})")

        # –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Å–∞–º
        print(f"\nüìä Accuracy –ø–æ –∫–ª–∞—Å—Å–∞–º:")
        for i, class_name in enumerate(class_names[:n_classes]):
            class_total = cm[i, :].sum()
            if class_total > 0:
                class_correct = cm[i, i]
                print(f"   {class_name}: {class_correct}/{class_total} ({class_correct / class_total:.1%})")

    def _show_important_features(self, top_n=20):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        """
        try:
            feature_importances = self.model.get_feature_importance()
            feature_names = self.vectorizer.get_feature_names_out()

            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç
            if len(feature_importances) != len(feature_names):
                print(
                    f"‚ö†Ô∏è  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–æ—Å—Ç–µ–π ({len(feature_importances)}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ({len(feature_names)})")
                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ n –≤–∞–∂–Ω–æ—Å—Ç–µ–π
                min_len = min(len(feature_importances), len(feature_names))
                feature_importances = feature_importances[:min_len]
                feature_names = feature_names[:min_len]

            print(f"\nüîç –¢–û–ü-{top_n} –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í (CatBoost):")

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            indices = np.argsort(feature_importances)[::-1]

            print(f"\n   –°–ê–ú–´–ï –í–ê–ñ–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò:")
            for i in range(min(top_n, len(indices))):
                idx = indices[i]
                print(f"      {feature_names[idx]}: {feature_importances[idx]:.4f}")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏
            total_importance = np.sum(feature_importances)
            top_n_importance = np.sum(feature_importances[indices[:top_n]])
            if total_importance > 0:
                print(
                    f"\n   üìä –¢–æ–ø-{top_n} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–±—ä—è—Å–Ω—è—é—Ç {top_n_importance / total_importance * 100:.1f}% –æ–±—â–µ–π –≤–∞–∂–Ω–æ—Å—Ç–∏")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")

    def _show_model_info(self):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        print(f"\nüìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û CATBOOST –ú–û–î–ï–õ–ò:")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤: {self.model.tree_count_}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {self.n_classes}")

        # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
        if hasattr(self.model, 'get_evals_result'):
            try:
                evals_result = self.model.get_evals_result()
                if evals_result and 'learn' in evals_result:
                    learn_accuracy = evals_result['learn']['Accuracy'][-1] if 'Accuracy' in evals_result['learn'] else 0
                    print(f"   Final train accuracy: {learn_accuracy:.4f}")

                if evals_result and 'validation' in evals_result:
                    val_accuracy = evals_result['validation']['Accuracy'][-1] if 'Accuracy' in evals_result[
                        'validation'] else 0
                    print(f"   Final validation accuracy: {val_accuracy:.4f}")
            except:
                pass

        # Best iteration
        try:
            best_iteration = self.model.get_best_iteration()
            print(f"   Best iteration: {best_iteration}")
        except:
            pass

    def get_feature_importance_df(self, top_n=50):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame —Å –≤–∞–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        try:
            feature_importances = self.model.get_feature_importance()
            feature_names = self.vectorizer.get_feature_names_out()

            # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
            min_len = min(len(feature_importances), len(feature_names))
            feature_importances = feature_importances[:min_len]
            feature_names = feature_names[:min_len]

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            indices = np.argsort(feature_importances)[::-1]

            importance_data = []
            for i in range(min(top_n, len(indices))):
                idx = indices[i]
                importance_data.append({
                    'feature': feature_names[idx],
                    'importance': feature_importances[idx],
                    'rank': i + 1
                })

            return importance_data
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ DataFrame –≤–∞–∂–Ω–æ—Å—Ç–∏: {e}")
            return None

    def get_class_distribution(self, data):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö
        """
        _, labels = self.prepare_data(data)
        unique, counts = np.unique(labels, return_counts=True)
        return dict(zip(unique, counts))

    def save_model(self, filename):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        joblib.dump({
            'model': self.model,
            'vectorizer': self.vectorizer,
            'label_encoder': self.label_encoder,
            'class_names': self.class_names,
            'n_classes': self.n_classes
        }, filename)
        print(f"üíæ –ú–æ–¥–µ–ª—å CatBoost —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")

        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ native CatBoost format
        cb_filename = filename.replace('.pkl', '.cbm')
        self.model.save_model(cb_filename)
        print(f"üíæ –ú–æ–¥–µ–ª—å CatBoost —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (native): {cb_filename}")

    def load_model(self, filename):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        loaded = joblib.load(filename)
        self.model = loaded['model']
        self.vectorizer = loaded['vectorizer']
        self.label_encoder = loaded['label_encoder']
        self.class_names = loaded.get('class_names', [])
        self.n_classes = loaded.get('n_classes', 0)
        self.is_trained = True
        print(f"üì• –ú–æ–¥–µ–ª—å CatBoost –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filename}")


# –°–ü–ï–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† –î–õ–Ø –¢–û–ù–ê–õ–¨–ù–û–°–¢–ò (3 –ö–õ–ê–°–°–ê)
class SentimentCatBoostClassifier(CatBoostMultiClassClassifier):
    """
    –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π CatBoost –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (–Ω–µ–≥–∞—Ç–∏–≤, –Ω–µ–π—Ç—Ä–∞–ª, –ø–æ–∑–∏—Ç–∏–≤)
    """

    def __init__(self, iterations=1000, learning_rate=0.1, depth=6,
                 l2_leaf_reg=3, random_state=42, verbose=100):
        super().__init__(
            iterations=iterations,
            learning_rate=learning_rate,
            depth=depth,
            l2_leaf_reg=l2_leaf_reg,
            random_state=random_state,
            verbose=verbose
        )

    def predict_sentiment(self, text):
        """
        –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        """
        result = super().predict_single(text)

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –¥–ª—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        sentiment_mapping = {
            'negative': '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π',
            'neutral': '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π',
            'positive': '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π',
            'neg': '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π',
            'neu': '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π',
            'pos': '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π',
            '0': '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π',
            '1': '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π',
            '2': '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π'
        }

        prediction = result['prediction']
        sentiment = sentiment_mapping.get(str(prediction).lower(), prediction)

        result['sentiment'] = sentiment
        result['sentiment_confidence'] = result['confidence']

        return result


# –°–†–ê–í–ù–ï–ù–ò–ï –†–ê–ó–ù–´–• –ü–ê–†–ê–ú–ï–¢–†–û–í CATBOOST
def compare_catboost_parameters_multiclass(train_data, val_data):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ CatBoost –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    """
    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –ü–ê–†–ê–ú–ï–¢–†–û–í CATBOOST (–º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π)")
    print("=" * 50)

    models = {}

    # 1. –†–∞–∑–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤
    for depth in [4, 6, 8]:
        print(f"\n1. CatBoost —Å depth={depth}:")
        model = CatBoostMultiClassClassifier(depth=depth, iterations=300, verbose=0)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º try-except –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
        try:
            model.train(train_data, val_data)
            models[f'CB_depth_{depth}'] = model
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
            continue

    # 2. –†–∞–∑–Ω—ã–π learning rate
    for lr in [0.05, 0.1, 0.2]:
        print(f"\n2. CatBoost —Å learning_rate={lr}:")
        model = CatBoostMultiClassClassifier(learning_rate=lr, iterations=300, verbose=0)
        try:
            model.train(train_data, val_data)
            models[f'CB_lr_{lr}'] = model
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
            continue

    return models


# –ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í –° –ì–†–£–ü–ü–ò–†–û–í–ö–û–ô
def analyze_catboost_features_multiclass(model, top_n=30):
    """
    –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è CatBoost (–º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π)
    """
    importance_data = model.get_feature_importance_df(top_n=top_n)

    if importance_data:
        print(f"\nüìà –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í CATBOOST (–¢–æ–ø-{top_n}):")
        print("=" * 60)

        for i, item in enumerate(importance_data[:top_n]):
            print(f"{i + 1:2d}. {item['feature']:20s} : {item['importance']:.4f}")

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        positive_words = []
        negative_words = []
        neutral_words = []

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π (–∞–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –ø–æ–¥ –≤–∞—à–∏ –∫–ª–∞—Å—Å—ã)
        if hasattr(model, 'class_names'):
            if 'positive' in model.class_names or 'pos' in model.class_names:
                positive_keywords = ['—Ö–æ—Ä–æ—à', '–æ—Ç–ª–∏—á', '–ø—Ä–µ–∫—Ä–∞—Å', '–¥–æ–≤–æ–ª', '—Ä–µ–∫–æ–º–µ–Ω–¥', '–≤–µ–ª–∏–∫–æ–ª', '–∑–∞–º–µ—á–∞—Ç']
            else:
                positive_keywords = []

            if 'negative' in model.class_names or 'neg' in model.class_names:
                negative_keywords = ['–ø–ª–æ—Ö', '—É–∂–∞—Å', '—Ä–∞–∑–æ—á–∞—Ä', '–Ω–µ–¥–æ–≤–æ–ª', '–ø—Ä–æ–±–ª–µ–º', '–∫–æ—à–º–∞—Ä', '–Ω–µ–∫–∞—á–µ—Å—Ç–≤']
            else:
                negative_keywords = []

        for item in importance_data:
            feature = item['feature']
            if any(keyword in feature for keyword in positive_keywords):
                positive_words.append(item)
            elif any(keyword in feature for keyword in negative_keywords):
                negative_words.append(item)
            else:
                neutral_words.append(item)

        if positive_words:
            print(f"\nüéØ –ü–û–õ–û–ñ–ò–¢–ï–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò:")
            for item in positive_words[:10]:
                print(f"   {item['feature']}: {item['importance']:.4f}")

        if negative_words:
            print(f"\nüéØ –û–¢–†–ò–¶–ê–¢–ï–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò:")
            for item in negative_words[:10]:
                print(f"   {item['feature']}: {item['importance']:.4f}")

        if neutral_words:
            print(f"\nüéØ –ù–ï–ô–¢–†–ê–õ–¨–ù–´–ï/–°–ú–ï–®–ê–ù–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò:")
            for item in neutral_words[:10]:
                print(f"   {item['feature']}: {item['importance']:.4f}")


# –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø
def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è CatBoost –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    """

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    def load_jsonl(filepath):
        with open(filepath, 'r', encoding='utf-8') as f:
            return [json.loads(line) for line in f]

    train_data = load_jsonl('../util/news_category_train.jsonl')
    test_data = load_jsonl('../util/news_category_test.jsonl')

    print(f"üìä –û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(train_data)}")
    print(f"üìä –¢–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(test_data)}")

    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val
    np.random.seed(42)
    indices = np.random.permutation(len(train_data))
    split_idx = int(0.8 * len(train_data))

    train_indices = indices[:split_idx]
    val_indices = indices[split_idx:]

    train_subset = [train_data[i] for i in train_indices]
    val_subset = [train_data[i] for i in val_indices]

    print(f"üìä Train: {len(train_subset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"üìä Val: {len(val_subset)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º CatBoost
    print("\n" + "=" * 50)
    print("üéØ –û–ë–£–ß–ï–ù–ò–ï CATBOOST (–º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π)")
    print("=" * 50)

    cb_classifier = CatBoostMultiClassClassifier(
        iterations=500,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        learning_rate=0.1,
        depth=6,
        verbose=100
    )

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    print("\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í –í –û–ë–£–ß–ê–Æ–©–ò–• –î–ê–ù–ù–´–•:")
    class_dist = cb_classifier.get_class_distribution(train_subset)
    for class_name, count in class_dist.items():
        print(f"   {class_name}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤ ({count / len(train_subset):.1%})")

    cb_classifier.train(train_subset, val_subset)

    # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    analyze_catboost_features_multiclass(cb_classifier, top_n=25)

    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\nüß™ –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•...")
    test_accuracy = cb_classifier.evaluate(test_data)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    cb_classifier.save_model("catboost_multiclass_model.pkl")

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö)
    print("\n" + "=" * 50)
    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –ü–ê–†–ê–ú–ï–¢–†–û–í")
    print("=" * 50)

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    small_train = train_subset[:60]
    small_val = val_subset[:15]

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
    if len(small_train) >= 10 and len(small_val) >= 5:
        models = compare_catboost_parameters_multiclass(small_train, small_val)
    else:
        print("‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")

    return cb_classifier


if __name__ == "__main__":
    print("üöÄ –ó–ê–ü–£–°–ö –ú–ù–û–ì–û–ö–õ–ê–°–°–û–í–û–ì–û CATBOOST –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê")
    print("=" * 80)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–∏–º–µ—Ä
    classifier = main()