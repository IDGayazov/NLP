from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import RandomizedSearchCV, cross_val_score
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import numpy as np
import joblib
from scipy.stats import randint, uniform
import warnings
import json

warnings.filterwarnings('ignore')


class SimpleAutoMLMultiClassClassifier:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π AutoML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    """

    def __init__(self, max_training_time=300, n_iter=50, random_state=42):
        """
        Args:
            max_training_time: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
            n_iter: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            random_state: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        """
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            min_df=2,
            max_df=0.8,
            ngram_range=(1, 2),
            stop_words=None
        )

        # –î–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫
        self.label_encoder = LabelEncoder()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
        self.models = {
            'logistic': {
                'model': LogisticRegression(random_state=random_state, multi_class='ovr'),
                'params': {
                    'C': uniform(0.001, 100),
                    'penalty': ['l1', 'l2', 'elasticnet'],
                    'solver': ['liblinear', 'saga'],
                    'max_iter': [1000, 2000]
                }
            },
            'svm': {
                'model': SVC(random_state=random_state, probability=True, decision_function_shape='ovr'),
                'params': {
                    'C': uniform(0.1, 10),
                    'kernel': ['linear', 'rbf', 'poly'],
                    'gamma': ['scale', 'auto'] + list(uniform(0.001, 0.1).rvs(5))
                }
            },
            'random_forest': {
                'model': RandomForestClassifier(random_state=random_state),
                'params': {
                    'n_estimators': randint(50, 300),
                    'max_depth': [None, 10, 20, 30],
                    'min_samples_split': randint(2, 20),
                    'min_samples_leaf': randint(1, 10),
                    'max_features': ['sqrt', 'log2', None]
                }
            },
            'naive_bayes': {
                'model': MultinomialNB(),
                'params': {
                    'alpha': uniform(0.001, 2.0)
                }
            },
            'gradient_boosting': {
                'model': GradientBoostingClassifier(random_state=random_state),
                'params': {
                    'n_estimators': randint(50, 200),
                    'learning_rate': uniform(0.01, 0.3),
                    'max_depth': randint(3, 10),
                    'min_samples_split': randint(2, 20)
                }
            }
        }

        self.max_training_time = max_training_time
        self.n_iter = n_iter
        self.random_state = random_state
        self.is_trained = False
        self.best_model = None
        self.best_model_name = None
        self.best_score = 0
        self.class_names = None
        self.n_classes = None

        print(f"üöÄ Multi-class AutoML –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {max_training_time} —Å–µ–∫")
        print(f"   –ò—Ç–µ—Ä–∞—Ü–∏–π –ø–æ–∏—Å–∫–∞: {n_iter}")
        print(f"   –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {list(self.models.keys())}")

    def prepare_data(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏
        """
        texts = [item['text'] for item in data]
        labels = [item['category'] for item in data]
        return texts, labels

    def train(self, train_data, val_data=None):
        """
        –û–±—É—á–µ–Ω–∏–µ AutoML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        print("üéØ –ê–í–¢–û–ú–ê–¢–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–û–î–ë–û–† –ú–û–î–ï–õ–ï–ô (–º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π)...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train = self.prepare_data(train_data)

        # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
        y_train_encoded = self.label_encoder.fit_transform(y_train)
        self.class_names = self.label_encoder.classes_
        self.n_classes = len(self.class_names)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
        unique_labels = set(y_train)
        print(f"üìä –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {self.n_classes} –∫–ª–∞—Å—Å–æ–≤: {list(self.class_names)}")

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
        print("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        X_train_vec = self.vectorizer.fit_transform(X_train)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_vec.shape}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {self.n_classes}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(y_train)}")

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –º–æ–¥–µ–ª–µ–π
        print("\nü§ñ –ó–ê–ü–£–°–ö –°–õ–£–ß–ê–ô–ù–û–ì–û –ü–û–ò–°–ö–ê –ü–û –ú–û–î–ï–õ–Ø–ú...")

        best_models = {}

        for model_name, model_config in self.models.items():
            print(f"   üîç –ü–æ–∏—Å–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è {model_name}...")

            try:
                # –°–ª—É—á–∞–π–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
                search = RandomizedSearchCV(
                    model_config['model'],
                    model_config['params'],
                    n_iter=self.n_iter // len(self.models),
                    cv=3,
                    scoring='accuracy',
                    random_state=self.random_state,
                    n_jobs=-1,
                    verbose=0
                )

                search.fit(X_train_vec, y_train_encoded)

                best_models[model_name] = {
                    'model': search.best_estimator_,
                    'score': search.best_score_,
                    'params': search.best_params_
                }

                print(f"      ‚úÖ –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {search.best_score_:.3f}")

            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–ª—è {model_name}: {e}")
                continue

        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if best_models:
            self.best_model_name = max(best_models.keys(), key=lambda x: best_models[x]['score'])
            self.best_model = best_models[self.best_model_name]['model']
            self.best_score = best_models[self.best_model_name]['score']
            self.best_params = best_models[self.best_model_name]['params']

            print(f"\nüèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {self.best_model_name}")
            print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {self.best_score:.3f}")
            print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {self.best_params}")

            self.is_trained = True

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
            self._show_model_comparison(best_models)

            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
            if val_data:
                val_accuracy = self.evaluate(val_data)
                print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ val: {val_accuracy:.3f}")
        else:
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å!")

    def _show_model_comparison(self, best_models):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        print(f"\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô:")
        print("-" * 50)

        for model_name, results in sorted(best_models.items(),
                                          key=lambda x: x[1]['score'], reverse=True):
            score = results['score']
            print(f"   {model_name:<15}: {score:.3f}")

    def predict(self, texts):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)
        predictions_encoded = self.best_model.predict(X_vec)

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = self.label_encoder.inverse_transform(predictions_encoded)
        probabilities = self.best_model.predict_proba(X_vec)

        return predictions, probabilities

    def predict_single(self, text):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        predictions, probabilities = self.predict([text])
        pred = predictions[0]
        pred_encoded = self.label_encoder.transform([pred])[0]
        prob = probabilities[0]

        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤
        class_probabilities = {}
        for class_name, prob_value in zip(self.class_names, prob):
            class_probabilities[class_name] = prob_value

        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã
        sorted_probs = sorted(class_probabilities.items(), key=lambda x: x[1], reverse=True)
        top_3 = sorted_probs[:3]

        return {
            'prediction': pred,
            'confidence': float(prob[pred_encoded]),
            'probabilities': class_probabilities,
            'top_3_predictions': top_3,
            'model_type': type(self.best_model).__name__,
            'model_name': self.best_model_name
        }

    def evaluate(self, test_data):
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        X_test, y_test = self.prepare_data(test_data)
        X_test_vec = self.vectorizer.transform(X_test)

        # –ö–æ–¥–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        y_test_encoded = self.label_encoder.transform(y_test)

        y_pred_encoded = self.best_model.predict(X_test_vec)
        y_pred = self.label_encoder.inverse_transform(y_pred_encoded)

        accuracy = accuracy_score(y_test_encoded, y_pred_encoded)

        print("\nüìä –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ (–ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è):")
        print(classification_report(y_test, y_pred, digits=4))

        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        print("\nüìà –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö:")
        cm = confusion_matrix(y_test_encoded, y_pred_encoded)

        # –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
        self._print_confusion_matrix(cm, self.class_names)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        print(f"\nüìä –û–ë–©–ò–ï –ú–ï–¢–†–ò–ö–ò:")
        print(f"   Accuracy: {accuracy:.4f}")

        # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        from sklearn.metrics import precision_score, recall_score, f1_score
        precision_macro = precision_score(y_test_encoded, y_pred_encoded, average='macro')
        recall_macro = recall_score(y_test_encoded, y_pred_encoded, average='macro')
        f1_macro = f1_score(y_test_encoded, y_pred_encoded, average='macro')

        precision_weighted = precision_score(y_test_encoded, y_pred_encoded, average='weighted')
        recall_weighted = recall_score(y_test_encoded, y_pred_encoded, average='weighted')
        f1_weighted = f1_score(y_test_encoded, y_pred_encoded, average='weighted')

        print(f"   Precision (macro): {precision_macro:.4f}")
        print(f"   Recall (macro): {recall_macro:.4f}")
        print(f"   F1-Score (macro): {f1_macro:.4f}")
        print(f"   Precision (weighted): {precision_weighted:.4f}")
        print(f"   Recall (weighted): {recall_weighted:.4f}")
        print(f"   F1-Score (weighted): {f1_weighted:.4f}")

        return accuracy

    def _print_confusion_matrix(self, cm, class_names):
        """
        –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
        """
        n_classes = len(class_names)

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        header = " " * 15 + "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ ‚Üí"
        print(header)

        # –ò–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        pred_header = " " * 10
        for name in class_names:
            pred_header += f"{name[:8]:^8} "
        print(pred_header)

        # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
        separator = " " * 10 + "‚îÄ" * (n_classes * 9)
        print(separator)

        # –°—Ç—Ä–æ–∫–∏ –º–∞—Ç—Ä–∏—Ü—ã
        for i, true_name in enumerate(class_names):
            row = f"–ò—Å—Ç–∏–Ω–Ω–æ {true_name[:8]:<8}‚îÇ"
            for j in range(n_classes):
                row += f"{cm[i][j]:^8} "
            print(row)

        # –í—ã—á–∏—Å–ª—è–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è)
        diagonal = cm.diagonal()
        total = cm.sum()
        correct = diagonal.sum()

        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: {correct}/{total} ({correct / total:.1%})")

        # –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Å–∞–º
        print(f"\nüìä Accuracy –ø–æ –∫–ª–∞—Å—Å–∞–º:")
        for i, class_name in enumerate(class_names):
            class_total = cm[i, :].sum()
            if class_total > 0:
                class_correct = cm[i, i]
                print(f"   {class_name}: {class_correct}/{class_total} ({class_correct / class_total:.1%})")

    def get_model_info(self):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        """
        return {
            'model_name': self.best_model_name,
            'model_type': type(self.best_model).__name__,
            'best_score': self.best_score,
            'parameters': self.best_params,
            'feature_count': len(self.vectorizer.get_feature_names_out()),
            'n_classes': self.n_classes,
            'class_names': list(self.class_names)
        }

    def get_class_distribution(self, data):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö
        """
        _, labels = self.prepare_data(data)
        unique, counts = np.unique(labels, return_counts=True)
        return dict(zip(unique, counts))

    def save_model(self, filename):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        joblib.dump({
            'best_model': self.best_model,
            'vectorizer': self.vectorizer,
            'label_encoder': self.label_encoder,
            'best_model_name': self.best_model_name,
            'best_score': self.best_score,
            'best_params': self.best_params,
            'class_names': self.class_names,
            'n_classes': self.n_classes
        }, filename)
        print(f"üíæ Multi-class AutoML –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")

    def load_model(self, filename):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        loaded = joblib.load(filename)
        self.best_model = loaded['best_model']
        self.vectorizer = loaded['vectorizer']
        self.label_encoder = loaded['label_encoder']
        self.best_model_name = loaded['best_model_name']
        self.best_score = loaded.get('best_score', 0)
        self.best_params = loaded.get('best_params', {})
        self.class_names = loaded.get('class_names', [])
        self.n_classes = loaded.get('n_classes', 0)
        self.is_trained = True
        print(f"üì• Multi-class AutoML –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filename}")


# –°–ü–ï–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† –î–õ–Ø –¢–û–ù–ê–õ–¨–ù–û–°–¢–ò (3 –ö–õ–ê–°–°–ê)
class SentimentAutoMLClassifier(SimpleAutoMLMultiClassClassifier):
    """
    –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π AutoML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (–Ω–µ–≥–∞—Ç–∏–≤, –Ω–µ–π—Ç—Ä–∞–ª, –ø–æ–∑–∏—Ç–∏–≤)
    """

    def __init__(self, max_training_time=300, n_iter=50, random_state=42):
        super().__init__(max_training_time, n_iter, random_state)

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
        self.models.update({
            'svm_rbf': {
                'model': SVC(kernel='rbf', probability=True, random_state=random_state),
                'params': {
                    'C': uniform(0.1, 10),
                    'gamma': uniform(0.001, 0.1)
                }
            },
            'xgboost': {
                'model': None,  # –ë—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏
                'params': {}
            }
        })

        # –ü—ã—Ç–∞–µ–º—Å—è –¥–æ–±–∞–≤–∏—Ç—å XGBoost –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
        try:
            from xgboost import XGBClassifier
            self.models['xgboost']['model'] = XGBClassifier(
                random_state=random_state,
                use_label_encoder=False,
                eval_metric='mlogloss'
            )
            self.models['xgboost']['params'] = {
                'n_estimators': randint(50, 200),
                'max_depth': randint(3, 10),
                'learning_rate': uniform(0.01, 0.3),
                'subsample': uniform(0.5, 0.5)
            }
        except ImportError:
            print("XGBoost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
            del self.models['xgboost']

    def predict_sentiment(self, text):
        """
        –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        """
        result = super().predict_single(text)

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –¥–ª—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        sentiment_mapping = {
            'negative': '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π',
            'neutral': '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π',
            'positive': '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π',
            'neg': '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π',
            'neu': '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π',
            'pos': '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π',
            '0': '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π',
            '1': '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π',
            '2': '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π'
        }

        prediction = result['prediction']
        sentiment = sentiment_mapping.get(str(prediction).lower(), prediction)

        result['sentiment'] = sentiment
        result['sentiment_confidence'] = result['confidence']

        return result


def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–≥–æ AutoML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    """

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö (–∞–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –ø–æ–¥ –≤–∞—à —Ñ–æ—Ä–º–∞—Ç)
    def load_jsonl(filepath):
        with open(filepath, 'r', encoding='utf-8') as f:
            return [json.loads(line) for line in f]

    print("üìÇ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•...")

    # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    # –í–∞—à–∏ –¥–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –ø–æ–ª–µ 'label' –≤–º–µ—Å—Ç–æ 'sentiment'
    train_data = load_jsonl('../util/news_category_train.jsonl')
    test_data = load_jsonl('../util/news_category_test.jsonl')

    print(f"üìä –û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(train_data)}")
    print(f"üìä –¢–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(test_data)}")

    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val
    np.random.seed(42)
    indices = np.random.permutation(len(train_data))
    split_idx = int(0.8 * len(train_data))

    train_indices = indices[:split_idx]
    val_indices = indices[split_idx:]

    train_subset = [train_data[i] for i in train_indices]
    val_subset = [train_data[i] for i in val_indices]

    print(f"üìä Train: {len(train_subset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"üìä Val: {len(val_subset)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    print("\n" + "=" * 60)
    print("üéØ –û–ë–£–ß–ï–ù–ò–ï –ú–ù–û–ì–û–ö–õ–ê–°–°–û–í–û–ì–û AUTOML")
    print("=" * 60)

    automl_classifier = SimpleAutoMLMultiClassClassifier(
        n_iter=30,
        max_training_time=120  # 2 –º–∏–Ω—É—Ç—ã
    )

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    print("\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í –í –û–ë–£–ß–ê–Æ–©–ò–• –î–ê–ù–ù–´–•:")
    class_dist = automl_classifier.get_class_distribution(train_subset)
    for class_name, count in class_dist.items():
        print(f"   {class_name}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤ ({count / len(train_subset):.1%})")

    # –û–±—É—á–∞–µ–º
    automl_classifier.train(train_subset, val_subset)

    # –û—Ü–µ–Ω–∏–≤–∞–µ–º
    print("\nüß™ –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•...")
    accuracy = automl_classifier.evaluate(test_data)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    automl_classifier.save_model('multiclass_automl_model.pkl')

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–µ—Å—Ç–∏—Ä—É–µ–º
    print("\nüß™ –¢–ï–°–¢ –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ò...")
    loaded_classifier = SimpleAutoMLMultiClassClassifier()
    loaded_classifier.load_model('multiclass_automl_model.pkl')

    # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    test_text = "–î–æ–≤–æ–ª—å–Ω–æ –Ω–µ–ø–ª–æ—Ö–æ, –Ω–æ –µ—Å—Ç—å –Ω–µ–±–æ–ª—å—à–∏–µ –∑–∞–º–µ—á–∞–Ω–∏—è"
    result = loaded_classifier.predict_single(test_text)
    print(f"\nüìù –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:")
    print(f"   –¢–µ–∫—Å—Ç: {test_text}")
    print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {result['prediction']}")
    print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.3f}")

    return automl_classifier


def analyze_multiclass_performance(automl_classifier, test_data):
    """
    –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    """
    from sklearn.metrics import precision_recall_fscore_support, cohen_kappa_score

    X_test, y_test = automl_classifier.prepare_data(test_data)
    y_pred, _ = automl_classifier.predict(X_test)

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    y_test_encoded = automl_classifier.label_encoder.transform(y_test)
    y_pred_encoded = automl_classifier.label_encoder.transform(y_pred)

    # Cohen's Kappa
    kappa = cohen_kappa_score(y_test_encoded, y_pred_encoded)

    # Precision, Recall, F1 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
    precision, recall, f1, support = precision_recall_fscore_support(
        y_test_encoded, y_pred_encoded, labels=range(automl_classifier.n_classes)
    )

    print("\nüìä –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:")
    print("=" * 60)

    print(f"\nüìà –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ö–∞–ø–ø–∞ (Cohen's Kappa): {kappa:.4f}")
    print("   (>0.8: –æ—Ç–ª–∏—á–Ω–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ, >0.6: —Ö–æ—Ä–æ—à–µ–µ, >0.4: —É–º–µ—Ä–µ–Ω–Ω–æ–µ)")

    print(f"\nüìä –ú–ï–¢–†–ò–ö–ò –ü–û –ö–õ–ê–°–°–ê–ú:")
    print("-" * 40)
    print(f"{'–ö–ª–∞—Å—Å':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}")
    print("-" * 40)

    for i, class_name in enumerate(automl_classifier.class_names):
        print(f"{class_name:<15} {precision[i]:<10.4f} {recall[i]:<10.4f} {f1[i]:<10.4f} {support[i]:<10}")

    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –≤ –≤–∏–¥–µ DataFrame –¥–ª—è –ª—É—á—à–µ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    cm = confusion_matrix(y_test_encoded, y_pred_encoded)
    cm_df = pd.DataFrame(cm,
                         index=automl_classifier.class_names,
                         columns=automl_classifier.class_names)

    print(f"\nüìä –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö (DataFrame):")
    print(cm_df)

    return {
        'kappa': kappa,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': cm_df
    }


if __name__ == "__main__":
    print("üöÄ –ó–ê–ü–£–°–ö –ú–ù–û–ì–û–ö–õ–ê–°–°–û–í–û–ì–û AUTOML –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê")
    print("=" * 80)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–∏–º–µ—Ä
    classifier = main()