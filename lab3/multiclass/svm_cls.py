from sklearn.svm import LinearSVC, SVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.calibration import CalibratedClassifierCV
from sklearn.preprocessing import LabelEncoder
import numpy as np
import joblib
import seaborn as sns
import matplotlib.pyplot as plt
from typing import List, Dict, Any, Optional, Tuple, Union
from collections import Counter
import pandas as pd
import warnings

warnings.filterwarnings('ignore')

from util.jsonl_process import read_jsonl_basic


class SVMCategoryClassifier:
    """
    –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ SVM
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏–Ω–µ–π–Ω—ã–µ –∏ RBF —è–¥—Ä–∞
    """

    def __init__(self,
                 C: float = 1.0,
                 kernel: str = 'linear',
                 loss: str = 'squared_hinge',
                 penalty: str = 'l2',
                 class_names: Optional[List[str]] = None,
                 text_field: str = 'text',
                 label_field: str = 'category',
                 calibrate_probabilities: bool = True,
                 multi_class_strategy: str = 'ovr',
                 random_state: int = 42):
        """
        Args:
            C: –ø–∞—Ä–∞–º–µ—Ç—Ä —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (–º–µ–Ω—å—à–µ = —Å–∏–ª—å–Ω–µ–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
            kernel: —Ç–∏–ø —è–¥—Ä–∞ ('linear', 'rbf', 'poly', 'sigmoid')
            loss: —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å ('hinge' –∏–ª–∏ 'squared_hinge') - —Ç–æ–ª—å–∫–æ –¥–ª—è linear
            penalty: —Ç–∏–ø —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ ('l1' –∏–ª–∏ 'l2') - —Ç–æ–ª—å–∫–æ –¥–ª—è linear
            class_names: —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∞—Å—Å–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            text_field: –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—è —Å —Ç–µ–∫—Å—Ç–æ–º
            label_field: –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—è —Å –º–µ—Ç–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            calibrate_probabilities: –∫–∞–ª–∏–±—Ä–æ–≤–∞—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è SVM)
            multi_class_strategy: —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ('ovr' –∏–ª–∏ 'ovr')
            random_state: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        """
        self.vectorizer = TfidfVectorizer(
            max_features=10000,
            min_df=2,
            max_df=0.9,
            ngram_range=(1, 2),
            stop_words=None
        )

        # –í—ã–±–∏—Ä–∞–µ–º —Ç–∏–ø SVM –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —è–¥—Ä–∞
        if kernel == 'linear':
            base_svm = LinearSVC(
                C=C,
                loss=loss,
                penalty=penalty,
                dual=True,
                random_state=random_state,
                max_iter=2000,
                multi_class=multi_class_strategy
            )
        else:
            # –î–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö —è–¥–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ–º SVC
            base_svm = SVC(
                C=C,
                kernel=kernel,
                random_state=random_state,
                max_iter=2000,
                decision_function_shape=multi_class_strategy,
                probability=calibrate_probabilities  # –í–∫–ª—é—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –µ—Å–ª–∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞
            )

        # –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è SVM (—Ç–æ–ª—å–∫–æ –¥–ª—è linear –∏–ª–∏ –µ—Å–ª–∏ probability=False –≤ SVC)
        if calibrate_probabilities:
            if kernel == 'linear' or not base_svm.probability:
                self.model = CalibratedClassifierCV(base_svm, cv=3, method='sigmoid')
            else:
                self.model = base_svm  # SVC —É–∂–µ –∏–º–µ–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏ probability=True
        else:
            self.model = base_svm

        self.label_encoder = LabelEncoder()
        self.class_names = class_names
        self.text_field = text_field
        self.label_field = label_field
        self.calibrate_probabilities = calibrate_probabilities
        self.kernel = kernel
        self.C = C
        self.multi_class_strategy = multi_class_strategy
        self.is_trained = False
        self.num_classes = 0
        self.random_state = random_state

    def prepare_data(self, data: List[Dict[str, Any]]) -> Tuple[List[str], List[str]]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        """
        texts = [item[self.text_field] for item in data]
        labels = [item[self.label_field] for item in data]
        return texts, labels

    def analyze_class_distribution(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö
        """
        _, labels = self.prepare_data(data)
        label_counts = Counter(labels)

        result = {
            'total_samples': len(data),
            'num_classes': len(label_counts),
            'classes': dict(label_counts),
            'class_percentages': {},
            'imbalance_ratio': None,
            'unique_labels': sorted(list(label_counts.keys()))
        }

        if label_counts:
            max_count = max(label_counts.values())
            min_count = min(label_counts.values())
            if min_count > 0:
                result['imbalance_ratio'] = max_count / min_count

            for label, count in label_counts.items():
                result['class_percentages'][label] = count / len(data) * 100

        return result

    def train(self, train_data: List[Dict[str, Any]],
              val_data: Optional[List[Dict[str, Any]]] = None,
              auto_detect_classes: bool = True) -> None:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ SVM
        """
        print("üéØ –û–ë–£–ß–ï–ù–ò–ï SVM –î–õ–Ø –ú–ù–û–ì–û–ö–õ–ê–°–°–û–í–û–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò...")
        print(f"   –ü–æ–ª–µ —Å —Ç–µ–∫—Å—Ç–æ–º: '{self.text_field}'")
        print(f"   –ü–æ–ª–µ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π: '{self.label_field}'")
        print(f"   –Ø–¥—Ä–æ: {self.kernel}")
        print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä C: {self.C}")
        print(f"   –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π: {self.calibrate_probabilities}")
        print(f"   –°—Ç—Ä–∞—Ç–µ–≥–∏—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {self.multi_class_strategy}")

        # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        train_dist = self.analyze_class_distribution(train_data)
        print(f"\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–ê–¢–ï–ì–û–†–ò–ô –í TRAIN:")
        print(f"   –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {train_dist['total_samples']}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {train_dist['num_classes']}")

        if train_dist['imbalance_ratio']:
            print(f"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞: {train_dist['imbalance_ratio']:.2f}")
            if train_dist['imbalance_ratio'] > 3:
                print("   ‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω —Å–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
                print("   ‚ÑπÔ∏è  –î–ª—è SVM —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å class_weight='balanced'")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train_raw = self.prepare_data(train_data)

        # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
        if auto_detect_classes:
            self.label_encoder.fit(y_train_raw)
            y_train = self.label_encoder.transform(y_train_raw)
            self.class_names = list(self.label_encoder.classes_)
        else:
            if self.class_names is None:
                raise ValueError("class_names –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∑–∞–¥–∞–Ω, –µ—Å–ª–∏ auto_detect_classes=False")
            self.label_encoder.fit(self.class_names)
            y_train = self.label_encoder.transform(y_train_raw)

        self.num_classes = len(self.class_names)

        # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö
        print(f"\nüìã –°–ü–ò–°–û–ö –ö–ê–¢–ï–ì–û–†–ò–ô ({self.num_classes}):")
        for i, (class_name, count) in enumerate(train_dist['classes'].items()):
            percentage = train_dist['class_percentages'].get(class_name, 0)
            print(f"   {i + 1:2d}. {class_name}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤ ({percentage:.1f}%)")

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
        print("\nüìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        X_train_vec = self.vectorizer.fit_transform(X_train)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_vec.shape}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤/—Ñ—Ä–∞–∑: {len(self.vectorizer.get_feature_names_out())}")
        print(f"   –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã: {X_train_vec.nnz / (X_train_vec.shape[0] * X_train_vec.shape[1]):.4f}")

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("\nü§ñ –û–±—É—á–µ–Ω–∏–µ SVM...")
        self.model.fit(X_train_vec, y_train)
        self.is_trained = True

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        train_pred = self.model.predict(X_train_vec)
        train_accuracy = accuracy_score(y_train, train_pred)
        print(f"\n‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train: {train_accuracy:.3f}")

        # –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º –Ω–∞ train
        print("\nüìä –û–¢–ß–ï–¢ –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú (train):")
        print(classification_report(y_train, train_pred, target_names=self.class_names))

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        if val_data:
            val_accuracy, _ = self.evaluate(val_data, detailed=False)
            print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ val: {val_accuracy:.3f}")

        # –ü–æ–∫–∞–∂–µ–º –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è –ª–∏–Ω–µ–π–Ω–æ–≥–æ —è–¥—Ä–∞)
        if self.kernel == 'linear':
            self._show_important_features(top_n=15)

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–¥–¥–µ—Ä–∂–∫–µ –≤–µ–∫—Ç–æ—Ä–æ–≤
        self._show_svm_info(X_train_vec)

        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –Ω–∞ train
        self._plot_confusion_matrix(y_train, train_pred, "Train Confusion Matrix")

    def predict(self, texts: List[str]) -> Tuple[List[str], np.ndarray]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)
        predictions_encoded = self.model.predict(X_vec)

        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        if hasattr(self.model, 'predict_proba'):
            probabilities = self.model.predict_proba(X_vec)
        else:
            # –ï—Å–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º decision function
            decision_scores = self.model.decision_function(X_vec)
            probabilities = self._decision_to_probability_multiclass(decision_scores)

        predictions = self.label_encoder.inverse_transform(predictions_encoded)
        return predictions, probabilities

    def _decision_to_probability_multiclass(self, decision_scores: np.ndarray) -> np.ndarray:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ decision function –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        # –î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–≥–æ —Å–ª—É—á–∞—è decision_scores –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (n_samples, n_classes)
        if len(decision_scores.shape) == 1:
            # –ë–∏–Ω–∞—Ä–Ω—ã–π —Å–ª—É—á–∞–π
            decision_scores = decision_scores.reshape(-1, 1)
            decision_scores = np.hstack([-decision_scores, decision_scores])

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º softmax –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        exp_scores = np.exp(decision_scores - np.max(decision_scores, axis=1, keepdims=True))
        probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

        return probabilities

    def predict_single(self, text: str) -> Dict[str, Any]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        predictions, probabilities = self.predict([text])
        pred = predictions[0]
        pred_encoded = self.label_encoder.transform([pred])[0]
        prob = probabilities[0]

        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        class_probs = {}
        for i, cls in enumerate(self.class_names):
            class_probs[cls] = prob[i]

        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-3 –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        top_n = min(3, self.num_classes)
        top_indices = np.argsort(prob)[-top_n:][::-1]
        top_categories = []
        for idx in top_indices:
            top_categories.append({
                'category': self.class_names[idx],
                'probability': prob[idx],
                'probability_percent': prob[idx] * 100
            })

        # Decision function –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        X_vec = self.vectorizer.transform([text])
        if hasattr(self.model, 'decision_function'):
            decision_scores = self.model.decision_function(X_vec)[0]
            # –ë–µ—Ä–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –≥–∏–ø–µ—Ä–ø–ª–æ—Å–∫–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
            if len(decision_scores.shape) == 0:
                decision_score = abs(decision_scores)
            else:
                decision_score = abs(decision_scores[pred_encoded])
        else:
            # –ï—Å–ª–∏ decision function –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–Ω–æ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            decision_score = prob[pred_encoded] - np.max(prob[np.arange(len(prob)) != pred_encoded])

        return {
            'prediction': pred,
            'category': pred,
            'prediction_encoded': pred_encoded,
            'category_probabilities': class_probs,
            'top_categories': top_categories,
            'confidence': prob[pred_encoded],
            'confidence_percent': prob[pred_encoded] * 100,
            'decision_score': decision_score,
            'margin': self._calculate_margin(X_vec, pred_encoded)
        }

    def _calculate_margin(self, X_vec, pred_encoded):
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∑–∞–∑–æ—Ä (margin) –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        """
        if hasattr(self.model, 'decision_function'):
            decision_scores = self.model.decision_function(X_vec)[0]

            if len(decision_scores.shape) == 0:
                # –ë–∏–Ω–∞—Ä–Ω—ã–π —Å–ª—É—á–∞–π
                return abs(decision_scores)
            else:
                # –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π —Å–ª—É—á–∞–π
                scores = decision_scores.copy()
                scores[pred_encoded] = -np.inf  # –ò—Å–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å
                second_best = np.max(scores)
                margin = decision_scores[pred_encoded] - second_best
                return margin
        return 0.0

    def evaluate(self, test_data: List[Dict[str, Any]],
                 detailed: bool = True,
                 plot_confusion_matrix: bool = True) -> Tuple[float, Dict]:
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        X_test, y_test_raw = self.prepare_data(test_data)

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
        y_test = []
        for label in y_test_raw:
            if label in self.label_encoder.classes_:
                y_test.append(label)
            else:
                # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –º–µ—Ç–∫–∏ –Ω–∞ –ø–µ—Ä–≤—É—é –∏–∑–≤–µ—Å—Ç–Ω—É—é
                y_test.append(self.class_names[0])

        y_test_encoded = self.label_encoder.transform(y_test)
        X_test_vec = self.vectorizer.transform(X_test)

        y_pred_encoded = self.model.predict(X_test_vec)
        y_pred = self.label_encoder.inverse_transform(y_pred_encoded)
        accuracy = accuracy_score(y_test_encoded, y_pred_encoded)

        if detailed:
            print(f"\nüìä –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•:")
            print(f"   –ü—Ä–∏–º–µ—Ä–æ–≤: {len(test_data)}")
            print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")

            print(f"\nüìà –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:")
            print(classification_report(y_test_encoded, y_pred_encoded,
                                        target_names=self.class_names, digits=3))

            # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
            if self.num_classes > 1:
                print(f"\nüìä –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö:")
                cm = confusion_matrix(y_test_encoded, y_pred_encoded)
                self._print_confusion_matrix(cm)

                if plot_confusion_matrix:
                    self._plot_confusion_matrix(y_test_encoded, y_pred_encoded,
                                                "Test Confusion Matrix")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        report_dict = classification_report(y_test_encoded, y_pred_encoded,
                                            target_names=self.class_names,
                                            output_dict=True)

        return accuracy, report_dict

    def _print_confusion_matrix(self, cm: np.ndarray) -> None:
        """
        –ö—Ä–∞—Å–∏–≤–æ –ø–µ—á–∞—Ç–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫
        """
        n_classes = len(self.class_names)

        if n_classes <= 1:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫")
            return

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        max_class_len = max(len(cls) for cls in self.class_names)
        header_padding = max(12, max_class_len + 2)

        header = " " * header_padding + " | "
        header += " ".join([f"{cls[:10]:>10}" for cls in self.class_names])
        print(header)
        print("-" * (header_padding + 3 + n_classes * 11))

        # –°—Ç—Ä–æ–∫–∏
        for i, cls in enumerate(self.class_names):
            row = f"{cls[:header_padding - 2]:>{header_padding - 2}} | "
            row += " ".join([f"{cm[i][j]:>10}" for j in range(n_classes)])
            print(row)

    def _plot_confusion_matrix(self, y_true: np.ndarray,
                               y_pred: np.ndarray,
                               title: str = "Confusion Matrix") -> None:
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
        """
        try:
            if self.num_classes <= 1:
                print("‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫")
                return

            cm = confusion_matrix(y_true, y_pred)

            plt.figure(figsize=(max(10, self.num_classes), max(8, self.num_classes * 0.8)))

            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º (–ø–æ –∏—Å—Ç–∏–Ω–Ω—ã–º –º–µ—Ç–∫–∞–º)
            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            cm_normalized = np.nan_to_num(cm_normalized)  # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ 0

            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                        xticklabels=self.class_names,
                        yticklabels=self.class_names,
                        vmin=0, vmax=1)
            plt.title(f"{title} (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–∞)")
            plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')
            plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')
            plt.tight_layout()

            filename = title.lower().replace(' ', '_').replace('-', '_')
            plt.savefig(f"{filename}.png", dpi=300, bbox_inches='tight')
            plt.show()
        except Exception as e:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫: {e}")

    def _show_important_features(self, top_n: int = 15):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ (—Ç–æ–ª—å–∫–æ –¥–ª—è –ª–∏–Ω–µ–π–Ω–æ–≥–æ —è–¥—Ä–∞)
        """
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –∏–∑ –±–∞–∑–æ–≤–æ–≥–æ estimator
        coef = None

        if hasattr(self.model, 'coef_'):
            coef = self.model.coef_
        elif hasattr(self.model, 'estimators_'):
            # –î–ª—è CalibratedClassifierCV –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π estimator
            for estimator in self.model.estimators_:
                if hasattr(estimator, 'coef_'):
                    coef = estimator.coef_
                    break

        if coef is None:
            print("‚ÑπÔ∏è  –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –ª–∏–Ω–µ–π–Ω–æ–≥–æ —è–¥—Ä–∞")
            return

        feature_names = self.vectorizer.get_feature_names_out()

        print(f"\nüîç –¢–û–ü-{top_n} –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í SVM (–ª–∏–Ω–µ–π–Ω–æ–µ —è–¥—Ä–æ):")

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–≤–æ–∏ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for class_idx, class_name in enumerate(self.class_names):
            print(f"\n   –ö–ê–¢–ï–ì–û–†–ò–Ø '{class_name}':")

            if len(coef.shape) == 1:
                # –ë–∏–Ω–∞—Ä–Ω—ã–π —Å–ª—É—á–∞–π
                coef_for_class = coef
            else:
                # –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π —Å–ª—É—á–∞–π
                coef_for_class = coef[class_idx]

            # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –¥–∞–Ω–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é)
            print(f"      –ü–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –î–õ–Ø –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:")
            pos_indices = np.argsort(coef_for_class)[-top_n:][::-1]
            for idx in pos_indices:
                print(f"        + {feature_names[idx]}: {coef_for_class[idx]:.3f}")

            # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—É–∫–∞–∑—ã–≤–∞—é—Ç –ü–†–û–¢–ò–í –¥–∞–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)
            print(f"      –ü–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –ü–†–û–¢–ò–í –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:")
            neg_indices = np.argsort(coef_for_class)[:top_n]
            for idx in neg_indices:
                print(f"        - {feature_names[idx]}: {coef_for_class[idx]:.3f}")

    def _show_svm_info(self, X_train_vec: Any) -> None:
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–µ–∫—Ç–æ—Ä–∞—Ö –ø–æ–¥–¥–µ—Ä–∂–∫–∏
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–π estimator
            base_estimator = None
            if hasattr(self.model, 'estimators_'):
                # –î–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
                base_estimator = self.model.estimators_[0]
            else:
                base_estimator = self.model

            if hasattr(base_estimator, 'support_'):
                n_support_vectors = len(base_estimator.support_)
                print(f"\nüìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û SVM:")
                print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø–æ–¥–¥–µ—Ä–∂–∫–∏: {n_support_vectors}")
                print(f"   –ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {n_support_vectors / X_train_vec.shape[0] * 100:.1f}%")

                if hasattr(base_estimator, 'support_vectors_'):
                    print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø–æ–¥–¥–µ—Ä–∂–∫–∏: {base_estimator.support_vectors_.shape}")

        except Exception as e:
            print(f"   ‚ÑπÔ∏è  –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–µ–∫—Ç–æ—Ä–∞—Ö –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")

    def get_decision_boundary_info(self, text: str) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–∏ –¥–æ —Ä–∞–∑–¥–µ–ª—è—é—â–µ–π –≥–∏–ø–µ—Ä–ø–ª–æ—Å–∫–æ—Å—Ç–∏
        """
        X_vec = self.vectorizer.transform([text])

        if hasattr(self.model, 'decision_function'):
            decision_scores = self.model.decision_function(X_vec)[0]

            if len(decision_scores.shape) == 0:
                # –ë–∏–Ω–∞—Ä–Ω—ã–π —Å–ª—É—á–∞–π
                decision_score = decision_scores
                distance_from_boundary = abs(decision_score)
                side = "positive" if decision_score > 0 else "negative"
            else:
                # –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π —Å–ª—É—á–∞–π
                decision_score = np.max(decision_scores)
                predicted_class = np.argmax(decision_scores)
                distance_from_boundary = decision_score
                side = self.class_names[predicted_class]
        else:
            # –ï—Å–ª–∏ decision function –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
            decision_scores = None
            distance_from_boundary = 0
            side = "unknown"

        return {
            'decision_scores': decision_scores,
            'distance_from_boundary': distance_from_boundary,
            'side': side,
            'confidence': min(distance_from_boundary, 1.0) if distance_from_boundary is not None else 0.5
        }

    def save_model(self, filename: str) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        joblib.dump({
            'model': self.model,
            'vectorizer': self.vectorizer,
            'label_encoder': self.label_encoder,
            'class_names': self.class_names,
            'text_field': self.text_field,
            'label_field': self.label_field,
            'calibrate_probabilities': self.calibrate_probabilities,
            'kernel': self.kernel,
            'C': self.C,
            'multi_class_strategy': self.multi_class_strategy,
            'num_classes': self.num_classes
        }, filename)
        print(f"üíæ –ú–æ–¥–µ–ª—å SVM —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")

    def load_model(self, filename: str) -> None:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        loaded = joblib.load(filename)
        self.model = loaded['model']
        self.vectorizer = loaded['vectorizer']
        self.label_encoder = loaded['label_encoder']
        self.class_names = loaded['class_names']
        self.text_field = loaded.get('text_field', 'text')
        self.label_field = loaded.get('label_field', 'category')
        self.calibrate_probabilities = loaded.get('calibrate_probabilities', True)
        self.kernel = loaded.get('kernel', 'linear')
        self.C = loaded.get('C', 1.0)
        self.multi_class_strategy = loaded.get('multi_class_strategy', 'ovr')
        self.num_classes = loaded.get('num_classes', len(self.class_names))
        self.is_trained = True

        print(f"üì• –ú–æ–¥–µ–ª—å SVM –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filename}")
        print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {self.class_names}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {self.num_classes}")
        print(f"   –Ø–¥—Ä–æ: {self.kernel}")
        print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä C: {self.C}")

    def predict_batch_with_details(self, texts: List[str]) -> List[Dict[str, Any]]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        predictions, probabilities = self.predict(texts)

        results = []
        for i, (text, pred, probs) in enumerate(zip(texts, predictions, probabilities)):
            pred_encoded = self.label_encoder.transform([pred])[0]

            # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            top_n = min(3, self.num_classes)
            top_indices = np.argsort(probs)[-top_n:][::-1]
            top_categories = []
            for idx in top_indices:
                top_categories.append({
                    'category': self.class_names[idx],
                    'probability': probs[idx],
                    'probability_percent': probs[idx] * 100
                })

            results.append({
                'text': text,
                'prediction': pred,
                'predicted_category': pred,
                'confidence': probs[pred_encoded],
                'confidence_percent': probs[pred_encoded] * 100,
                'top_categories': top_categories
            })

        return results


def compare_svm_kernels(train_data: List[Dict[str, Any]],
                        val_data: List[Dict[str, Any]],
                        text_field: str = 'text',
                        label_field: str = 'category') -> Dict[str, SVMCategoryClassifier]:
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —è–¥–µ—Ä SVM
    """
    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –†–ê–ó–ù–´–• –Ø–î–ï–† SVM")
    print("=" * 60)

    kernels = ['linear', 'rbf']  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å 'poly', 'sigmoid' –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    models = {}

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª–∞—Å—Å—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö
    all_labels = [item[label_field] for item in train_data]
    class_names = sorted(list(set(all_labels)))

    if len(class_names) < 2:
        print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–º–∏–Ω–∏–º—É–º 2)")
        return models

    for kernel in kernels:
        print(f"\nüéØ SVM —Å —è–¥—Ä–æ–º '{kernel}':")
        try:
            model = SVMCategoryClassifier(
                C=1.0,
                kernel=kernel,
                class_names=class_names,
                text_field=text_field,
                label_field=label_field,
                calibrate_probabilities=True
            )

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏
            if len(train_data) > 300:
                train_subset = train_data[:300]
                val_subset = val_data[:100] if len(val_data) > 100 else val_data
                print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ: {len(train_subset)} train, {len(val_subset)} val")
            else:
                train_subset = train_data
                val_subset = val_data

            model.train(train_subset, val_subset, auto_detect_classes=False)
            models[f'SVM_{kernel}'] = model

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å —è–¥—Ä–æ–º '{kernel}': {e}")

    return models


def compare_svm_C_values(train_data: List[Dict[str, Any]],
                         val_data: List[Dict[str, Any]],
                         text_field: str = 'text',
                         label_field: str = 'category') -> Dict[str, SVMCategoryClassifier]:
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ C
    """
    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –†–ê–ó–ù–´–• –ó–ù–ê–ß–ï–ù–ò–ô –ü–ê–†–ê–ú–ï–¢–†–ê C")
    print("=" * 60)

    C_values = [0.1, 1.0, 10.0]
    models = {}

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª–∞—Å—Å—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö
    all_labels = [item[label_field] for item in train_data]
    class_names = sorted(list(set(all_labels)))

    if len(class_names) < 2:
        print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–º–∏–Ω–∏–º—É–º 2)")
        return models

    for C in C_values:
        print(f"\nüéØ SVM —Å C={C}:")
        try:
            model = SVMCategoryClassifier(
                C=C,
                kernel='linear',
                class_names=class_names,
                text_field=text_field,
                label_field=label_field,
                calibrate_probabilities=True
            )

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            if len(train_data) > 200:
                train_subset = train_data[:200]
                val_subset = val_data[:50] if len(val_data) > 50 else val_data
            else:
                train_subset = train_data
                val_subset = val_data

            model.train(train_subset, val_subset, auto_detect_classes=False)
            models[f'SVM_C_{C}'] = model

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å C={C}: {e}")

    return models


def quick_train_svm(train_file: str,
                    val_file: Optional[str] = None,
                    test_file: Optional[str] = None,
                    text_field: str = 'text',
                    label_field: str = 'category',
                    kernel: str = 'linear',
                    C: float = 1.0,
                    output_model: str = 'svm_category_classifier.pkl') -> Optional[SVMCategoryClassifier]:
    """
    –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ SVM –º–æ–¥–µ–ª–∏ –∏–∑ —Ñ–∞–π–ª–æ–≤
    """
    import json
    import os

    def load_jsonl(filepath: str) -> List[Dict[str, Any]]:
        if not os.path.exists(filepath):
            print(f"‚ö†Ô∏è  –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
            return []
        with open(filepath, 'r', encoding='utf-8') as f:
            return [json.loads(line) for line in f]

    print("üöÄ –ó–ê–ü–£–°–ö –ë–´–°–¢–†–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø SVM")
    print("=" * 60)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    train_data = load_jsonl(train_file)
    if not train_data:
        print(f"‚ùå –û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {train_file}")
        return None

    print(f"   Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    if val_file:
        val_data = load_jsonl(val_file)
        print(f"   Val: {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    else:
        val_data = None

    if test_file:
        test_data = load_jsonl(test_file)
        print(f"   Test: {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    else:
        test_data = None

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
    if train_data:
        sample_item = train_data[0]
        if text_field not in sample_item:
            print(f"‚ùå –û—à–∏–±–∫–∞: –ø–æ–ª–µ '{text_field}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–∞–Ω–Ω—ã—Ö")
            return None
        if label_field not in sample_item:
            print(f"‚ùå –û—à–∏–±–∫–∞: –ø–æ–ª–µ '{label_field}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–∞–Ω–Ω—ã—Ö")
            return None

    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    print(f"\nüéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è SVM...")
    print(f"   –Ø–¥—Ä–æ: {kernel}")
    print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä C: {C}")

    classifier = SVMCategoryClassifier(
        C=C,
        kernel=kernel,
        text_field=text_field,
        label_field=label_field,
        calibrate_probabilities=True
    )

    classifier.train(train_data, val_data, auto_detect_classes=True)

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º, –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    if test_data:
        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        accuracy, report = classifier.evaluate(test_data, detailed=True)
        print(f"\nüéØ –ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {accuracy:.3f}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        if report:
            report_df = pd.DataFrame(report).transpose()
            report_df.to_csv('svm_classification_report.csv', index=True)
            print(f"üìÑ –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ 'svm_classification_report.csv'")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    classifier.save_model(output_model)

    # –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä
    print(f"\nüß™ –¢–ï–°–¢–û–í–´–ô –ü–†–ò–ú–ï–† –†–ê–ë–û–¢–´ –ú–û–î–ï–õ–ò:")
    if train_data:
        sample_text = train_data[0][text_field]
        if len(sample_text) > 100:
            sample_text = sample_text[:100] + "..."
        result = classifier.predict_single(sample_text)
        print(f"   –¢–µ–∫—Å—Ç: '{sample_text}'")
        print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {result['prediction']}")
        print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence_percent']:.1f}%")

        if result['top_categories']:
            print(f"   –¢–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:")
            for i, cat in enumerate(result['top_categories'], 1):
                print(f"     {i}. {cat['category']}: {cat['probability_percent']:.1f}%")

    return classifier


def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è SVM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    """
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        train_data = read_jsonl_basic('../util/news_category_train.jsonl')
        val_data = read_jsonl_basic('../util/news_category_val.jsonl')
        test_data = read_jsonl_basic('../util/news_category_test.jsonl')

        print(f"üìä –î–∞–Ω–Ω—ã–µ: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
        if train_data:
            print(f"\nüìã –ü–†–ò–ú–ï–† –î–ê–ù–ù–´–•:")
            sample = train_data[0]
            print(f"   –ü–æ–ª—è: {list(sample.keys())}")
            print(f"   –¢–µ–∫—Å—Ç: {sample.get('text', 'N/A')[:100]}...")
            print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {sample.get('category', 'N/A')}")

        # 1. –û–±—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å SVM —Å –ª–∏–Ω–µ–π–Ω—ã–º —è–¥—Ä–æ–º
        print("\n" + "=" * 60)

        svm_classifier = SVMCategoryClassifier(
            C=1.0,
            kernel='linear',
            text_field='text',
            label_field='category',
            calibrate_probabilities=True
        )

        svm_classifier.train(train_data, val_data, auto_detect_classes=True)

        # 3. –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if test_data and len(test_data) > 0:
            print("\n" + "=" * 60)
            print("üß™ –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•:")
            test_subset = test_data[:200] if len(test_data) > 200 else test_data
            test_accuracy, test_report = svm_classifier.evaluate(test_subset, detailed=True)
            print(f"\nüìä –ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {test_accuracy:.3f}")

        # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        svm_classifier.save_model("svm_category_classifier.pkl")
    except FileNotFoundError as e:
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
        print("‚ÑπÔ∏è  –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –¥–∞–Ω–Ω—ã—Ö")
    except Exception as e:
        print(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞
    print("üöÄ –ó–ê–ü–£–°–ö –ü–†–ò–ú–ï–†–ê –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø SVM")
    print("=" * 60)
    main()