from sklearn.ensemble import BaggingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import numpy as np
import joblib
import json


class BaggingMultiClassClassifier:
    """
    –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ Bagging –∞–Ω—Å–∞–º–±–ª—è
    """

    def __init__(self, base_estimator='logistic', n_estimators=10,
                 max_samples=1.0, max_features=1.0, bootstrap=True,
                 bootstrap_features=False, random_state=42):
        """
        Args:
            base_estimator: 'logistic' –∏–ª–∏ 'tree' - –±–∞–∑–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
            n_estimators: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–∑–æ–≤—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤
            max_samples: –¥–æ–ª—è/–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ samples –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
            max_features: –¥–æ–ª—è/–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ features –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
            bootstrap: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ bootstrap sampling
            bootstrap_features: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ bootstrap –¥–ª—è features
            random_state: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        """
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            min_df=2,
            max_df=0.8,
            ngram_range=(1, 2)
        )

        # –î–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫
        self.label_encoder = LabelEncoder()

        # –í—ã–±–æ—Ä –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        if base_estimator == 'logistic':
            base_est = LogisticRegression(
                random_state=random_state,
                max_iter=1000,
                C=1.0,
                multi_class='ovr'  # –î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            )
        elif base_estimator == 'tree':
            base_est = DecisionTreeClassifier(
                random_state=random_state,
                max_depth=None
            )
        else:
            raise ValueError("base_estimator –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 'logistic' –∏–ª–∏ 'tree'")

        self.model = BaggingClassifier(
            estimator=base_est,
            n_estimators=n_estimators,
            max_samples=max_samples,
            max_features=max_features,
            bootstrap=bootstrap,
            bootstrap_features=bootstrap_features,
            random_state=random_state,
            n_jobs=-1,
            verbose=0
        )

        self.base_estimator = base_estimator
        self.is_trained = False
        self.class_names = None
        self.n_classes = None

    def prepare_data(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏
        """
        texts = [item['text'] for item in data]
        labels = [item['category'] for item in data]
        return texts, labels

    def train(self, train_data, val_data=None):
        """
        –û–±—É—á–µ–Ω–∏–µ Bagging –º–æ–¥–µ–ª–∏ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        print(f"üéØ –û–ë–£–ß–ï–ù–ò–ï BAGGING ({self.base_estimator.upper()})...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train = self.prepare_data(train_data)

        # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
        y_train_encoded = self.label_encoder.fit_transform(y_train)
        self.class_names = self.label_encoder.classes_
        self.n_classes = len(self.class_names)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
        unique_labels = set(y_train)
        print(f"üìä –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {self.n_classes} –∫–ª–∞—Å—Å–æ–≤: {list(self.class_names)}")

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
        print("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        X_train_vec = self.vectorizer.fit_transform(X_train)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_vec.shape}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {self.n_classes}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(y_train)}")
        print(f"   –ë–∞–∑–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º: {self.base_estimator}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π: {self.model.n_estimators}")
        print(f"   Max samples: {self.model.max_samples}")
        print(f"   Max features: {self.model.max_features}")
        print(f"   Bootstrap: {self.model.bootstrap}")
        print(f"   Bootstrap features: {self.model.bootstrap_features}")

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("ü§ñ –û–±—É—á–µ–Ω–∏–µ Bagging –∞–Ω—Å–∞–º–±–ª—è...")
        self.model.fit(X_train_vec, y_train_encoded)
        self.is_trained = True

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        train_pred = self.model.predict(X_train_vec)
        train_pred_decoded = self.label_encoder.inverse_transform(train_pred)
        train_accuracy = accuracy_score(y_train, train_pred_decoded)
        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train: {train_accuracy:.3f}")

        # Out-of-bag –æ—Ü–µ–Ω–∫–∞ (–µ—Å–ª–∏ bootstrap=True)
        if hasattr(self.model, 'oob_score_') and self.model.oob_score:
            print(f"‚úÖ Out-of-bag score: {self.model.oob_score_:.3f}")

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        if val_data:
            val_accuracy = self.evaluate(val_data)
            print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ val: {val_accuracy:.3f}")

        # –ê–Ω–∞–ª–∏–∑ –∞–Ω—Å–∞–º–±–ª—è
        self._analyze_ensemble()

        # –ü–æ–∫–∞–∂–µ–º –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ)
        if self.base_estimator == 'logistic':
            self._show_important_features(top_n=10)

    def predict(self, texts):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)

        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        predictions_encoded = self.model.predict(X_vec)
        predictions = self.label_encoder.inverse_transform(predictions_encoded)
        probabilities = self.model.predict_proba(X_vec)

        return predictions, probabilities

    def predict_single(self, text):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        predictions, probabilities = self.predict([text])
        pred = predictions[0]
        pred_encoded = self.label_encoder.transform([pred])[0]
        prob = probabilities[0]

        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤
        class_probabilities = {}
        for class_name, prob_value in zip(self.class_names, prob):
            class_probabilities[class_name] = prob_value

        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã
        sorted_probs = sorted(class_probabilities.items(), key=lambda x: x[1], reverse=True)
        top_3 = sorted_probs[:3]

        return {
            'prediction': pred,
            'confidence': float(prob[pred_encoded]),
            'probabilities': class_probabilities,
            'top_3_predictions': top_3,
            'model_type': type(self.model.estimator).__name__,
            'ensemble_size': self.model.n_estimators
        }

    def predict_with_voting_details(self, texts):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–∏ –∞–Ω—Å–∞–º–±–ª—è
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)

        # –ú–∞—Ç—Ä–∏—Ü–∞ –≥–æ–ª–æ—Å–æ–≤: [n_estimators, n_samples]
        all_predictions = []

        for i, estimator in enumerate(self.model.estimators_):
            try:
                # –£—á–∏—Ç—ã–≤–∞–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è
                if hasattr(self.model, 'estimators_features_') and self.model.estimators_features_:
                    features_idx = self.model.estimators_features_[i]
                    X_subset = X_vec[:, features_idx]
                else:
                    X_subset = X_vec

                predictions_encoded = estimator.predict(X_subset)
                all_predictions.append(predictions_encoded)
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ –º–æ–¥–µ–ª–∏ {i}: {e}")
                continue

        if not all_predictions:
            raise Exception("–ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ —Å–º–æ–≥–ª–∞ —Å–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ")

        all_predictions = np.array(all_predictions)  # [n_estimators, n_samples]
        n_estimators, n_samples = all_predictions.shape

        # –û—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        final_predictions_encoded = self.model.predict(X_vec)
        final_predictions = self.label_encoder.inverse_transform(final_predictions_encoded)
        probabilities = self.model.predict_proba(X_vec)

        results = []
        for i in range(n_samples):
            # –ì–æ–ª–æ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
            votes = {}
            for class_idx, class_name in enumerate(self.class_names):
                votes[class_name] = np.sum(all_predictions[:, i] == class_idx)

            # –°—á–∏—Ç–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–æ–ª–æ—Å–æ–≤
            total_votes = n_estimators
            sorted_votes = sorted(votes.items(), key=lambda x: x[1], reverse=True)
            consensus_ratio = sorted_votes[0][1] / total_votes

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–±–µ–¥–∏–≤—à–µ–º –∫–ª–∞—Å—Å–µ
            winner_class = final_predictions[i]
            winner_votes = votes[winner_class]

            results.append({
                'prediction': winner_class,
                'probability': probabilities[i],
                'votes': votes,
                'total_votes': total_votes,
                'winner_votes': winner_votes,
                'consensus_ratio': consensus_ratio,
                'unanimous': consensus_ratio == 1.0,
                'vote_distribution': sorted_votes
            })

        return results

    def evaluate(self, test_data):
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        X_test, y_test = self.prepare_data(test_data)
        X_test_vec = self.vectorizer.transform(X_test)

        # –ö–æ–¥–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç–∫–∏
        y_test_encoded = self.label_encoder.transform(y_test)

        y_pred_encoded = self.model.predict(X_test_vec)
        y_pred = self.label_encoder.inverse_transform(y_pred_encoded)

        accuracy = accuracy_score(y_test_encoded, y_pred_encoded)

        print("\nüìä –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(classification_report(y_test, y_pred, digits=4))

        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        print("\nüìà –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö:")
        cm = confusion_matrix(y_test_encoded, y_pred_encoded)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤ –∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        unique_classes_in_test = np.unique(y_pred)
        actual_class_names = [str(cls) for cls in unique_classes_in_test]

        # –ï—Å–ª–∏ –≤ —Ç–µ—Å—Ç–µ –Ω–µ—Ç –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –µ—Å—Ç—å
        if len(actual_class_names) < self.n_classes:
            print(f"‚ö†Ô∏è  –í —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ –≤—Å–µ –∫–ª–∞—Å—Å—ã. –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ: {len(actual_class_names)} –∏–∑ {self.n_classes}")
            self._print_confusion_matrix_simple(cm, actual_class_names)
        else:
            self._print_confusion_matrix(cm, self.class_names)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        print(f"\nüìä –û–ë–©–ò–ï –ú–ï–¢–†–ò–ö–ò:")
        print(f"   Accuracy: {accuracy:.4f}")

        # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        from sklearn.metrics import precision_score, recall_score, f1_score
        precision_macro = precision_score(y_test_encoded, y_pred_encoded, average='macro')
        recall_macro = recall_score(y_test_encoded, y_pred_encoded, average='macro')
        f1_macro = f1_score(y_test_encoded, y_pred_encoded, average='macro')

        precision_weighted = precision_score(y_test_encoded, y_pred_encoded, average='weighted')
        recall_weighted = recall_score(y_test_encoded, y_pred_encoded, average='weighted')
        f1_weighted = f1_score(y_test_encoded, y_pred_encoded, average='weighted')

        print(f"   Precision (macro): {precision_macro:.4f}")
        print(f"   Recall (macro): {recall_macro:.4f}")
        print(f"   F1-Score (macro): {f1_macro:.4f}")
        print(f"   Precision (weighted): {precision_weighted:.4f}")
        print(f"   Recall (weighted): {recall_weighted:.4f}")
        print(f"   F1-Score (weighted): {f1_weighted:.4f}")

        return accuracy

    def _print_confusion_matrix(self, cm, class_names):
        """
        –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
        """
        n_classes = len(class_names)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã
        if cm.shape[0] != n_classes:
            print(f"‚ö†Ô∏è  –†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã ({cm.shape[0]}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–ª–∞—Å—Å–æ–≤ ({n_classes})")
            # –û–±—Ä–µ–∑–∞–µ–º –∏–ª–∏ –¥–æ–ø–æ–ª–Ω—è–µ–º –∏–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–æ —Ä–∞–∑–º–µ—Ä–∞ –º–∞—Ç—Ä–∏—Ü—ã
            if cm.shape[0] < n_classes:
                class_names = class_names[:cm.shape[0]]
            n_classes = cm.shape[0]

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        header = " " * 15 + "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ ‚Üí"
        print(header)

        # –ò–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        pred_header = " " * 10
        for name in class_names[:n_classes]:
            pred_header += f"{str(name)[:8]:^8} "
        print(pred_header)

        # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
        separator = " " * 10 + "‚îÄ" * (n_classes * 9)
        print(separator)

        # –°—Ç—Ä–æ–∫–∏ –º–∞—Ç—Ä–∏—Ü—ã
        for i, true_name in enumerate(class_names[:n_classes]):
            row = f"–ò—Å—Ç–∏–Ω–Ω–æ {str(true_name)[:8]:<8}‚îÇ"
            for j in range(n_classes):
                row += f"{cm[i][j]:^8} "
            print(row)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        diagonal = cm.diagonal()
        total = cm.sum()
        correct = diagonal.sum()

        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: {correct}/{total} ({correct / total:.1%})")

    def _print_confusion_matrix_simple(self, cm, class_names):
        """
        –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
        """
        n_classes = len(class_names)

        print("\n      –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ")
        print("      " + " ".join([f"{name[:4]:>4}" for name in class_names]))
        print("     ‚îå" + "‚îÄ" * (n_classes * 5 + n_classes - 1) + "‚îê")

        for i, true_name in enumerate(class_names):
            row = f"{true_name[:4]:>4} ‚îÇ"
            for j in range(n_classes):
                row += f" {cm[i][j]:>3}"
            row += " ‚îÇ"
            print(row)

            if i < n_classes - 1:
                print("     ‚îú" + "‚îÄ" * (n_classes * 5 + n_classes - 1) + "‚î§")

        print("     ‚îî" + "‚îÄ" * (n_classes * 5 + n_classes - 1) + "‚îò")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        diagonal = cm.diagonal()
        total = cm.sum()
        correct = diagonal.sum()

        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: {correct}/{total} ({correct / total:.1%})")

    def _analyze_ensemble(self):
        """
        –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω—Å–∞–º–±–ª—è
        """
        print(f"\nüìä –ê–ù–ê–õ–ò–ó BAGGING –ê–ù–°–ê–ú–ë–õ–Ø:")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π: {len(self.model.estimators_)}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {self.n_classes}")

        if hasattr(self.model, 'estimators_features_'):
            unique_features_sets = len(set(
                tuple(sorted(features)) for features in self.model.estimators_features_
            ))
            print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {unique_features_sets}")

        # Out-of-bag score
        if hasattr(self.model, 'oob_score_'):
            print(f"   Out-of-bag score: {self.model.oob_score_:.3f}")

    def _show_important_features(self, top_n=10):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ (–¥–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏)
        """
        if self.base_estimator != 'logistic':
            print(f"\n‚ö†Ô∏è  –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ {self.base_estimator}")
            return

        try:
            # –î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã: [n_classes, n_features]
            all_coefs = []
            for estimator in self.model.estimators_:
                if hasattr(estimator, 'coef_'):
                    all_coefs.append(estimator.coef_)  # [n_classes, n_features]

            if not all_coefs:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–æ–¥–µ–ª–µ–π")
                return

            # –£—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –ø–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º –∞–Ω—Å–∞–º–±–ª—è
            avg_coefs = np.mean(all_coefs, axis=0)  # [n_classes, n_features]
            feature_names = self.vectorizer.get_feature_names_out()

            print(f"\nüîç –í–ê–ñ–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò –î–õ–Ø –ö–ê–ñ–î–û–ì–û –ö–õ–ê–°–°–ê:")

            for class_idx, class_name in enumerate(self.class_names):
                print(f"\n   üéØ –ö–õ–ê–°–°: {class_name}")

                # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞
                class_coefs = avg_coefs[class_idx]

                # –¢–æ–ø –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —ç—Ç–æ—Ç –∫–ª–∞—Å—Å)
                pos_indices = np.argsort(class_coefs)[-top_n:][::-1]
                if len(pos_indices) > 0:
                    print(f"      –¢–æ–ø –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
                    for idx in pos_indices[:min(top_n, len(pos_indices))]:
                        print(f"        {feature_names[idx]}: {class_coefs[idx]:.3f}")

                # –¢–æ–ø –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø—Ä–æ—Ç–∏–≤ —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞)
                neg_indices = np.argsort(class_coefs)[:top_n]
                if len(neg_indices) > 0:
                    print(f"      –¢–æ–ø –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
                    for idx in neg_indices[:min(top_n, len(neg_indices))]:
                        print(f"        {feature_names[idx]}: {class_coefs[idx]:.3f}")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")

    def get_ensemble_diversity(self, data):
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∞–Ω—Å–∞–º–±–ª—è –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        X, y = self.prepare_data(data)
        X_vec = self.vectorizer.transform(X)
        y_encoded = self.label_encoder.transform(y)

        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        all_predictions = []

        for i, estimator in enumerate(self.model.estimators_):
            try:
                if hasattr(self.model, 'estimators_features_') and self.model.estimators_features_:
                    features_idx = self.model.estimators_features_[i]
                    X_subset = X_vec[:, features_idx]
                else:
                    X_subset = X_vec

                predictions = estimator.predict(X_subset)
                all_predictions.append(predictions)
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ –º–æ–¥–µ–ª–∏ {i} –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è: {e}")
                continue

        if not all_predictions:
            return {'diversity_score': 0, 'average_disagreement': 0, 'n_models': 0}

        all_predictions = np.array(all_predictions)  # [n_models, n_samples]
        n_models, n_samples = all_predictions.shape

        # –°—á–∏—Ç–∞–µ–º –ø–æ–ø–∞—Ä–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è
        disagreements = 0
        total_pairs = 0

        for i in range(n_models):
            for j in range(i + 1, n_models):
                disagreements += np.sum(all_predictions[i] != all_predictions[j])
                total_pairs += n_samples

        diversity_score = disagreements / total_pairs if total_pairs > 0 else 0

        return {
            'diversity_score': diversity_score,
            'average_disagreement': disagreements / (n_models * (n_models - 1) / 2) if n_models > 1 else 0,
            'n_models': n_models,
            'n_classes': self.n_classes
        }

    def get_class_distribution(self, data):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö
        """
        _, labels = self.prepare_data(data)
        unique, counts = np.unique(labels, return_counts=True)
        return dict(zip(unique, counts))

    def save_model(self, filename):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        joblib.dump({
            'model': self.model,
            'vectorizer': self.vectorizer,
            'label_encoder': self.label_encoder,
            'base_estimator': self.base_estimator,
            'class_names': self.class_names,
            'n_classes': self.n_classes
        }, filename)
        print(f"üíæ Bagging –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")

    def load_model(self, filename):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        loaded = joblib.load(filename)
        self.model = loaded['model']
        self.vectorizer = loaded['vectorizer']
        self.label_encoder = loaded['label_encoder']
        self.base_estimator = loaded.get('base_estimator', 'logistic')
        self.class_names = loaded.get('class_names', [])
        self.n_classes = loaded.get('n_classes', 0)
        self.is_trained = True
        print(f"üì• Bagging –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filename}")


# –°–ü–ï–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† –î–õ–Ø –¢–û–ù–ê–õ–¨–ù–û–°–¢–ò (3 –ö–õ–ê–°–°–ê)
class SentimentBaggingClassifier(BaggingMultiClassClassifier):
    """
    –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Bagging –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (–Ω–µ–≥–∞—Ç–∏–≤, –Ω–µ–π—Ç—Ä–∞–ª, –ø–æ–∑–∏—Ç–∏–≤)
    """

    def __init__(self, base_estimator='logistic', n_estimators=15,
                 max_samples=0.8, max_features=0.8, bootstrap=True,
                 random_state=42):
        super().__init__(
            base_estimator=base_estimator,
            n_estimators=n_estimators,
            max_samples=max_samples,
            max_features=max_features,
            bootstrap=bootstrap,
            random_state=random_state
        )

    def predict_sentiment(self, text):
        """
        –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        """
        result = super().predict_single(text)

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –¥–ª—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        sentiment_mapping = {
            'negative': '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π',
            'neutral': '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π',
            'positive': '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π',
            'neg': '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π',
            'neu': '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π',
            'pos': '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π',
            '0': '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π',
            '1': '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π',
            '2': '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π'
        }

        prediction = result['prediction']
        sentiment = sentiment_mapping.get(str(prediction).lower(), prediction)

        result['sentiment'] = sentiment
        result['sentiment_confidence'] = result['confidence']

        # –î–µ—Ç–∞–ª–∏ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è –¥–ª—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        voting_details = self.predict_with_voting_details([text])[0]
        result['voting_details'] = voting_details

        return result


# –°–†–ê–í–ù–ï–ù–ò–ï –†–ê–ó–ù–´–• –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ô BAGGING
def compare_bagging_configs_multiclass(train_data, val_data):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π Bagging –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    """
    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ô BAGGING (–º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π)")
    print("=" * 50)

    models = {}

    # 1. Bagging —Å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π
    configs = [
        ('logistic', 10, 0.8, 0.8),
        ('logistic', 20, 0.8, 0.8),
        ('logistic', 10, 0.6, 0.6),
    ]

    for base_est, n_est, max_samp, max_feat in configs:
        print(f"\n1. Bagging {base_est} (n_est={n_est}, samples={max_samp}, features={max_feat}):")
        model = BaggingMultiClassClassifier(
            base_estimator=base_est,
            n_estimators=n_est,
            max_samples=max_samp,
            max_features=max_feat
        )
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º try-except –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
        try:
            model.train(train_data, val_data)
            models[f'Bagging_{base_est}_{n_est}'] = model
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
            continue

    # 2. Bagging —Å –¥–µ—Ä–µ–≤—å—è–º–∏ —Ä–µ—à–µ–Ω–∏–π
    print(f"\n2. Bagging —Å Decision Trees:")
    model_tree = BaggingMultiClassClassifier(
        base_estimator='tree',
        n_estimators=15,
        max_samples=0.7,
        max_features=0.7
    )
    try:
        model_tree.train(train_data, val_data)
        models['Bagging_tree'] = model_tree
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")

    return models


# –ê–ù–ê–õ–ò–ó –°–¢–ê–ë–ò–õ–¨–ù–û–°–¢–ò –ê–ù–°–ê–ú–ë–õ–Ø
def analyze_ensemble_stability_multiclass(model, data):
    """
    –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∞–Ω—Å–∞–º–±–ª—è –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    """
    print(f"\nüìä –ê–ù–ê–õ–ò–ó –°–¢–ê–ë–ò–õ–¨–ù–û–°–¢–ò –ê–ù–°–ê–ú–ë–õ–Ø (–º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π):")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (—á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å)
    sample_data = data[:20] if len(data) > 20 else data

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –¥–µ—Ç–∞–ª—è–º–∏ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è
    try:
        results = model.predict_with_voting_details([item['text'] for item in sample_data])
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏: {e}")
        return

    unanimous_count = sum(1 for r in results if r['unanimous'])
    high_consensus = sum(1 for r in results if r['consensus_ratio'] >= 0.8)
    low_consensus = sum(1 for r in results if r['consensus_ratio'] < 0.5)

    print(f"   –ï–¥–∏–Ω–æ–≥–ª–∞—Å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è: {unanimous_count}/{len(results)} ({unanimous_count / len(results) * 100:.1f}%)")
    print(f"   –í—ã—Å–æ–∫–∏–π –∫–æ–Ω—Å–µ–Ω—Å—É—Å (‚â•80%): {high_consensus}/{len(results)} ({high_consensus / len(results) * 100:.1f}%)")
    print(f"   –ù–∏–∑–∫–∏–π –∫–æ–Ω—Å–µ–Ω—Å—É—Å (<50%): {low_consensus}/{len(results)} ({low_consensus / len(results) * 100:.1f}%)")

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–æ–ª–æ—Å–æ–≤ –ø–æ –∫–ª–∞—Å—Å–∞–º
    print(f"\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ì–û–õ–û–°–û–í:")
    all_votes = {}
    for class_name in model.class_names:
        all_votes[class_name] = 0

    for result in results:
        for class_name, votes in result['votes'].items():
            all_votes[class_name] += votes

    total_votes = sum(all_votes.values())
    for class_name, votes in all_votes.items():
        if total_votes > 0:
            percentage = (votes / total_votes) * 100
            print(f"   {class_name}: {votes} –≥–æ–ª–æ—Å–æ–≤ ({percentage:.1f}%)")

    # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∞–Ω—Å–∞–º–±–ª—è
    diversity = model.get_ensemble_diversity(sample_data)
    print(f"\nüìä –†–ê–ó–ù–û–û–ë–†–ê–ó–ò–ï –ê–ù–°–ê–ú–ë–õ–Ø:")
    print(f"   Score —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è: {diversity['diversity_score']:.3f}")
    if diversity['n_models'] > 1:
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –Ω–µ—Å–æ–≥–ª–∞—Å–∏–µ: {diversity['average_disagreement']:.1f} –ø–∞—Ä –Ω–∞ –º–æ–¥–µ–ª—å")


# –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø
def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–≥–æ Bagging –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    """

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    def load_jsonl(filepath):
        with open(filepath, 'r', encoding='utf-8') as f:
            return [json.loads(line) for line in f]

    train_data = load_jsonl('../util/news_category_train.jsonl')
    test_data = load_jsonl('../util/news_category_test.jsonl')

    print(f"üìä –û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(train_data)}")
    print(f"üìä –¢–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(test_data)}")

    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val
    np.random.seed(42)
    indices = np.random.permutation(len(train_data))
    split_idx = int(0.8 * len(train_data))

    train_indices = indices[:split_idx]
    val_indices = indices[split_idx:]

    train_subset = [train_data[i] for i in train_indices]
    val_subset = [train_data[i] for i in val_indices]

    print(f"üìä Train: {len(train_subset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"üìä Val: {len(val_subset)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º Bagging —Å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π
    print("\n" + "=" * 50)
    print("üéØ –û–ë–£–ß–ï–ù–ò–ï BAGGING (–º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π)")
    print("=" * 50)

    bagging_classifier = BaggingMultiClassClassifier(
        base_estimator='logistic',
        n_estimators=10,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        max_samples=0.8,
        max_features=0.8,
        bootstrap=True
    )

    bagging_classifier.train(train_subset, val_subset)

    # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∞–Ω—Å–∞–º–±–ª—è
    analyze_ensemble_stability_multiclass(bagging_classifier, val_subset[:20])

    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\nüß™ –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•...")
    test_accuracy = bagging_classifier.evaluate(test_data)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    bagging_classifier.save_model("bagging_multiclass_model.pkl")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–µ—Å—Ç–∏—Ä—É–µ–º
    print("\nüß™ –¢–ï–°–¢ –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ò...")
    loaded_classifier = BaggingMultiClassClassifier()
    loaded_classifier.load_model("bagging_multiclass_model.pkl")

    # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    test_text = "–î–æ–≤–æ–ª—å–Ω–æ –Ω–µ–ø–ª–æ—Ö–æ, –Ω–æ –µ—Å—Ç—å –Ω–µ–±–æ–ª—å—à–∏–µ –∑–∞–º–µ—á–∞–Ω–∏—è"
    result = loaded_classifier.predict_single(test_text)
    print(f"\nüìù –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:")
    print(f"   –¢–µ–∫—Å—Ç: {test_text}")
    print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {result['prediction']}")
    print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.3f}")

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö)
    print("\n" + "=" * 50)
    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ô")
    print("=" * 50)

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    small_train = train_subset[:60]
    small_val = val_subset[:15]

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
    if len(small_train) >= 10 and len(small_val) >= 5:
        models = compare_bagging_configs_multiclass(small_train, small_val)
    else:
        print("‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π")

    return bagging_classifier


if __name__ == "__main__":
    print("üöÄ –ó–ê–ü–£–°–ö –ú–ù–û–ì–û–ö–õ–ê–°–°–û–í–û–ì–û BAGGING –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê")
    print("=" * 80)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–∏–º–µ—Ä
    classifier = main()