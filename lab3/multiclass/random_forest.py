from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import numpy as np
import joblib
import seaborn as sns
import matplotlib.pyplot as plt
from typing import List, Dict, Any, Optional, Tuple, Union
from collections import Counter
import pandas as pd
import warnings

warnings.filterwarnings('ignore')

from util.jsonl_process import read_jsonl_basic


class RandomForestCategoryClassifier:
    """
    –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞
    """

    def __init__(self,
                 n_estimators: int = 100,
                 max_depth: Optional[int] = None,
                 max_features: Union[str, int, float] = 'sqrt',
                 random_state: int = 42,
                 class_names: Optional[List[str]] = None,
                 text_field: str = 'text',
                 label_field: str = 'category',
                 class_weight: Optional[str] = None,
                 min_samples_split: int = 2,
                 min_samples_leaf: int = 1):
        """
        Args:
            n_estimators: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ –≤ –ª–µ—Å—É
            max_depth: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤
            max_features: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è –≤ –∫–∞–∂–¥–æ–º —Ä–∞–∑–±–∏–µ–Ω–∏–∏
            random_state: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            class_names: —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∞—Å—Å–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            text_field: –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—è —Å —Ç–µ–∫—Å—Ç–æ–º
            label_field: –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—è —Å –º–µ—Ç–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            class_weight: –≤–µ—Å –∫–ª–∞—Å—Å–æ–≤ ('balanced', 'balanced_subsample' –∏–ª–∏ None)
            min_samples_split: –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —É–∑–ª–∞
            min_samples_leaf: –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –≤ –ª–∏—Å—Ç–µ
        """
        self.vectorizer = TfidfVectorizer(
            max_features=10000,
            min_df=2,
            max_df=0.9,
            ngram_range=(1, 2),
            stop_words=None
        )

        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            max_features=max_features,
            random_state=random_state,
            n_jobs=-1,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —è–¥—Ä–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
            bootstrap=True,
            oob_score=True,  # Out-of-bag score –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            class_weight=class_weight,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            verbose=0
        )

        self.label_encoder = LabelEncoder()
        self.class_names = class_names
        self.is_trained = False
        self.num_classes = 0
        self.class_mapping = {}
        self.text_field = text_field
        self.label_field = label_field
        self.random_state = random_state
        self.all_classes_fitted = []  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –∫–ª–∞—Å—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

    def prepare_data(self, data: List[Dict[str, Any]]) -> Tuple[List[str], List[str]]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        """
        texts = [item[self.text_field] for item in data]
        labels = [item[self.label_field] for item in data]
        return texts, labels

    def analyze_class_distribution(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö
        """
        _, labels = self.prepare_data(data)
        label_counts = Counter(labels)

        result = {
            'total_samples': len(data),
            'num_classes': len(label_counts),
            'classes': dict(label_counts),
            'class_percentages': {},
            'imbalance_ratio': None,
            'unique_labels': sorted(list(label_counts.keys()))
        }

        if label_counts:
            max_count = max(label_counts.values())
            min_count = min(label_counts.values())
            if min_count > 0:
                result['imbalance_ratio'] = max_count / min_count

            for label, count in label_counts.items():
                result['class_percentages'][label] = count / len(data) * 100

        return result

    def train(self, train_data: List[Dict[str, Any]],
              val_data: Optional[List[Dict[str, Any]]] = None,
              auto_detect_classes: bool = True) -> None:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞
        """
        print("üéØ –û–ë–£–ß–ï–ù–ò–ï –ú–ù–û–ì–û–ö–õ–ê–°–°–û–í–û–ì–û –°–õ–£–ß–ê–ô–ù–û–ì–û –õ–ï–°–ê...")
        print(f"   –ü–æ–ª–µ —Å —Ç–µ–∫—Å—Ç–æ–º: '{self.text_field}'")
        print(f"   –ü–æ–ª–µ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π: '{self.label_field}'")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤: {self.model.n_estimators}")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞: {self.model.max_depth}")

        # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        train_dist = self.analyze_class_distribution(train_data)
        print(f"\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–ê–¢–ï–ì–û–†–ò–ô –í TRAIN:")
        print(f"   –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {train_dist['total_samples']}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {train_dist['num_classes']}")

        if train_dist['imbalance_ratio']:
            print(f"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞: {train_dist['imbalance_ratio']:.2f}")
            if train_dist['imbalance_ratio'] > 3 and self.model.class_weight is None:
                print("   ‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω —Å–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
                print("   ‚ÑπÔ∏è  –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å class_weight='balanced'")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train_raw = self.prepare_data(train_data)

        # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
        if auto_detect_classes:
            self.label_encoder.fit(y_train_raw)
            y_train = self.label_encoder.transform(y_train_raw)
            self.class_names = list(self.label_encoder.classes_)
        else:
            if self.class_names is None:
                raise ValueError("class_names –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∑–∞–¥–∞–Ω, –µ—Å–ª–∏ auto_detect_classes=False")
            self.label_encoder.fit(self.class_names)
            y_train = self.label_encoder.transform(y_train_raw)

        self.num_classes = len(self.class_names)
        self.class_mapping = {i: cls for i, cls in enumerate(self.class_names)}
        self.all_classes_fitted = self.class_names.copy()  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –∫–ª–∞—Å—Å—ã –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

        # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö
        print(f"\nüìã –°–ü–ò–°–û–ö –ö–ê–¢–ï–ì–û–†–ò–ô ({self.num_classes}):")
        for i, (class_name, count) in enumerate(train_dist['classes'].items()):
            percentage = train_dist['class_percentages'].get(class_name, 0)
            print(f"   {i + 1:2d}. {class_name}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤ ({percentage:.1f}%)")

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
        print("\nüìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        X_train_vec = self.vectorizer.fit_transform(X_train)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_vec.shape}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤/—Ñ—Ä–∞–∑: {len(self.vectorizer.get_feature_names_out())}")
        print(f"   –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã: {X_train_vec.nnz / (X_train_vec.shape[0] * X_train_vec.shape[1]):.4f}")

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("\nü§ñ –û–±—É—á–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞...")
        self.model.fit(X_train_vec, y_train)
        self.is_trained = True

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        train_pred = self.model.predict(X_train_vec)
        train_accuracy = accuracy_score(y_train, train_pred)
        print(f"\n‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train: {train_accuracy:.3f}")

        # Out-of-bag score
        if hasattr(self.model, 'oob_score_'):
            print(f"‚úÖ Out-of-bag score: {self.model.oob_score_:.3f}")

        # –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º –Ω–∞ train
        print("\nüìä –û–¢–ß–ï–¢ –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú (train):")
        print(classification_report(y_train, train_pred, target_names=self.class_names))

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        if val_data:
            val_accuracy, _ = self.evaluate(val_data, detailed=False)
            print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ val: {val_accuracy:.3f}")

        # –ü–æ–∫–∞–∂–µ–º –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        self._show_important_features(top_n=20)

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        self._show_model_info()

        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –Ω–∞ train
        self._plot_confusion_matrix(y_train, train_pred, "Train Confusion Matrix")

    def predict(self, texts: List[str]) -> Tuple[List[str], np.ndarray]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)
        predictions_encoded = self.model.predict(X_vec)
        predictions = self.label_encoder.inverse_transform(predictions_encoded)
        probabilities = self.model.predict_proba(X_vec)

        return predictions, probabilities

    def predict_single(self, text: str) -> Dict[str, Any]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        predictions, probabilities = self.predict([text])
        pred = predictions[0]
        pred_encoded = self.label_encoder.transform([pred])[0]
        prob = probabilities[0]

        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        class_probs = {}
        for i, cls in enumerate(self.class_names):
            class_probs[cls] = prob[i]

        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-3 –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        top_n = min(3, self.num_classes)
        top_indices = np.argsort(prob)[-top_n:][::-1]
        top_categories = []
        for idx in top_indices:
            top_categories.append({
                'category': self.class_names[idx],
                'probability': prob[idx],
                'probability_percent': prob[idx] * 100
            })

        return {
            'prediction': pred,
            'category': pred,
            'prediction_encoded': pred_encoded,
            'category_probabilities': class_probs,
            'top_categories': top_categories,
            'confidence': prob[pred_encoded],
            'confidence_percent': prob[pred_encoded] * 100
        }

    def _safe_transform_labels(self, labels_raw: List[str]) -> np.ndarray:
        """
        –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫, —É—á–∏—Ç—ã–≤–∞—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–ª–∞—Å—Å—ã
        """
        try:
            return self.label_encoder.transform(labels_raw)
        except ValueError as e:
            # –ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –º–µ—Ç–∫–∏, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –≤ –¥–∞–Ω–Ω—ã—Ö –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏—Å—å –Ω–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±—ã–ª–æ –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ")

            # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–µ—Ç–æ–∫
            known_labels = []
            unknown_count = 0
            for label in labels_raw:
                if label in self.label_encoder.classes_:
                    known_labels.append(label)
                else:
                    known_labels.append(self.class_names[0])  # –ó–∞–º–µ–Ω—è–µ–º –Ω–∞ –ø–µ—Ä–≤—ã–π –∫–ª–∞—Å—Å
                    unknown_count += 1

            if unknown_count > 0:
                print(f"   –ó–∞–º–µ–Ω–µ–Ω–æ {unknown_count} –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–µ—Ç–æ–∫ –Ω–∞ '{self.class_names[0]}'")

            return self.label_encoder.transform(known_labels)

    def evaluate(self, test_data: List[Dict[str, Any]],
                 detailed: bool = True,
                 plot_confusion_matrix: bool = True) -> Tuple[float, Dict]:
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        X_test, y_test_raw = self.prepare_data(test_data)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –µ—Å—Ç—å –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        test_dist = self.analyze_class_distribution(test_data)
        test_classes = set(test_dist['unique_labels'])
        train_classes = set(self.class_names)

        missing_in_test = train_classes - test_classes
        missing_in_train = test_classes - train_classes

        if missing_in_train:
            print(f"‚ö†Ô∏è  –í —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –µ—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±—ã–ª–æ –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ: {missing_in_train}")
            print(f"   –≠—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –±—É–¥—É—Ç –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω—ã –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ")

        if missing_in_test:
            print(f"‚ÑπÔ∏è  –í —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏: {missing_in_test}")

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
        y_test = self._safe_transform_labels(y_test_raw)
        X_test_vec = self.vectorizer.transform(X_test)

        y_pred_encoded = self.model.predict(X_test_vec)
        y_pred = self.label_encoder.inverse_transform(y_pred_encoded)
        accuracy = accuracy_score(y_test, y_pred_encoded)

        if detailed:
            print(f"\nüìä –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•:")
            print(f"   –ü—Ä–∏–º–µ—Ä–æ–≤: {len(test_data)}")
            print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–π –≤ —Ç–µ—Å—Ç–µ: {test_dist['num_classes']}")
            print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")

            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –∫–ª–∞—Å—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –∏ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö –∏ –≤ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–µ—Ç–∫–∞—Ö
            unique_true = np.unique(y_test)
            unique_pred = np.unique(y_pred_encoded)
            common_classes = np.intersect1d(unique_true, unique_pred)

            if len(common_classes) > 0:
                # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ç–æ–ª—å–∫–æ –æ–±—â–∏—Ö –∫–ª–∞—Å—Å–æ–≤
                mask = np.isin(y_test, common_classes) & np.isin(y_pred_encoded, common_classes)

                if np.sum(mask) > 0:
                    y_test_filtered = y_test[mask]
                    y_pred_filtered = y_pred_encoded[mask]

                    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—â–∏—Ö –∫–ª–∞—Å—Å–æ–≤
                    available_classes = self.label_encoder.inverse_transform(common_classes)

                    print(f"\nüìà –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú (—Ç–æ–ª—å–∫–æ –æ–±—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏):")
                    print(classification_report(
                        y_test_filtered,
                        y_pred_filtered,
                        target_names=available_classes,
                        digits=3
                    ))
                else:
                    print("‚ùå –ù–µ—Ç –æ–±—â–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
            else:
                print("‚ùå –ù–µ—Ç –æ–±—â–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏")

            # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—â–∏—Ö –∫–ª–∞—Å—Å–æ–≤
            if len(common_classes) > 0:
                print(f"\nüìä –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö (—Ç–æ–ª—å–∫–æ –æ–±—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏):")
                cm = confusion_matrix(y_test, y_pred_encoded, labels=common_classes)
                self._print_confusion_matrix_custom(cm, common_classes)

                if plot_confusion_matrix and len(common_classes) > 1:
                    self._plot_confusion_matrix_custom(
                        y_test, y_pred_encoded, common_classes,
                        "Test Confusion Matrix (Common Classes Only)"
                    )

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        report_dict = {}
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –æ—Ç—á–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—â–∏—Ö –∫–ª–∞—Å—Å–æ–≤
            unique_true = np.unique(y_test)
            unique_pred = np.unique(y_pred_encoded)
            common_classes = np.intersect1d(unique_true, unique_pred)

            if len(common_classes) > 0:
                mask = np.isin(y_test, common_classes) & np.isin(y_pred_encoded, common_classes)
                if np.sum(mask) > 0:
                    y_test_filtered = y_test[mask]
                    y_pred_filtered = y_pred_encoded[mask]
                    available_classes = self.label_encoder.inverse_transform(common_classes)

                    report_dict = classification_report(
                        y_test_filtered,
                        y_pred_filtered,
                        target_names=available_classes,
                        output_dict=True
                    )
        except Exception as e:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç: {e}")

        return accuracy, report_dict

    def _print_confusion_matrix_custom(self, cm: np.ndarray, classes: np.ndarray) -> None:
        """
        –ü–µ—á–∞—Ç—å –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –¥–ª—è –∑–∞–¥–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
        """
        n_classes = len(classes)
        class_names = self.label_encoder.inverse_transform(classes)

        if n_classes == 0:
            print("‚ùå –ù–µ—Ç –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫")
            return

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        max_class_len = max(len(cls) for cls in class_names)
        header_padding = max(12, max_class_len + 2)

        header = " " * header_padding + " | "
        header += " ".join([f"{cls[:10]:>10}" for cls in class_names])
        print(header)
        print("-" * (header_padding + 3 + n_classes * 11))

        # –°—Ç—Ä–æ–∫–∏
        for i, cls in enumerate(class_names):
            row = f"{cls[:header_padding - 2]:>{header_padding - 2}} | "
            row += " ".join([f"{cm[i][j]:>10}" for j in range(n_classes)])
            print(row)

    def _plot_confusion_matrix_custom(self, y_true: np.ndarray,
                                      y_pred: np.ndarray,
                                      classes: np.ndarray,
                                      title: str = "Confusion Matrix") -> None:
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –¥–ª—è –∑–∞–¥–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
        """
        try:
            if len(classes) <= 1:
                print("‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫")
                return

            cm = confusion_matrix(y_true, y_pred, labels=classes)
            class_names = self.label_encoder.inverse_transform(classes)

            plt.figure(figsize=(max(8, len(classes)), max(6, len(classes) * 0.7)))

            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º (–ø–æ –∏—Å—Ç–∏–Ω–Ω—ã–º –º–µ—Ç–∫–∞–º)
            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            cm_normalized = np.nan_to_num(cm_normalized)  # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ 0

            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                        xticklabels=class_names,
                        yticklabels=class_names,
                        vmin=0, vmax=1)
            plt.title(f"{title} (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–∞)")
            plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')
            plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')
            plt.tight_layout()

            filename = title.lower().replace(' ', '_').replace('(', '').replace(')', '')
            plt.savefig(f"{filename}.png", dpi=300, bbox_inches='tight')
            plt.show()
        except Exception as e:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫: {e}")

    def _plot_confusion_matrix(self, y_true: np.ndarray,
                               y_pred: np.ndarray,
                               title: str = "Confusion Matrix") -> None:
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –¥–ª—è –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤
        """
        try:
            if self.num_classes <= 1:
                print("‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫")
                return

            cm = confusion_matrix(y_true, y_pred)

            plt.figure(figsize=(max(10, self.num_classes), max(8, self.num_classes * 0.8)))

            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º (–ø–æ –∏—Å—Ç–∏–Ω–Ω—ã–º –º–µ—Ç–∫–∞–º)
            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            cm_normalized = np.nan_to_num(cm_normalized)  # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ 0

            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                        xticklabels=self.class_names,
                        yticklabels=self.class_names,
                        vmin=0, vmax=1)
            plt.title(f"{title} (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–∞)")
            plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')
            plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')
            plt.tight_layout()

            filename = title.lower().replace(' ', '_').replace('-', '_')
            plt.savefig(f"{filename}.png", dpi=300, bbox_inches='tight')
            plt.show()
        except Exception as e:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫: {e}")

    def _show_important_features(self, top_n: int = 20):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        """
        if not hasattr(self.model, 'feature_importances_'):
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            return

        feature_names = self.vectorizer.get_feature_names_out()
        importances = self.model.feature_importances_

        print(f"\nüîç –¢–û–ü-{top_n} –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í (Random Forest):")

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        indices = np.argsort(importances)[::-1]

        print(f"\n   –°–ê–ú–´–ï –í–ê–ñ–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò –î–õ–Ø –í–°–ï–• –ö–ê–¢–ï–ì–û–†–ò–ô:")
        for i in range(min(top_n, len(indices))):
            idx = indices[i]
            print(f"      {i + 1:2d}. {feature_names[idx]:20s}: {importances[idx]:.5f}")

        # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (–∫–æ—Å–≤–µ–Ω–Ω—ã–π)
        print(f"\n   üìä –û–ë–©–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:")
        total_importance = np.sum(importances)
        top_n_importance = np.sum(importances[indices[:top_n]])
        print(f"      –¢–æ–ø-{top_n} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–±—ä—è—Å–Ω—è—é—Ç {top_n_importance / total_importance * 100:.1f}% –æ–±—â–µ–π –≤–∞–∂–Ω–æ—Å—Ç–∏")

        # –î–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω—É–ª–µ–≤–æ–π –≤–∞–∂–Ω–æ—Å—Ç—å—é
        zero_importance_count = np.sum(importances == 0)
        print(
            f"      –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω—É–ª–µ–≤–æ–π –≤–∞–∂–Ω–æ—Å—Ç—å—é: {zero_importance_count} ({zero_importance_count / len(importances) * 100:.1f}%)")

    def _show_model_info(self):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        print(f"\nüìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –°–õ–£–ß–ê–ô–ù–û–ú –õ–ï–°–ï:")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤: {len(self.model.estimators_)}")

        # –ì–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤
        depths = [est.tree_.max_depth for est in self.model.estimators_]
        if depths:
            print(f"   –ì–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤: {np.min(depths)} (–º–∏–Ω), {np.mean(depths):.1f} (—Å—Ä), {np.max(depths)} (–º–∞–∫—Å)")

        # –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç—å–µ–≤
        n_leaves = [est.tree_.n_leaves for est in self.model.estimators_]
        if n_leaves:
            print(f"   –õ–∏—Å—Ç—å—è –≤ –¥–µ—Ä–µ–≤–µ: {np.mean(n_leaves):.0f} (–≤ —Å—Ä–µ–¥–Ω–µ–º)")

        if hasattr(self.model, 'oob_score_'):
            print(f"   Out-of-bag score: {self.model.oob_score_:.3f}")

    def get_feature_importance_df(self, top_n: int = 50) -> Optional[pd.DataFrame]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame —Å –≤–∞–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        if not hasattr(self.model, 'feature_importances_'):
            return None

        feature_names = self.vectorizer.get_feature_names_out()
        importances = self.model.feature_importances_

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        indices = np.argsort(importances)[::-1]

        data = {
            'feature': feature_names[indices[:top_n]],
            'importance': importances[indices[:top_n]],
            'rank': range(1, top_n + 1)
        }

        return pd.DataFrame(data)

    def save_model(self, filename: str) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        joblib.dump({
            'model': self.model,
            'vectorizer': self.vectorizer,
            'label_encoder': self.label_encoder,
            'class_names': self.class_names,
            'class_mapping': self.class_mapping,
            'num_classes': self.num_classes,
            'all_classes_fitted': self.all_classes_fitted,
            'text_field': self.text_field,
            'label_field': self.label_field,
            'random_state': self.random_state
        }, filename)
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")

    def load_model(self, filename: str) -> None:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        loaded = joblib.load(filename)
        self.model = loaded['model']
        self.vectorizer = loaded['vectorizer']
        self.label_encoder = loaded['label_encoder']
        self.class_names = loaded['class_names']
        self.class_mapping = loaded['class_mapping']
        self.num_classes = loaded['num_classes']
        self.all_classes_fitted = loaded.get('all_classes_fitted', self.class_names)
        self.text_field = loaded.get('text_field', 'text')
        self.label_field = loaded.get('label_field', 'category')
        self.random_state = loaded.get('random_state', 42)
        self.is_trained = True

        print(f"üì• –ú–æ–¥–µ–ª—å —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filename}")
        print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {self.class_names}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {self.num_classes}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤: {self.model.n_estimators}")

    def predict_batch_with_details(self, texts: List[str]) -> List[Dict[str, Any]]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        predictions, probabilities = self.predict(texts)

        results = []
        for i, (text, pred, probs) in enumerate(zip(texts, predictions, probabilities)):
            pred_encoded = self.label_encoder.transform([pred])[0]

            # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            top_n = min(3, self.num_classes)
            top_indices = np.argsort(probs)[-top_n:][::-1]
            top_categories = []
            for idx in top_indices:
                top_categories.append({
                    'category': self.class_names[idx],
                    'probability': probs[idx],
                    'probability_percent': probs[idx] * 100
                })

            results.append({
                'text': text,
                'prediction': pred,
                'predicted_category': pred,
                'confidence': probs[pred_encoded],
                'confidence_percent': probs[pred_encoded] * 100,
                'top_categories': top_categories
            })

        return results


def compare_rf_parameters(train_data: List[Dict[str, Any]],
                          val_data: Optional[List[Dict[str, Any]]] = None,
                          text_field: str = 'text',
                          label_field: str = 'category',
                          use_subset: bool = True) -> Dict[str, RandomForestCategoryClassifier]:
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞ (–±–µ–∑–æ–ø–∞—Å–Ω–∞—è –≤–µ—Ä—Å–∏—è)
    """
    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –ü–ê–†–ê–ú–ï–¢–†–û–í –°–õ–£–ß–ê–ô–ù–û–ì–û –õ–ï–°–ê")
    print("=" * 60)

    models = {}

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if use_subset and len(train_data) > 300:
        train_subset = train_data[:300]
        print(f"‚ÑπÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏–∑ {len(train_subset)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
    else:
        train_subset = train_data

    if use_subset and val_data and len(val_data) > 100:
        val_subset = val_data[:100]
        print(f"‚ÑπÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏–∑ {len(val_subset)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
    else:
        val_subset = val_data

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª–∞—Å—Å—ã –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    all_labels = [item[label_field] for item in train_subset]
    class_names = sorted(list(set(all_labels)))

    if len(class_names) < 2:
        print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–º–∏–Ω–∏–º—É–º 2)")
        return models

    # 1. –†–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ (—Ç–æ–ª—å–∫–æ 2 –≤–∞—Ä–∏–∞–Ω—Ç–∞ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
    print("\n1. –°–†–ê–í–ù–ï–ù–ò–ï –†–ê–ó–ù–û–ì–û –ö–û–õ–ò–ß–ï–°–¢–í–ê –î–ï–†–ï–í–¨–ï–í:")
    for n_trees in [50, 100]:  # –£–º–µ–Ω—å—à–∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
        print(f"\n   Random Forest —Å {n_trees} –¥–µ—Ä–µ–≤—å—è–º–∏:")
        try:
            model = RandomForestCategoryClassifier(
                n_estimators=n_trees,
                max_depth=None,
                class_names=class_names,
                text_field=text_field,
                label_field=label_field,
                class_weight=None
            )
            model.train(train_subset, val_subset, auto_detect_classes=False)
            models[f'RF_{n_trees}_trees'] = model
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å {n_trees} –¥–µ—Ä–µ–≤—å—è–º–∏: {e}")

    # 2. –†–∞–∑–Ω–∞—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ (—Ç–æ–ª—å–∫–æ 2 –≤–∞—Ä–∏–∞–Ω—Ç–∞)
    print("\n2. –°–†–ê–í–ù–ï–ù–ò–ï –†–ê–ó–ù–û–ô –ì–õ–£–ë–ò–ù–´ –î–ï–†–ï–í–¨–ï–í:")
    for depth in [10, None]:  # –£–º–µ–Ω—å—à–∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
        depth_name = "None" if depth is None else depth
        print(f"\n   Random Forest —Å max_depth={depth_name}:")
        try:
            model = RandomForestCategoryClassifier(
                n_estimators=50,  # –ú–µ–Ω—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                max_depth=depth,
                class_names=class_names,
                text_field=text_field,
                label_field=label_field,
                class_weight=None
            )
            model.train(train_subset, val_subset, auto_detect_classes=False)
            models[f'RF_depth_{depth_name}'] = model
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å max_depth={depth_name}: {e}")

    return models


def analyze_feature_importance(model: RandomForestCategoryClassifier,
                               top_n: int = 30,
                               save_to_csv: bool = True) -> None:
    """
    –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    """
    importance_df = model.get_feature_importance_df(top_n=top_n)

    if importance_df is not None:
        print(f"\nüìà –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í (–¢–æ–ø-{top_n}):")
        print("=" * 60)

        # –í—ã–≤–æ–¥–∏–º —Ç–∞–±–ª–∏—Ü—É
        pd.set_option('display.max_rows', top_n)
        print(importance_df.to_string(index=False))
        pd.reset_option('display.max_rows')

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
        if save_to_csv:
            csv_filename = f"feature_importance_{len(model.class_names)}_categories.csv"
            importance_df.to_csv(csv_filename, index=False)
            print(f"\nüíæ –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {csv_filename}")

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        try:
            plt.figure(figsize=(12, 8))
            plt.barh(range(top_n), importance_df['importance'].values[:top_n][::-1])
            plt.yticks(range(top_n), importance_df['feature'].values[:top_n][::-1])
            plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞')
            plt.title(f'–¢–æ–ø-{top_n} –≤–∞–∂–Ω–µ–π—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {len(model.class_names)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π')
            plt.tight_layout()
            plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
            plt.show()
        except Exception as e:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")


def quick_train_rf(train_file: str,
                   val_file: Optional[str] = None,
                   test_file: Optional[str] = None,
                   text_field: str = 'text',
                   label_field: str = 'category',
                   n_estimators: int = 100,
                   max_depth: Optional[int] = None,
                   class_weight: Optional[str] = None,
                   output_model: str = 'rf_category_classifier.pkl',
                   use_subset_for_training: bool = False) -> Optional[RandomForestCategoryClassifier]:
    """
    –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞ –∏–∑ —Ñ–∞–π–ª–æ–≤
    """
    import json
    import os

    def load_jsonl(filepath: str) -> List[Dict[str, Any]]:
        if not os.path.exists(filepath):
            print(f"‚ö†Ô∏è  –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
            return []
        with open(filepath, 'r', encoding='utf-8') as f:
            return [json.loads(line) for line in f]

    print("üöÄ –ó–ê–ü–£–°–ö –ë–´–°–¢–†–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø –°–õ–£–ß–ê–ô–ù–û–ì–û –õ–ï–°–ê")
    print("=" * 60)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    train_data = load_jsonl(train_file)
    if not train_data:
        print(f"‚ùå –û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {train_file}")
        return None

    print(f"   Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if use_subset_for_training and len(train_data) > 1000:
        train_data = train_data[:1000]
        print(f"   ‚ÑπÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏–∑ {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")

    if val_file:
        val_data = load_jsonl(val_file)
        print(f"   Val: {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    else:
        val_data = None

    if test_file:
        test_data = load_jsonl(test_file)
        print(f"   Test: {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    else:
        test_data = None

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
    if train_data:
        sample_item = train_data[0]
        if text_field not in sample_item:
            print(f"‚ùå –û—à–∏–±–∫–∞: –ø–æ–ª–µ '{text_field}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–∞–Ω–Ω—ã—Ö")
            return None
        if label_field not in sample_item:
            print(f"‚ùå –û—à–∏–±–∫–∞: –ø–æ–ª–µ '{label_field}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–∞–Ω–Ω—ã—Ö")
            return None

    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    print(f"\nüéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞...")
    print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {n_estimators} –¥–µ—Ä–µ–≤—å–µ–≤, max_depth={max_depth}")

    classifier = RandomForestCategoryClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        class_weight=class_weight,
        text_field=text_field,
        label_field=label_field
    )

    classifier.train(train_data, val_data, auto_detect_classes=True)

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º, –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    if test_data:
        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        accuracy, report = classifier.evaluate(test_data, detailed=True)
        print(f"\nüéØ –ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {accuracy:.3f}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        if report:
            report_df = pd.DataFrame(report).transpose()
            report_df.to_csv('rf_classification_report.csv', index=True)
            print(f"üìÑ –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ 'rf_classification_report.csv'")

    # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    analyze_feature_importance(classifier, top_n=25, save_to_csv=True)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    classifier.save_model(output_model)

    # –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä
    print(f"\nüß™ –¢–ï–°–¢–û–í–´–ô –ü–†–ò–ú–ï–† –†–ê–ë–û–¢–´ –ú–û–î–ï–õ–ò:")
    if train_data:
        sample_text = train_data[0][text_field]
        if len(sample_text) > 100:
            sample_text = sample_text[:100] + "..."
        result = classifier.predict_single(sample_text)
        print(f"   –¢–µ–∫—Å—Ç: '{sample_text}'")
        print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {result['prediction']}")
        print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence_percent']:.1f}%")

        if result['top_categories']:
            print(f"   –¢–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:")
            for i, cat in enumerate(result['top_categories'], 1):
                print(f"     {i}. {cat['category']}: {cat['probability_percent']:.1f}%")

    return classifier


def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    """
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        train_data = read_jsonl_basic('../../util/news_category_train.jsonl')
        val_data = read_jsonl_basic('../../util/news_category_val.jsonl')
        test_data = read_jsonl_basic('../../util/news_category_test.jsonl')

        print(f"üìä –î–∞–Ω–Ω—ã–µ: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
        if train_data:
            print(f"\nüìã –ü–†–ò–ú–ï–† –î–ê–ù–ù–´–•:")
            sample = train_data[0]
            print(f"   –ü–æ–ª—è: {list(sample.keys())}")
            print(f"   –¢–µ–∫—Å—Ç: {sample.get('text', 'N/A')[:100]}...")
            print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {sample.get('category', 'N/A')}")

        # 1. –û–±—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞
        print("\n" + "=" * 60)

        rf_classifier = RandomForestCategoryClassifier(
            n_estimators=50,  # –ú–µ–Ω—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            max_depth=None,
            text_field='text',
            label_field='category',
            class_weight=None
        )

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ
        if len(train_data) > 500:
            print(f"‚ÑπÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏–∑ 500 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏")
            train_subset = train_data[:500]
            val_subset = val_data[:100] if val_data and len(val_data) > 100 else val_data
        else:
            train_subset = train_data
            val_subset = val_data

        rf_classifier.train(train_subset, val_subset, auto_detect_classes=True)

        # 3. –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if test_data and len(test_data) > 0:
            print("\n" + "=" * 60)
            print("üß™ –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•:")
            test_subset = test_data[:200] if len(test_data) > 200 else test_data
            test_accuracy, test_report = rf_classifier.evaluate(test_subset, detailed=True)
            print(f"\nüìä –ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {test_accuracy:.3f}")

        # 4. –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        print("\n" + "=" * 60)
        analyze_feature_importance(rf_classifier, top_n=15, save_to_csv=True)

        # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        rf_classifier.save_model("random_forest_category_classifier.pkl")

        # 6. –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        print("\n" + "=" * 60)
        print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –ü–ê–†–ê–ú–ï–¢–†–û–í (–Ω–∞ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ):")

        small_train = train_data[:200] if len(train_data) > 200 else train_data
        small_val = val_data[:50] if val_data and len(val_data) > 50 else val_data

        if small_train and small_val:
            models = compare_rf_parameters(
                small_train, small_val,
                'text', 'category',
                use_subset=False  # –£–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ
            )
            print(f"\n‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –û–±—É—á–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(models)}")
        else:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")

    except FileNotFoundError as e:
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
        print("‚ÑπÔ∏è  –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –¥–∞–Ω–Ω—ã—Ö")
    except Exception as e:
        print(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞
    print("üöÄ –ó–ê–ü–£–°–ö –ü–†–ò–ú–ï–†–ê –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø –°–õ–£–ß–ê–ô–ù–û–ì–û –õ–ï–°–ê")
    print("=" * 60)
    main()