import h2o
from h2o.automl import H2OAutoML
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import numpy as np
import pandas as pd
import joblib
import json
import warnings

warnings.filterwarnings('ignore')


class H2OMultiClassClassifier:
    """
    –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ H2O.ai AutoML
    """

    def __init__(self, max_runtime_secs=300, max_models=10, nfolds=5):
        """
        Args:
            max_runtime_secs: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã AutoML –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            max_models: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è
            nfolds: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è H2O –∫–ª–∞—Å—Ç–µ—Ä–∞
        h2o.init()
        print(f"üöÄ H2O –∫–ª–∞—Å—Ç–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            min_df=2,
            max_df=0.8,
            ngram_range=(1, 2),
            stop_words=None
        )

        # –î–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫
        self.label_encoder = LabelEncoder()

        # H2O AutoML –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        self.aml = H2OAutoML(
            max_runtime_secs=max_runtime_secs,
            max_models=max_models,
            nfolds=nfolds,
            seed=42,
            sort_metric="logloss",  # –î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å logloss
            verbosity="info"
        )

        self.max_runtime_secs = max_runtime_secs
        self.max_models = max_models
        self.is_trained = False
        self.leader_model = None
        self.class_names = None
        self.n_classes = None

        print(f"üéØ H2O.AI AUTOML –î–õ–Ø –ú–ù–û–ì–û–ö–õ–ê–°–°–û–í–û–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò:")
        print(f"   –í—Ä–µ–º—è: {max_runtime_secs} —Å–µ–∫")
        print(f"   –ú–∞–∫—Å. –º–æ–¥–µ–ª–µ–π: {max_models}")
        print(f"   CV —Ñ–æ–ª–¥–æ–≤: {nfolds}")
        print(f"   H2O –≤–µ—Ä—Å–∏—è: {h2o.__version__}")

    def prepare_features(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö
        """
        texts = [item['text'] for item in data]
        return texts

    def prepare_labels(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–æ–∫ –∏–∑ –¥–∞–Ω–Ω—ã—Ö
        """
        labels = [item['category'] for item in data]
        return labels

    def _create_h2o_frame(self, texts, labels=None):
        """
        –°–æ–∑–¥–∞–µ—Ç H2O Frame –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ –∏ –º–µ—Ç–æ–∫
        """
        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
        X_vec = self.vectorizer.transform(texts)
        X_dense = X_vec.toarray()

        # –°–æ–∑–¥–∞–µ–º DataFrame
        feature_names = [f"feature_{i}" for i in range(X_dense.shape[1])]
        df = pd.DataFrame(X_dense, columns=feature_names)

        if labels is not None:
            df['label'] = labels

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ H2O Frame
        h2o_frame = h2o.H2OFrame(df)

        if labels is not None:
            # –£–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ label - –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
            h2o_frame['label'] = h2o_frame['label'].asfactor()

        return h2o_frame

    def train(self, train_data, val_data=None):
        """
        –û–±—É—á–µ–Ω–∏–µ H2O AutoML –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        print("üéØ –ê–í–¢–û–ú–ê–¢–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–û–ò–°–ö –ú–û–î–ï–õ–ï–ô –° H2O.AI...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train_texts = self.prepare_features(train_data)
        y_train = self.prepare_labels(train_data)

        # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
        y_train_encoded = self.label_encoder.fit_transform(y_train)
        self.class_names = self.label_encoder.classes_
        self.n_classes = len(self.class_names)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª–∞—Å—Å—ã
        print(f"üìä –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {self.n_classes} –∫–ª–∞—Å—Å–æ–≤: {list(self.class_names)}")
        print(f"üìä –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(X_train_texts)} –ø—Ä–∏–º–µ—Ä–æ–≤")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
        unique, counts = np.unique(y_train, return_counts=True)
        for cls, count in zip(unique, counts):
            percentage = (count / len(y_train)) * 100
            print(f"   {cls}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤ ({percentage:.1f}%)")

        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–π–∑–µ—Ä –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
        print("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        self.vectorizer.fit(X_train_texts)

        # –°–æ–∑–¥–∞–µ–º H2O Frame –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        print("üìä –°–æ–∑–¥–∞–Ω–∏–µ H2O Frame...")
        train_frame = self._create_h2o_frame(X_train_texts, y_train)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {train_frame.shape}")
        print(f"   –ö–æ–ª–æ–Ω–∫–∏: {train_frame.columns[:5]}...")  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º features –∏ target
        x = train_frame.columns[:-1]  # –í—Å–µ –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∫–æ–ª–æ–Ω–∫–∏ (target)
        y = 'label'

        print(f"   Features: {len(x)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        print(f"   Target: {y}")
        print(f"   –ö–ª–∞—Å—Å—ã: {self.n_classes}")

        # –ó–∞–ø—É—Å–∫ AutoML
        print(f"\nü§ñ –ó–ê–ü–£–°–ö H2O AUTOML...")
        print(f"   –≠—Ç–æ –∑–∞–π–º–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ {self.max_runtime_secs} —Å–µ–∫—É–Ω–¥")

        self.aml.train(x=x, y=y, training_frame=train_frame)
        self.is_trained = True

        # –ü–æ–ª—É—á–∞–µ–º –ª–∏–¥–µ—Ä-–º–æ–¥–µ–ª—å
        self.leader_model = self.aml.leader

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._show_training_summary()

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
        if val_data:
            print("\nüîç –û–¶–ï–ù–ö–ê –ù–ê –í–ê–õ–ò–î–ê–¶–ò–ò:")
            self.evaluate(val_data)

    def _show_training_summary(self):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è AutoML
        """
        print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ H2O AUTOML:")
        print("=" * 60)

        # –õ–∏–¥–µ—Ä–±–æ—Ä–¥
        lb = self.aml.leaderboard
        print("üèÜ –õ–ò–î–ï–†–ë–û–†–î –ú–û–î–ï–õ–ï–ô:")
        print(lb.head(rows=min(10, len(lb))))  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ 10 –º–æ–¥–µ–ª–µ–π

        # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
        print(f"\nüéØ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨:")
        print(f"   –ê–ª–≥–æ—Ä–∏—Ç–º: {self.leader_model.algo}")
        print(f"   Model ID: {self.leader_model.model_id}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {self.n_classes}")

        # –ú–µ—Ç—Ä–∏–∫–∏ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        try:
            performance = self.leader_model.model_performance()

            # –î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥—Ä—É–≥–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            if hasattr(performance, 'logloss'):
                print(f"   Logloss: {performance.logloss():.4f}")
            if hasattr(performance, 'mean_per_class_error'):
                print(f"   Mean per class error: {performance.mean_per_class_error():.4f}")
            if hasattr(performance, 'mse'):
                print(f"   MSE: {performance.mse():.4f}")
            if hasattr(performance, 'accuracy'):
                print(f"   Accuracy: {performance.accuracy():.4f}")

            # –ö–æ–Ω—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
            try:
                cm = performance.confusion_matrix()
                if cm is not None:
                    print(f"\nüìä –ö–æ–Ω—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ (–ø–µ—Ä–≤—ã–µ 5x5):")
                    cm_df = cm.as_data_frame()
                    print(cm_df.head())
            except:
                pass

        except Exception as e:
            print(f"   –ú–µ—Ç—Ä–∏–∫–∏: {e}")

    def predict(self, data):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        texts = self.prepare_features(data)

        # –°–æ–∑–¥–∞–µ–º H2O Frame –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predict_frame = self._create_h2o_frame(texts)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = self.leader_model.predict(predict_frame)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy arrays
        pred_array = predictions['predict'].as_data_frame().values.flatten()

        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤
        prob_columns = [f'p{i}' for i in range(self.n_classes)]
        if all(col in predictions.columns for col in prob_columns):
            prob_array = predictions[prob_columns].as_data_frame().values
        else:
            # –ï—Å–ª–∏ —Å—Ç–æ–ª–±—Ü—ã –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è –ø–æ-–¥—Ä—É–≥–æ–º—É
            prob_columns = [col for col in predictions.columns if col.startswith('p')]
            prob_array = predictions[prob_columns].as_data_frame().values

        return pred_array, prob_array

    def predict_single(self, text):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        # –°–æ–∑–¥–∞–µ–º H2O Frame –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        predict_frame = self._create_h2o_frame([text])

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = self.leader_model.predict(predict_frame)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        pred = prediction['predict'].as_data_frame().values[0][0]

        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤
        prob_columns = [f'p{i}' for i in range(self.n_classes)]
        if all(col in prediction.columns for col in prob_columns):
            probabilities = prediction[prob_columns].as_data_frame().values[0]
        else:
            # –ï—Å–ª–∏ —Å—Ç–æ–ª–±—Ü—ã –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è –ø–æ-–¥—Ä—É–≥–æ–º—É
            prob_columns = sorted([col for col in prediction.columns if col.startswith('p')])
            probabilities = prediction[prob_columns].as_data_frame().values[0]

        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø–æ –∫–ª–∞—Å—Å–∞–º
        class_probabilities = {}
        for i, class_name in enumerate(self.class_names):
            if i < len(probabilities):
                class_probabilities[class_name] = probabilities[i]
            else:
                class_probabilities[class_name] = 0.0

        # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
        pred_idx = list(self.class_names).index(pred) if pred in self.class_names else 0

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        sorted_probs = sorted(class_probabilities.items(), key=lambda x: x[1], reverse=True)
        top_3 = sorted_probs[:3]

        result = {
            'prediction': pred,
            'confidence': f"{probabilities[pred_idx]:.3f}",
            'probabilities': class_probabilities,
            'top_3_predictions': top_3,
            'model_type': self.leader_model.algo
        }

        return result

    def evaluate(self, test_data):
        """
        –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        X_test_texts = self.prepare_features(test_data)
        y_test = self.prepare_labels(test_data)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions, probabilities = self.predict(test_data)

        # –¢–æ—á–Ω–æ—Å—Ç—å
        accuracy = accuracy_score(y_test, predictions)

        print(f"üìä –¢–ï–°–¢–û–í–ê–Ø –¢–û–ß–ù–û–°–¢–¨: {accuracy:.4f}")

        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        print("\nüìà –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢:")
        print(classification_report(y_test, predictions, digits=4))

        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        print("\nüìä –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö:")
        y_test_encoded = self.label_encoder.transform(y_test)
        predictions_encoded = self.label_encoder.transform(predictions)
        cm = confusion_matrix(y_test_encoded, predictions_encoded)

        # –í—ã–≤–æ–¥–∏–º –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫
        self._print_confusion_matrix(cm, self.class_names)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        from sklearn.metrics import precision_score, recall_score, f1_score
        precision_macro = precision_score(y_test_encoded, predictions_encoded, average='macro')
        recall_macro = recall_score(y_test_encoded, predictions_encoded, average='macro')
        f1_macro = f1_score(y_test_encoded, predictions_encoded, average='macro')

        precision_weighted = precision_score(y_test_encoded, predictions_encoded, average='weighted')
        recall_weighted = recall_score(y_test_encoded, predictions_encoded, average='weighted')
        f1_weighted = f1_score(y_test_encoded, predictions_encoded, average='weighted')

        print(f"\nüìä –°–†–ï–î–ù–ò–ï –ú–ï–¢–†–ò–ö–ò:")
        print(f"   Precision (macro): {precision_macro:.4f}")
        print(f"   Recall (macro): {recall_macro:.4f}")
        print(f"   F1-Score (macro): {f1_macro:.4f}")
        print(f"   Precision (weighted): {precision_weighted:.4f}")
        print(f"   Recall (weighted): {recall_weighted:.4f}")
        print(f"   F1-Score (weighted): {f1_weighted:.4f}")

        return accuracy

    def _print_confusion_matrix(self, cm, class_names):
        """
        –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
        """
        n_classes = len(class_names)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã
        if cm.shape[0] != n_classes:
            print(f"‚ö†Ô∏è  –†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã ({cm.shape[0]}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–ª–∞—Å—Å–æ–≤ ({n_classes})")
            # –û–±—Ä–µ–∑–∞–µ–º –∏–ª–∏ –¥–æ–ø–æ–ª–Ω—è–µ–º –∏–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–æ —Ä–∞–∑–º–µ—Ä–∞ –º–∞—Ç—Ä–∏—Ü—ã
            if cm.shape[0] < n_classes:
                class_names = class_names[:cm.shape[0]]
            n_classes = cm.shape[0]

        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–æ–≤
        if n_classes > 8:
            print("   (–ú–∞—Ç—Ä–∏—Ü–∞ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è)")
            print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: {np.trace(cm)}/{np.sum(cm)} ({np.trace(cm) / np.sum(cm):.1%})")
            return

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        header = " " * 10 + "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ ‚Üí"
        print(header)

        # –ò–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        pred_header = " " * 5
        for name in class_names[:n_classes]:
            pred_header += f"{str(name)[:6]:^6} "
        print(pred_header)

        # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
        separator = " " * 5 + "‚îÄ" * (n_classes * 7 + 1)
        print(separator)

        # –°—Ç—Ä–æ–∫–∏ –º–∞—Ç—Ä–∏—Ü—ã
        for i, true_name in enumerate(class_names[:n_classes]):
            row = f"{str(true_name)[:5]:>5} ‚îÇ"
            for j in range(n_classes):
                row += f"{cm[i][j]:^6} "
            print(row)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        diagonal = cm.diagonal()
        total = cm.sum()
        correct = diagonal.sum()

        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: {correct}/{total} ({correct / total:.1%})")

    def get_model_info(self):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö
        """
        if not self.is_trained:
            return {"error": "–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞"}

        lb = self.aml.leaderboard.as_data_frame()

        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        available_columns = []
        for col in ['model_id', 'auc', 'logloss', 'mean_per_class_error', 'mse', 'rmse']:
            if col in lb.columns:
                available_columns.append(col)

        if available_columns:
            top_models = lb.head(5)[available_columns].to_dict('records')
        else:
            top_models = lb.head(5).to_dict('records')

        return {
            "leader_model": self.leader_model.model_id,
            "leader_algorithm": self.leader_model.algo,
            "n_classes": self.n_classes,
            "class_names": list(self.class_names),
            "top_models": top_models,
            "total_models_trained": len(lb),
            "feature_count": len(self.vectorizer.get_feature_names_out())
        }

    def get_class_distribution(self, data):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö
        """
        labels = self.prepare_labels(data)
        unique, counts = np.unique(labels, return_counts=True)
        return dict(zip(unique, counts))

    def save_model(self, filename):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        if not self.is_trained:
            print("‚ùå –ù–µ–ª—å–∑—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–µ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å")
            return

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º H2O –º–æ–¥–µ–ª—å
        model_path = h2o.save_model(model=self.leader_model, path=filename, force=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–π–∑–µ—Ä –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        joblib.dump({
            'vectorizer': self.vectorizer,
            'label_encoder': self.label_encoder,
            'model_path': model_path,
            'model_id': self.leader_model.model_id,
            'class_names': self.class_names,
            'n_classes': self.n_classes
        }, f"{filename}_meta.pkl")

        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")

    def load_model(self, filename):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        meta = joblib.load(f"{filename}_meta.pkl")
        self.vectorizer = meta['vectorizer']
        self.label_encoder = meta['label_encoder']
        self.class_names = meta['class_names']
        self.n_classes = meta['n_classes']

        # –ó–∞–≥—Ä—É–∂–∞–µ–º H2O –º–æ–¥–µ–ª—å
        self.leader_model = h2o.load_model(meta['model_path'])
        self.is_trained = True

        print(f"üì• –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {meta['model_id']}")
        print(f"üì• –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {self.n_classes}")
        print(f"üì• –ò–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤: {list(self.class_names)}")

    def __del__(self):
        """
        –ó–∞–∫—Ä—ã—Ç–∏–µ H2O –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–∞
        """
        try:
            h2o.cluster().shutdown()
        except:
            pass


# –°–ü–ï–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† –î–õ–Ø –¢–û–ù–ê–õ–¨–ù–û–°–¢–ò (3 –ö–õ–ê–°–°–ê)
class H2OSentimentClassifier(H2OMultiClassClassifier):
    """
    –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π H2O –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (–Ω–µ–≥–∞—Ç–∏–≤, –Ω–µ–π—Ç—Ä–∞–ª, –ø–æ–∑–∏—Ç–∏–≤)
    """

    def __init__(self, max_runtime_secs=300, max_models=10, nfolds=5):
        super().__init__(
            max_runtime_secs=max_runtime_secs,
            max_models=max_models,
            nfolds=nfolds
        )

    def predict_sentiment(self, text):
        """
        –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        """
        result = super().predict_single(text)

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –¥–ª—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        sentiment_mapping = {
            'negative': '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π',
            'neutral': '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π',
            'positive': '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π',
            'neg': '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π',
            'neu': '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π',
            'pos': '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π',
            '0': '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π',
            '1': '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π',
            '2': '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π'
        }

        prediction = result['prediction']
        sentiment = sentiment_mapping.get(str(prediction).lower(), prediction)

        result['sentiment'] = sentiment
        result['sentiment_confidence'] = result['confidence']

        return result


# –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞
class SimpleH2OMultiClassClassifier:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è H2O –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    """

    def __init__(self, time_seconds=120):
        h2o.init()
        print(f"üöÄ H2O –∫–ª–∞—Å—Ç–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.label_encoder = LabelEncoder()
        self.aml = H2OAutoML(max_runtime_secs=time_seconds, seed=42, verbosity="info")
        self.is_trained = False
        self.class_names = None

        print(f"‚ö° SimpleH2O (–º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π): {time_seconds} —Å–µ–∫")

    def train_and_test(self, train_data, test_data):
        """–û–±—É—á–∞–µ—Ç –∏ —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç –∑–∞ –æ–¥–∏–Ω —à–∞–≥"""
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        train_texts = [item['text'] for item in train_data]
        train_labels = [item['category'] for item in train_data]
        test_texts = [item['text'] for item in test_data]
        test_labels = [item['category'] for item in test_data]

        # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
        train_labels_encoded = self.label_encoder.fit_transform(train_labels)
        test_labels_encoded = self.label_encoder.transform(test_labels)
        self.class_names = self.label_encoder.classes_

        print(f"üìö –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(train_texts)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")
        print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(self.class_names)}")
        print(f"üìä –ö–ª–∞—Å—Å—ã: {list(self.class_names)}")

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ H2O Frame
        self.vectorizer.fit(train_texts)

        train_features = self.vectorizer.transform(train_texts).toarray()
        test_features = self.vectorizer.transform(test_texts).toarray()

        # –°–æ–∑–¥–∞–µ–º DataFrame
        feature_names = [f"f_{i}" for i in range(train_features.shape[1])]

        train_df = pd.DataFrame(train_features, columns=feature_names)
        train_df['target'] = train_labels  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏

        test_df = pd.DataFrame(test_features, columns=feature_names)
        test_df['target'] = test_labels

        # H2O Frames
        train_frame = h2o.H2OFrame(train_df)
        test_frame = h2o.H2OFrame(test_df)
        train_frame['target'] = train_frame['target'].asfactor()
        test_frame['target'] = test_frame['target'].asfactor()

        # AutoML
        print("ü§ñ –ó–∞–ø—É—Å–∫ AutoML...")
        self.aml.train(x=feature_names, y='target', training_frame=train_frame)
        self.is_trained = True

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = self.aml.leader.predict(test_frame)
        pred_array = predictions['predict'].as_data_frame().values.flatten()

        accuracy = accuracy_score(test_labels, pred_array)

        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        print(f"   –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {self.aml.leader.algo}")
        print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f}")

        # Classification report
        print("\nüìà –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:")
        print(classification_report(test_labels, pred_array, digits=3))

        return accuracy

    def __del__(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ H2O –∫–ª–∞—Å—Ç–µ—Ä–∞"""
        try:
            h2o.cluster().shutdown()
        except:
            pass


# –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø
def main():
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è H2O AutoML –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    """

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    def load_jsonl(filepath):
        with open(filepath, 'r', encoding='utf-8') as f:
            return [json.loads(line) for line in f]

    print("=" * 80)
    print("üöÄ H2O.AI AUTOML –î–õ–Ø –ú–ù–û–ì–û–ö–õ–ê–°–°–û–í–û–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò")
    print("=" * 80)

    train_data = load_jsonl('../util/news_category_train.jsonl')
    test_data = load_jsonl('../util/news_category_test.jsonl')

    print(f"üìÅ Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"üìÅ Test: {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val
    np.random.seed(42)
    indices = np.random.permutation(len(train_data))
    split_idx = int(0.8 * len(train_data))

    train_indices = indices[:split_idx]
    val_indices = indices[split_idx:]

    train_subset = [train_data[i] for i in train_indices]
    val_subset = [train_data[i] for i in val_indices]

    print(f"üìä Train: {len(train_subset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"üìä Val: {len(val_subset)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ü–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è H2O
    print("\n" + "=" * 50)
    print("üéØ –í–ê–†–ò–ê–ù–¢ 1: –ü–û–õ–ù–ê–Ø –í–ï–†–°–ò–Ø H2O AUTOML")

    try:
        h2o_model = H2OMultiClassClassifier(max_runtime_secs=180)  # 3 –º–∏–Ω—É—Ç—ã

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
        print("\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í:")
        class_dist = h2o_model.get_class_distribution(train_subset)
        for class_name, count in class_dist.items():
            percentage = (count / len(train_subset)) * 100
            print(f"   {class_name}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤ ({percentage:.1f}%)")

        h2o_model.train(train_subset, val_subset)
        h2o_accuracy = h2o_model.evaluate(test_data)

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö
        model_info = h2o_model.get_model_info()
        print(f"\nüìã –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ú–û–î–ï–õ–Ø–•:")
        print(f"   –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {model_info['leader_algorithm']}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {model_info['n_classes']}")
        print(f"   –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π: {model_info['total_models_trained']}")
        print(f"   –ö–ª–∞—Å—Å—ã: {model_info['class_names']}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        h2o_model.save_model("h2o_multiclass_model")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ H2O: {e}")
        import traceback
        traceback.print_exc()
        h2o_accuracy = 0

    # –í–∞—Ä–∏–∞–Ω—Ç 2: –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
    print("\n" + "=" * 50)
    print("üéØ –í–ê–†–ò–ê–ù–¢ 2: –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø")

    try:
        simple_h2o = SimpleH2OMultiClassClassifier(time_seconds=120)  # 2 –º–∏–Ω—É—Ç—ã
        simple_accuracy = simple_h2o.train_and_test(train_subset[:50], test_data[:20])
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        simple_accuracy = 0

    # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã H2O
    print("\nüîÑ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã H2O –∫–ª–∞—Å—Ç–µ—Ä–∞...")
    try:
        h2o.cluster().shutdown()
        print("‚úÖ H2O –∫–ª–∞—Å—Ç–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except:
        print("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å H2O –∫–ª–∞—Å—Ç–µ—Ä")


if __name__ == "__main__":
    print("üöÄ –ó–ê–ü–£–°–ö H2O.AI AUTOML –î–õ–Ø –ú–ù–û–ì–û–ö–õ–ê–°–°–û–í–û–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò")
    print("=" * 80)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–∏–º–µ—Ä
    main()