from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC, SVC
from sklearn.ensemble import RandomForestClassifier, StackingClassifier, VotingClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import numpy as np
import joblib
import warnings
import seaborn as sns
import matplotlib.pyplot as plt
from typing import List, Dict, Any, Optional, Tuple, Union
from collections import Counter
import pandas as pd

# –î–ª—è CatBoost (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
try:
    from catboost import CatBoostClassifier

    CATBOOST_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  CatBoost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ sklearn –º–æ–¥–µ–ª–∏.")
    CATBOOST_AVAILABLE = False

from util.jsonl_process import read_jsonl_basic

warnings.filterwarnings('ignore')


class StackingCategoryClassifier:
    """
    –°—Ç–µ–∫–∏–Ω–≥/–±–ª–µ–Ω–¥–∏–Ω–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    """

    def __init__(self,
                 use_blending: bool = True,
                 meta_model: str = 'logistic',
                 class_names: Optional[List[str]] = None,
                 text_field: str = 'text',
                 label_field: str = 'category',
                 random_state: int = 42,
                 use_catboost: bool = False):
        """
        Args:
            use_blending: True –¥–ª—è –±–ª–µ–Ω–¥–∏–Ω–≥–∞, False –¥–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞
            meta_model: —Ç–∏–ø –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ ('logistic', 'svm', 'random_forest')
            class_names: —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∞—Å—Å–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            text_field: –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—è —Å —Ç–µ–∫—Å—Ç–æ–º
            label_field: –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—è —Å –º–µ—Ç–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            random_state: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
            use_catboost: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CatBoost –≤ –∞–Ω—Å–∞–º–±–ª–µ
        """
        self.vectorizer = TfidfVectorizer(
            max_features=10000,
            min_df=2,
            max_df=0.9,
            ngram_range=(1, 2),
            stop_words=None
        )

        # –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (level-0)
        self.base_models = {
            'svm': LinearSVC(
                C=1.0,
                random_state=random_state,
                max_iter=2000,
                dual=True,
                multi_class='ovr'
            ),
            'logistic': LogisticRegression(
                C=1.0,
                random_state=random_state,
                max_iter=1000,
                solver='lbfgs',
                multi_class='multinomial'
            ),
            'random_forest': RandomForestClassifier(
                n_estimators=100,
                random_state=random_state,
                max_depth=None,
                n_jobs=-1
            )
        }

        # –î–æ–±–∞–≤–ª—è–µ–º CatBoost –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –∑–∞–ø—Ä–æ—à–µ–Ω
        if use_catboost and CATBOOST_AVAILABLE:
            self.base_models['catboost'] = CatBoostClassifier(
                iterations=300,
                learning_rate=0.1,
                depth=6,
                random_seed=random_state,
                verbose=0,
                thread_count=-1,
                loss_function='MultiClass'
            )
        elif use_catboost:
            print("‚ö†Ô∏è  CatBoost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º CatBoost –≤ –∞–Ω—Å–∞–º–±–ª–µ.")

        # –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å (level-1)
        if meta_model == 'logistic':
            self.meta_model = LogisticRegression(
                C=1.0,
                random_state=random_state,
                max_iter=1000,
                solver='lbfgs',
                multi_class='multinomial'
            )
        elif meta_model == 'svm':
            self.meta_model = SVC(
                C=1.0,
                random_state=random_state,
                probability=True,  # –î–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞ –Ω—É–∂–Ω—ã –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
                kernel='linear'
            )
        elif meta_model == 'random_forest':
            self.meta_model = RandomForestClassifier(
                n_estimators=100,
                random_state=random_state,
                max_depth=None
            )
        else:
            raise ValueError("meta_model –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 'logistic', 'svm' –∏–ª–∏ 'random_forest'")

        self.label_encoder = LabelEncoder()
        self.class_names = class_names
        self.use_blending = use_blending
        self.meta_model_type = meta_model
        self.text_field = text_field
        self.label_field = label_field
        self.is_trained = False
        self.num_classes = 0
        self.random_state = random_state
        self.use_catboost = use_catboost

        # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø—Ä–∏ –±–ª–µ–Ω–¥–∏–Ω–≥–µ
        self.base_predictions = {}
        self.base_probabilities = {}

    def prepare_data(self, data: List[Dict[str, Any]]) -> Tuple[List[str], List[str]]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        """
        texts = [item[self.text_field] for item in data]
        labels = [item[self.label_field] for item in data]
        return texts, labels

    def analyze_class_distribution(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö
        """
        _, labels = self.prepare_data(data)
        label_counts = Counter(labels)

        result = {
            'total_samples': len(data),
            'num_classes': len(label_counts),
            'classes': dict(label_counts),
            'class_percentages': {},
            'imbalance_ratio': None,
            'unique_labels': sorted(list(label_counts.keys()))
        }

        if label_counts:
            max_count = max(label_counts.values())
            min_count = min(label_counts.values())
            if min_count > 0:
                result['imbalance_ratio'] = max_count / min_count

            for label, count in label_counts.items():
                result['class_percentages'][label] = count / len(data) * 100

        return result

    def train_blending(self, train_data: List[Dict[str, Any]],
                       val_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        –û–±—É—á–µ–Ω–∏–µ —Å –±–ª–µ–Ω–¥–∏–Ω–≥–æ–º (–∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π validation set)
        """
        print("üéØ –û–ë–£–ß–ï–ù–ò–ï –° –ë–õ–ï–ù–î–ò–ù–ì–û–ú...")

        X_train, y_train_raw = self.prepare_data(train_data)
        X_val, y_val_raw = self.prepare_data(val_data)

        # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
        if self.class_names is None:
            self.label_encoder.fit(y_train_raw + y_val_raw)
            self.class_names = list(self.label_encoder.classes_)
        else:
            self.label_encoder.fit(self.class_names)

        y_train = self.label_encoder.transform(y_train_raw)
        y_val = self.label_encoder.transform(y_val_raw)
        self.num_classes = len(self.class_names)

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        print("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        X_train_vec = self.vectorizer.fit_transform(X_train)
        X_val_vec = self.vectorizer.transform(X_val)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_vec.shape}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {self.num_classes}")
        print(f"   –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: {list(self.base_models.keys())}")
        print(f"   –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å: {self.meta_model_type}")

        # –û–±—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        base_val_probabilities = []

        print("\nü§ñ –û–ë–£–ß–ï–ù–ò–ï –ë–ê–ó–û–í–´–• –ú–û–î–ï–õ–ï–ô:")
        for name, model in self.base_models.items():
            print(f"   –û–±—É—á–µ–Ω–∏–µ {name}...")

            try:
                if name == 'catboost' and CATBOOST_AVAILABLE:
                    # CatBoost —Ç—Ä–µ–±—É–µ—Ç –ø–ª–æ—Ç–Ω—ã–µ –º–∞—Å—Å–∏–≤—ã
                    X_train_dense = X_train_vec.toarray()
                    X_val_dense = X_val_vec.toarray()
                    model.fit(X_train_dense, y_train)

                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ validation set
                    val_pred = model.predict(X_val_dense)
                    val_prob = model.predict_proba(X_val_dense)
                else:
                    model.fit(X_train_vec, y_train)
                    val_pred = model.predict(X_val_vec)

                    # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
                    if hasattr(model, 'predict_proba'):
                        val_prob = model.predict_proba(X_val_vec)
                    else:
                        # –î–ª—è LinearSVC –∏—Å–ø–æ–ª—å–∑—É–µ–º decision function
                        decision_scores = model.decision_function(X_val_vec)
                        val_prob = self._decision_to_probability_multiclass(decision_scores)

                accuracy = accuracy_score(y_val, val_pred)
                print(f"      ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ val: {accuracy:.3f}")

                base_val_probabilities.append(val_prob)
                self.base_predictions[name] = val_pred
                self.base_probabilities[name] = val_prob

            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ {name}: {e}")
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç—É –º–æ–¥–µ–ª—å
                if name in self.base_models:
                    del self.base_models[name]

        if not base_val_probabilities:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å")

        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        meta_features = np.hstack(base_val_probabilities)

        print(f"\nüìä –ú–ï–¢–ê-–ü–†–ò–ó–ù–ê–ö–ò:")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {meta_features.shape}")
        print(f"   –û–±—É—á–µ–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –Ω–∞ {len(y_val)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")

        # –û–±—É—á–∞–µ–º –º–µ—Ç–∞-–º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        self.meta_model.fit(meta_features, y_val)

        # –û—Ü–µ–Ω–∫–∞ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –Ω–∞ validation set
        meta_pred = self.meta_model.predict(meta_features)
        meta_accuracy = accuracy_score(y_val, meta_pred)
        print(f"   ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –Ω–∞ val: {meta_accuracy:.3f}")

        self.is_trained = True

        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        base_accuracies = {}
        for name, pred in self.base_predictions.items():
            base_accuracies[name] = accuracy_score(y_val, pred)

        return {
            'base_accuracies': base_accuracies,
            'meta_accuracy': meta_accuracy,
            'num_classes': self.num_classes,
            'class_names': self.class_names
        }

    def train_stacking(self, train_data: List[Dict[str, Any]],
                       val_data: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """
        –û–±—É—á–µ–Ω–∏–µ —Å–æ —Å—Ç–µ–∫–∏–Ω–≥–æ–º (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é)
        """
        print("üéØ –û–ë–£–ß–ï–ù–ò–ï –°–û –°–¢–ï–ö–ò–ù–ì–û–ú...")

        X_train, y_train_raw = self.prepare_data(train_data)

        # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
        if self.class_names is None:
            self.label_encoder.fit(y_train_raw)
            self.class_names = list(self.label_encoder.classes_)
        else:
            self.label_encoder.fit(self.class_names)

        y_train = self.label_encoder.transform(y_train_raw)
        self.num_classes = len(self.class_names)

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        print("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        X_train_vec = self.vectorizer.fit_transform(X_train)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_vec.shape}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {self.num_classes}")
        print(f"   –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: {list(self.base_models.keys())}")
        print(f"   –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å: {self.meta_model_type}")

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞
        estimators = []
        for name, model in self.base_models.items():
            estimators.append((name, model))

        if not estimators:
            raise ValueError("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞")

        # –°–æ–∑–¥–∞–µ–º —Å—Ç–µ–∫–∏–Ω–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.stacking_model = StackingClassifier(
            estimators=estimators,
            final_estimator=self.meta_model,
            cv=3,
            passthrough=False,
            n_jobs=-1,
            verbose=0
        )

        print("\nü§ñ –û–ë–£–ß–ï–ù–ò–ï –°–¢–ï–ö–ò–ù–ì –ú–û–î–ï–õ–ò...")

        # –û–±—É—á–∞–µ–º —Å—Ç–µ–∫–∏–Ω–≥ –º–æ–¥–µ–ª—å
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ CatBoost —Å—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π
            has_catboost = any(name == 'catboost' for name in self.base_models.keys())

            if has_catboost and CATBOOST_AVAILABLE:
                X_train_dense = X_train_vec.toarray()
                self.stacking_model.fit(X_train_dense, y_train)
            else:
                self.stacking_model.fit(X_train_vec, y_train)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å—Ç–µ–∫–∏–Ω–≥–∞: {e}")
            # –ü—Ä–æ–±—É–µ–º –±–µ–∑ CatBoost
            if 'catboost' in self.base_models:
                print("‚ö†Ô∏è  –ü—Ä–æ–±—É–µ–º –±–µ–∑ CatBoost...")
                del self.base_models['catboost']
                estimators = [(name, model) for name, model in self.base_models.items()]
                self.stacking_model = StackingClassifier(
                    estimators=estimators,
                    final_estimator=self.meta_model,
                    cv=3,
                    passthrough=False,
                    n_jobs=-1,
                    verbose=0
                )
                self.stacking_model.fit(X_train_vec, y_train)

        self.is_trained = True

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if has_catboost and CATBOOST_AVAILABLE:
            train_pred = self.stacking_model.predict(X_train_dense)
        else:
            train_pred = self.stacking_model.predict(X_train_vec)

        train_accuracy = accuracy_score(y_train, train_pred)
        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train: {train_accuracy:.3f}")

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        if val_data:
            val_accuracy, _ = self.evaluate(val_data, detailed=False)
            print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ val: {val_accuracy:.3f}")

        return {
            'train_accuracy': train_accuracy,
            'num_classes': self.num_classes,
            'class_names': self.class_names
        }

    def train(self, train_data: List[Dict[str, Any]],
              val_data: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è
        """
        if self.use_blending:
            if val_data is None:
                raise ValueError("–î–ª—è –±–ª–µ–Ω–¥–∏–Ω–≥–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º validation set")
            return self.train_blending(train_data, val_data)
        else:
            return self.train_stacking(train_data, val_data)

    def predict(self, texts: List[str]) -> Tuple[List[str], np.ndarray]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)

        if self.use_blending:
            predictions_encoded, probabilities = self._predict_blending(X_vec)
        else:
            predictions_encoded, probabilities = self._predict_stacking(X_vec)

        predictions = self.label_encoder.inverse_transform(predictions_encoded)
        return predictions, probabilities

    def _predict_blending(self, X_vec: Any) -> Tuple[np.ndarray, np.ndarray]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –±–ª–µ–Ω–¥–∏–Ω–≥–∞
        """
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –≤—Å–µ—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        base_probabilities = []

        for name, model in self.base_models.items():
            try:
                if name == 'catboost' and CATBOOST_AVAILABLE:
                    X_dense = X_vec.toarray()
                    prob = model.predict_proba(X_dense)
                else:
                    if hasattr(model, 'predict_proba'):
                        prob = model.predict_proba(X_vec)
                    else:
                        decision_scores = model.decision_function(X_vec)
                        prob = self._decision_to_probability_multiclass(decision_scores)

                base_probabilities.append(prob)
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ –º–æ–¥–µ–ª–∏ {name} –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
                continue

        if not base_probabilities:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∏ –æ—Ç –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏")

        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏
        meta_features = np.hstack(base_probabilities)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
        predictions_encoded = self.meta_model.predict(meta_features)

        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –æ—Ç –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
        if hasattr(self.meta_model, 'predict_proba'):
            probabilities = self.meta_model.predict_proba(meta_features)
        else:
            # –î–ª—è SVM –±–µ–∑ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            decision_scores = self.meta_model.decision_function(meta_features)
            probabilities = self._decision_to_probability_multiclass(decision_scores)

        return predictions_encoded, probabilities

    def _predict_stacking(self, X_vec: Any) -> Tuple[np.ndarray, np.ndarray]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ CatBoost –≤ –º–æ–¥–µ–ª—è—Ö
        has_catboost = any(name == 'catboost' for name in self.base_models.keys())

        try:
            if has_catboost and CATBOOST_AVAILABLE:
                X_dense = X_vec.toarray()
                predictions_encoded = self.stacking_model.predict(X_dense)
            else:
                predictions_encoded = self.stacking_model.predict(X_vec)

            # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            if hasattr(self.stacking_model, 'predict_proba'):
                if has_catboost and CATBOOST_AVAILABLE:
                    probabilities = self.stacking_model.predict_proba(X_dense)
                else:
                    probabilities = self.stacking_model.predict_proba(X_vec)
            else:
                # –ï—Å–ª–∏ predict_proba –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
                if has_catboost and CATBOOST_AVAILABLE:
                    decision_scores = self.stacking_model.decision_function(X_dense)
                else:
                    decision_scores = self.stacking_model.decision_function(X_vec)
                probabilities = self._decision_to_probability_multiclass(decision_scores)

            return predictions_encoded, probabilities

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ —Å—Ç–µ–∫–∏–Ω–≥ –º–æ–¥–µ–ª—å—é: {e}")
            raise

    def _decision_to_probability_multiclass(self, decision_scores: np.ndarray) -> np.ndarray:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ decision function –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        # –î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–≥–æ —Å–ª—É—á–∞—è decision_scores –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (n_samples, n_classes)
        if len(decision_scores.shape) == 1:
            # –ë–∏–Ω–∞—Ä–Ω—ã–π —Å–ª—É—á–∞–π
            decision_scores = decision_scores.reshape(-1, 1)
            decision_scores = np.hstack([-decision_scores, decision_scores])

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º softmax –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        exp_scores = np.exp(decision_scores - np.max(decision_scores, axis=1, keepdims=True))
        probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

        return probabilities

    def predict_single(self, text: str) -> Dict[str, Any]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        predictions, probabilities = self.predict([text])
        pred = predictions[0]
        pred_encoded = self.label_encoder.transform([pred])[0]
        prob = probabilities[0]

        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        class_probs = {}
        for i, cls in enumerate(self.class_names):
            class_probs[cls] = prob[i]

        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-3 –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        top_n = min(3, self.num_classes)
        top_indices = np.argsort(prob)[-top_n:][::-1]
        top_categories = []
        for idx in top_indices:
            top_categories.append({
                'category': self.class_names[idx],
                'probability': prob[idx],
                'probability_percent': prob[idx] * 100
            })

        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        base_predictions = self._get_base_predictions(text)

        return {
            'prediction': pred,
            'prediction_encoded': pred_encoded,
            'category': pred,
            'category_probabilities': class_probs,
            'top_categories': top_categories,
            'confidence': prob[pred_encoded],
            'confidence_percent': prob[pred_encoded] * 100,
            'base_predictions': base_predictions,
            'consensus': self._get_consensus(base_predictions) if base_predictions else None
        }

    def _get_base_predictions(self, text: str) -> Dict[str, Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Å–µ—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        X_vec = self.vectorizer.transform([text])
        base_results = {}

        if self.use_blending:
            # –î–ª—è –±–ª–µ–Ω–¥–∏–Ω–≥–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
            for name, model in self.base_models.items():
                try:
                    if name == 'catboost' and CATBOOST_AVAILABLE:
                        X_dense = X_vec.toarray()
                        pred_encoded = model.predict(X_dense)[0]
                        prob = model.predict_proba(X_dense)[0]
                    else:
                        pred_encoded = model.predict(X_vec)[0]
                        if hasattr(model, 'predict_proba'):
                            prob = model.predict_proba(X_vec)[0]
                        else:
                            # –î–ª—è SVM –±–µ–∑ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
                            decision_score = model.decision_function(X_vec)
                            prob = self._decision_to_probability_multiclass(decision_score)[0]

                    pred = self.label_encoder.inverse_transform([pred_encoded])[0]

                    base_results[name] = {
                        'prediction': pred,
                        'prediction_encoded': pred_encoded,
                        'probability': prob,
                        'top_category': self.class_names[np.argmax(prob)],
                        'top_probability': np.max(prob)
                    }
                except Exception as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ –º–æ–¥–µ–ª–∏ {name}: {e}")
                    continue
        else:
            # –î–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞ - –ø–æ–ª—É—á–∞–µ–º –∏–∑ named_estimators_
            try:
                for name, model in self.stacking_model.named_estimators_.items():
                    try:
                        if name == 'catboost' and CATBOOST_AVAILABLE:
                            X_dense = X_vec.toarray()
                            pred_encoded = model.predict(X_dense)[0]
                            prob = model.predict_proba(X_dense)[0]
                        else:
                            pred_encoded = model.predict(X_vec)[0]
                            if hasattr(model, 'predict_proba'):
                                prob = model.predict_proba(X_vec)[0]
                            else:
                                decision_score = model.decision_function(X_vec)
                                prob = self._decision_to_probability_multiclass(decision_score)[0]

                        pred = self.label_encoder.inverse_transform([pred_encoded])[0]

                        base_results[name] = {
                            'prediction': pred,
                            'prediction_encoded': pred_encoded,
                            'probability': prob,
                            'top_category': self.class_names[np.argmax(prob)],
                            'top_probability': np.max(prob)
                        }
                    except Exception as e:
                        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ –º–æ–¥–µ–ª–∏ {name}: {e}")
                        continue
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: {e}")

        return base_results

    def _get_consensus(self, base_predictions: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        predictions = [data['prediction'] for data in base_predictions.values()]

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –≥–æ–ª–æ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        vote_counts = {}
        for pred in predictions:
            vote_counts[pred] = vote_counts.get(pred, 0) + 1

        total_votes = len(predictions)
        max_votes = max(vote_counts.values()) if vote_counts else 0
        winning_category = max(vote_counts, key=vote_counts.get) if vote_counts else None

        return {
            'vote_counts': vote_counts,
            'total_votes': total_votes,
            'winning_category': winning_category,
            'max_votes': max_votes,
            'consensus_ratio': max_votes / total_votes if total_votes > 0 else 0,
            'unanimous': len(set(predictions)) == 1 if predictions else False
        }

    def evaluate(self, test_data: List[Dict[str, Any]],
                 detailed: bool = True,
                 plot_confusion_matrix: bool = True) -> Tuple[float, Dict]:
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        X_test, y_test_raw = self.prepare_data(test_data)

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
        y_test = []
        for label in y_test_raw:
            if label in self.label_encoder.classes_:
                y_test.append(label)
            else:
                # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –º–µ—Ç–∫–∏ –Ω–∞ –ø–µ—Ä–≤—É—é –∏–∑–≤–µ—Å—Ç–Ω—É—é
                y_test.append(self.class_names[0])

        y_test_encoded = self.label_encoder.transform(y_test)

        predictions, probabilities = self.predict(X_test)
        y_pred_encoded = self.label_encoder.transform(predictions)
        accuracy = accuracy_score(y_test_encoded, y_pred_encoded)

        if detailed:
            print(f"\nüìä –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•:")
            print(f"   –ü—Ä–∏–º–µ—Ä–æ–≤: {len(test_data)}")
            print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")

            print(f"\nüìà –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:")
            print(classification_report(y_test_encoded, y_pred_encoded,
                                        target_names=self.class_names, digits=3))

            # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
            if self.num_classes > 1:
                print(f"\nüìä –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö:")
                cm = confusion_matrix(y_test_encoded, y_pred_encoded)
                self._print_confusion_matrix(cm)

                if plot_confusion_matrix:
                    self._plot_confusion_matrix(y_test_encoded, y_pred_encoded,
                                                "Test Confusion Matrix")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        report_dict = classification_report(y_test_encoded, y_pred_encoded,
                                            target_names=self.class_names,
                                            output_dict=True)

        return accuracy, report_dict

    def _print_confusion_matrix(self, cm: np.ndarray) -> None:
        """
        –ö—Ä–∞—Å–∏–≤–æ –ø–µ—á–∞—Ç–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫
        """
        n_classes = len(self.class_names)

        if n_classes <= 1:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫")
            return

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        max_class_len = max(len(cls) for cls in self.class_names)
        header_padding = max(12, max_class_len + 2)

        header = " " * header_padding + " | "
        header += " ".join([f"{cls[:10]:>10}" for cls in self.class_names])
        print(header)
        print("-" * (header_padding + 3 + n_classes * 11))

        # –°—Ç—Ä–æ–∫–∏
        for i, cls in enumerate(self.class_names):
            row = f"{cls[:header_padding - 2]:>{header_padding - 2}} | "
            row += " ".join([f"{cm[i][j]:>10}" for j in range(n_classes)])
            print(row)

    def _plot_confusion_matrix(self, y_true: np.ndarray,
                               y_pred: np.ndarray,
                               title: str = "Confusion Matrix") -> None:
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
        """
        try:
            if self.num_classes <= 1:
                print("‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫")
                return

            cm = confusion_matrix(y_true, y_pred)

            plt.figure(figsize=(max(10, self.num_classes), max(8, self.num_classes * 0.8)))

            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º (–ø–æ –∏—Å—Ç–∏–Ω–Ω—ã–º –º–µ—Ç–∫–∞–º)
            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            cm_normalized = np.nan_to_num(cm_normalized)  # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ 0

            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                        xticklabels=self.class_names,
                        yticklabels=self.class_names,
                        vmin=0, vmax=1)
            plt.title(f"{title} (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–∞)")
            plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')
            plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')
            plt.tight_layout()

            filename = title.lower().replace(' ', '_').replace('-', '_')
            plt.savefig(f"{filename}.png", dpi=300, bbox_inches='tight')
            plt.show()
        except Exception as e:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫: {e}")

    def analyze_model_performance(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        """
        X, y_raw = self.prepare_data(data)

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
        y = []
        for label in y_raw:
            if label in self.label_encoder.classes_:
                y.append(label)
            else:
                y.append(self.class_names[0])

        y_encoded = self.label_encoder.transform(y)
        X_vec = self.vectorizer.transform(X)

        print("\nüìä –ê–ù–ê–õ–ò–ó –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò –ú–û–î–ï–õ–ï–ô:")
        print("=" * 60)

        base_accuracies = {}

        if self.use_blending:
            # –î–ª—è –±–ª–µ–Ω–¥–∏–Ω–≥–∞ - –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã –æ—Ç–¥–µ–ª—å–Ω–æ
            for name, model in self.base_models.items():
                try:
                    if name == 'catboost' and CATBOOST_AVAILABLE:
                        X_dense = X_vec.toarray()
                        pred_encoded = model.predict(X_dense)
                    else:
                        pred_encoded = model.predict(X_vec)

                    accuracy = accuracy_score(y_encoded, pred_encoded)
                    base_accuracies[name] = accuracy
                    print(f"   {name.upper():<12}: {accuracy:.3f}")
                except Exception as e:
                    print(f"   {name.upper():<12}: ‚ùå –û—à–∏–±–∫–∞: {e}")
        else:
            # –î–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞ - –ø–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
            try:
                for name, model in self.stacking_model.named_estimators_.items():
                    try:
                        if name == 'catboost' and CATBOOST_AVAILABLE:
                            X_dense = X_vec.toarray()
                            pred_encoded = model.predict(X_dense)
                        else:
                            pred_encoded = model.predict(X_vec)

                        accuracy = accuracy_score(y_encoded, pred_encoded)
                        base_accuracies[name] = accuracy
                        print(f"   {name.upper():<12}: {accuracy:.3f}")
                    except Exception as e:
                        print(f"   {name.upper():<12}: ‚ùå –û—à–∏–±–∫–∞: {e}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: {e}")

        # –û—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è
        ensemble_pred, _ = self.predict(X)
        ensemble_pred_encoded = self.label_encoder.transform(ensemble_pred)
        ensemble_accuracy = accuracy_score(y_encoded, ensemble_pred_encoded)
        print(f"   {'ENSEMBLE':<12}: {ensemble_accuracy:.3f}")

        # –£–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ª—É—á—à–µ–π –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
        improvement = 0
        if base_accuracies:
            best_base_accuracy = max(base_accuracies.values())
            improvement = ensemble_accuracy - best_base_accuracy
            print(f"\n   üìà –£–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–¥ –ª—É—á—à–µ–π –º–æ–¥–µ–ª—å—é: {improvement:.3f}")
            if best_base_accuracy > 0:
                print(f"   üìà –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ: {improvement / best_base_accuracy * 100:.1f}%")

        return {
            'base_accuracies': base_accuracies,
            'ensemble_accuracy': ensemble_accuracy,
            'improvement': improvement
        }

    def save_model(self, filename: str) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        if self.use_blending:
            to_save = {
                'base_models': self.base_models,
                'meta_model': self.meta_model,
                'vectorizer': self.vectorizer,
                'label_encoder': self.label_encoder,
                'class_names': self.class_names,
                'use_blending': self.use_blending,
                'meta_model_type': self.meta_model_type,
                'text_field': self.text_field,
                'label_field': self.label_field,
                'num_classes': self.num_classes
            }
        else:
            to_save = {
                'stacking_model': self.stacking_model,
                'vectorizer': self.vectorizer,
                'label_encoder': self.label_encoder,
                'class_names': self.class_names,
                'use_blending': self.use_blending,
                'meta_model_type': self.meta_model_type,
                'text_field': self.text_field,
                'label_field': self.label_field,
                'num_classes': self.num_classes
            }

        joblib.dump(to_save, filename)
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")

    def load_model(self, filename: str) -> None:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        loaded = joblib.load(filename)

        self.vectorizer = loaded['vectorizer']
        self.label_encoder = loaded['label_encoder']
        self.class_names = loaded['class_names']
        self.num_classes = loaded['num_classes']
        self.use_blending = loaded['use_blending']
        self.meta_model_type = loaded['meta_model_type']
        self.text_field = loaded.get('text_field', 'text')
        self.label_field = loaded.get('label_field', 'category')

        if self.use_blending:
            self.base_models = loaded['base_models']
            self.meta_model = loaded['meta_model']
        else:
            self.stacking_model = loaded['stacking_model']

        self.is_trained = True
        print(f"üì• –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filename}")
        print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {self.class_names}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {self.num_classes}")
        print(f"   –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {'Blending' if self.use_blending else 'Stacking'}")
        print(f"   –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å: {self.meta_model_type}")


def compare_ensemble_strategies(train_data: List[Dict[str, Any]],
                                val_data: List[Dict[str, Any]],
                                test_data: List[Dict[str, Any]],
                                text_field: str = 'text',
                                label_field: str = 'category') -> Dict[str, Any]:
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç–µ–∫–∏–Ω–≥–∞ –∏ –±–ª–µ–Ω–¥–∏–Ω–≥–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–∞-–º–æ–¥–µ–ª—è–º–∏
    """
    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –°–¢–†–ê–¢–ï–ì–ò–ô –ê–ù–°–ê–ú–ë–õ–ò–†–û–í–ê–ù–ò–Ø")
    print("=" * 60)

    strategies = [
        ('blending_logistic', True, 'logistic'),
        ('blending_svm', True, 'svm'),
        ('blending_rf', True, 'random_forest'),
        ('stacking_logistic', False, 'logistic'),
        ('stacking_svm', False, 'svm'),
        ('stacking_rf', False, 'random_forest'),
    ]

    results = {}

    for name, use_blending, meta_model in strategies:
        print(f"\nüéØ {name.upper()}:")
        try:
            ensemble = StackingCategoryClassifier(
                use_blending=use_blending,
                meta_model=meta_model,
                text_field=text_field,
                label_field=label_field,
                use_catboost=False  # –û—Ç–∫–ª—é—á–∞–µ–º CatBoost –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            )

            if use_blending:
                ensemble.train(train_data, val_data)
            else:
                ensemble.train(train_data, val_data)

            test_accuracy, _ = ensemble.evaluate(test_data, detailed=False)

            # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            performance = ensemble.analyze_model_performance(test_data)

            results[name] = {
                'model': ensemble,
                'accuracy': test_accuracy,
                'improvement': performance['improvement'] if 'improvement' in performance else 0
            }

            print(f"   ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {test_accuracy:.3f}")

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
            results[name] = {
                'model': None,
                'accuracy': 0,
                'error': str(e)
            }

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüìä –ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï:")
    print("=" * 50)

    successful_results = {k: v for k, v in results.items() if 'error' not in v and v['model'] is not None}

    if successful_results:
        for name, result in sorted(successful_results.items(),
                                   key=lambda x: x[1]['accuracy'],
                                   reverse=True):
            improvement_info = ""
            if result['improvement'] > 0:
                improvement_info = f" (+{result['improvement']:.3f})"
            print(f"   {name:<25}: {result['accuracy']:.3f}{improvement_info}")
    else:
        print("   ‚ùå –ù–∏ –æ–¥–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –Ω–µ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —É—Å–ø–µ—à–Ω–æ")

    return results


def quick_train_ensemble(train_file: str,
                         val_file: str,
                         test_file: Optional[str] = None,
                         text_field: str = 'text',
                         label_field: str = 'category',
                         use_blending: bool = True,
                         meta_model: str = 'logistic',
                         use_catboost: bool = False,
                         output_model: str = 'ensemble_category_classifier.pkl') -> Optional[
    StackingCategoryClassifier]:
    """
    –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –∏–∑ —Ñ–∞–π–ª–æ–≤
    """
    import json
    import os

    def load_jsonl(filepath: str) -> List[Dict[str, Any]]:
        if not os.path.exists(filepath):
            print(f"‚ö†Ô∏è  –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
            return []
        with open(filepath, 'r', encoding='utf-8') as f:
            return [json.loads(line) for line in f]

    print("üöÄ –ó–ê–ü–£–°–ö –ë–´–°–¢–†–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø –ê–ù–°–ê–ú–ë–õ–Ø")
    print("=" * 60)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    train_data = load_jsonl(train_file)
    if not train_data:
        print(f"‚ùå –û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {train_file}")
        return None

    val_data = load_jsonl(val_file)
    if not val_data:
        print(f"‚ùå –û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {val_file}")
        return None

    print(f"   Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"   Val: {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    if test_file:
        test_data = load_jsonl(test_file)
        print(f"   Test: {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    else:
        test_data = None

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
    if train_data:
        sample_item = train_data[0]
        if text_field not in sample_item:
            print(f"‚ùå –û—à–∏–±–∫–∞: –ø–æ–ª–µ '{text_field}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–∞–Ω–Ω—ã—Ö")
            return None
        if label_field not in sample_item:
            print(f"‚ùå –û—à–∏–±–∫–∞: –ø–æ–ª–µ '{label_field}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–∞–Ω–Ω—ã—Ö")
            return None

    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    print(f"\nüéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è...")
    print(f"   –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {'Blending' if use_blending else 'Stacking'}")
    print(f"   –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å: {meta_model}")
    print(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CatBoost: {'–î–∞' if use_catboost else '–ù–µ—Ç'}")

    classifier = StackingCategoryClassifier(
        use_blending=use_blending,
        meta_model=meta_model,
        text_field=text_field,
        label_field=label_field,
        use_catboost=use_catboost
    )

    results = classifier.train(train_data, val_data)

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º, –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    if test_data:
        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        accuracy, report = classifier.evaluate(test_data, detailed=True)
        print(f"\nüéØ –ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {accuracy:.3f}")

        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        performance = classifier.analyze_model_performance(test_data)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        if report:
            report_df = pd.DataFrame(report).transpose()
            report_df.to_csv('ensemble_classification_report.csv', index=True)
            print(f"üìÑ –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ 'ensemble_classification_report.csv'")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    classifier.save_model(output_model)

    # –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä
    print(f"\nüß™ –¢–ï–°–¢–û–í–´–ô –ü–†–ò–ú–ï–† –†–ê–ë–û–¢–´ –ú–û–î–ï–õ–ò:")
    if train_data:
        sample_text = train_data[0][text_field]
        if len(sample_text) > 100:
            sample_text = sample_text[:100] + "..."
        result = classifier.predict_single(sample_text)
        print(f"   –¢–µ–∫—Å—Ç: '{sample_text}'")
        print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {result['prediction']}")
        print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence_percent']:.1f}%")

        if result['top_categories']:
            print(f"   –¢–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:")
            for i, cat in enumerate(result['top_categories'], 1):
                print(f"     {i}. {cat['category']}: {cat['probability_percent']:.1f}%")

        if result['consensus']:
            print(f"   –ö–æ–Ω—Å–µ–Ω—Å—É—Å –º–æ–¥–µ–ª–µ–π:")
            for category, votes in result['consensus']['vote_counts'].items():
                print(f"     {category}: {votes} –≥–æ–ª–æ—Å–æ–≤")

    return classifier


def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å—Ç–µ–∫–∏–Ω–≥/–±–ª–µ–Ω–¥–∏–Ω–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    """
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        train_data = read_jsonl_basic('../util/news_category_train.jsonl')
        val_data = read_jsonl_basic('../util/news_category_val.jsonl')
        test_data = read_jsonl_basic('../util/news_category_test.jsonl')

        print(f"üìä –î–∞–Ω–Ω—ã–µ: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
        if train_data:
            print(f"\nüìã –ü–†–ò–ú–ï–† –î–ê–ù–ù–´–•:")
            sample = train_data[0]
            print(f"   –ü–æ–ª—è: {list(sample.keys())}")
            print(f"   –¢–µ–∫—Å—Ç: {sample.get('text', 'N/A')[:100]}...")
            print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {sample.get('category', 'N/A')}")

        # 1. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–ª–µ–Ω–¥–∏–Ω–≥–∞
        print("\n" + "=" * 60)
        print("üéØ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ë–õ–ï–ù–î–ò–ù–ì–ê")

        blending_classifier = StackingCategoryClassifier(
            use_blending=True,
            meta_model='logistic',
            text_field='text',
            label_field='category',
            use_catboost=False
        )

        blending_results = blending_classifier.train(train_data, val_data)

        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        print("\nüìä –ê–ù–ê–õ–ò–ó –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò –ë–õ–ï–ù–î–ò–ù–ì–ê:")
        blending_classifier.analyze_model_performance(test_data)

        # 2. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–µ–∫–∏–Ω–≥–∞
        print("\n" + "=" * 60)
        print("üéØ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –°–¢–ï–ö–ò–ù–ì–ê")

        stacking_classifier = StackingCategoryClassifier(
            use_blending=False,
            meta_model='logistic',
            text_field='text',
            label_field='category',
            use_catboost=False
        )

        stacking_classifier.train(train_data, val_data)

        print("\nüìä –ê–ù–ê–õ–ò–ó –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò –°–¢–ï–ö–ò–ù–ì–ê:")
        stacking_classifier.analyze_model_performance(test_data)

        # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        blending_classifier.save_model("blending_category_classifier.pkl")
        stacking_classifier.save_model("stacking_category_classifier.pkl")

        # 5. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π (–Ω–∞ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        print("\n" + "=" * 60)
        print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –í–°–ï–• –°–¢–†–ê–¢–ï–ì–ò–ô (–Ω–∞ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ)")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        if len(train_data) > 300:
            train_subset = train_data[:300]
            val_subset = val_data[:100]
            test_subset = test_data[:100] if test_data else None
            print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ: {len(train_subset)} train, "
                  f"{len(val_subset)} val")
        else:
            train_subset = train_data
            val_subset = val_data
            test_subset = test_data

        if test_subset:
            results = compare_ensemble_strategies(
                train_subset, val_subset, test_subset,
                'text', 'category'
            )

    except FileNotFoundError as e:
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
    except Exception as e:
        print(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞
    print("üöÄ –ó–ê–ü–£–°–ö –ü–†–ò–ú–ï–†–ê –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø –ê–ù–°–ê–ú–ë–õ–ï–ô")
    print("=" * 60)
    main()