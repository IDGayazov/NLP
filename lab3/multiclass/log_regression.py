from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import numpy as np
import joblib
import seaborn as sns
import matplotlib.pyplot as plt
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter

from util.jsonl_process import read_jsonl_basic


class MultiCategoryClassifier:
    """
    –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
    """

    def __init__(self,
                 regularization: str = 'l2',
                 C: float = 1.0,
                 class_names: Optional[List[str]] = None,
                 solver: str = 'lbfgs',
                 max_iter: int = 1000,
                 text_field: str = 'text',
                 label_field: str = 'category'):
        """
        Args:
            regularization: 'l1' –∏–ª–∏ 'l2' —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
            C: –ø–∞—Ä–∞–º–µ—Ç—Ä —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (–º–µ–Ω—å—à–µ = —Å–∏–ª—å–Ω–µ–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
            class_names: —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∞—Å—Å–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            solver: –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ ('lbfgs', 'newton-cg', 'saga', 'sag')
            max_iter: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π
            text_field: –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—è —Å —Ç–µ–∫—Å—Ç–æ–º
            label_field: –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—è —Å –º–µ—Ç–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        """
        self.vectorizer = TfidfVectorizer(
            max_features=10000,
            min_df=2,
            max_df=0.9,
            ngram_range=(1, 2),
            stop_words=None
        )

        # –î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º multinomial –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é
        self.model = LogisticRegression(
            penalty=regularization,
            C=C,
            random_state=42,
            solver=solver,
            max_iter=max_iter,
            multi_class='multinomial'
        )

        self.label_encoder = LabelEncoder()
        self.class_names = class_names
        self.is_trained = False
        self.num_classes = 0
        self.class_mapping = {}
        self.text_field = text_field
        self.label_field = label_field

    def prepare_data(self, data: List[Dict[str, Any]]) -> Tuple[List[str], List[str]]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        """
        texts = [item[self.text_field] for item in data]
        labels = [item[self.label_field] for item in data]
        return texts, labels

    def analyze_class_distribution(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö
        """
        _, labels = self.prepare_data(data)
        label_counts = Counter(labels)

        result = {
            'total_samples': len(data),
            'num_classes': len(label_counts),
            'classes': dict(label_counts),
            'class_percentages': {},
            'imbalance_ratio': None
        }

        if label_counts:
            max_count = max(label_counts.values())
            min_count = min(label_counts.values())
            if min_count > 0:
                result['imbalance_ratio'] = max_count / min_count

            for label, count in label_counts.items():
                result['class_percentages'][label] = count / len(data) * 100

        return result

    def train(self, train_data: List[Dict[str, Any]],
              val_data: Optional[List[Dict[str, Any]]] = None,
              auto_detect_classes: bool = True,
              handle_imbalance: bool = True) -> None:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        print("üéØ –û–ë–£–ß–ï–ù–ò–ï –ú–ù–û–ì–û–ö–õ–ê–°–°–û–í–û–ì–û –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê –ö–ê–¢–ï–ì–û–†–ò–ô...")
        print(f"   –ü–æ–ª–µ —Å —Ç–µ–∫—Å—Ç–æ–º: '{self.text_field}'")
        print(f"   –ü–æ–ª–µ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π: '{self.label_field}'")

        # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        train_dist = self.analyze_class_distribution(train_data)
        print(f"\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–ê–¢–ï–ì–û–†–ò–ô –í TRAIN:")
        print(f"   –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {train_dist['total_samples']}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {train_dist['num_classes']}")

        if train_dist['imbalance_ratio']:
            print(f"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞: {train_dist['imbalance_ratio']:.2f}")
            if train_dist['imbalance_ratio'] > 3:
                print("   ‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω —Å–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
                if handle_imbalance:
                    print("   ‚úÖ –ë—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–æ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤")
                    self.model.set_params(class_weight='balanced')
            else:
                print("   ‚úÖ –î–∏—Å–±–∞–ª–∞–Ω—Å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –Ω–æ—Ä–º—ã")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train_raw = self.prepare_data(train_data)

        # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
        if auto_detect_classes:
            self.label_encoder.fit(y_train_raw)
            y_train = self.label_encoder.transform(y_train_raw)
            self.class_names = list(self.label_encoder.classes_)
        else:
            if self.class_names is None:
                raise ValueError("class_names –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∑–∞–¥–∞–Ω, –µ—Å–ª–∏ auto_detect_classes=False")
            self.label_encoder.fit(self.class_names)
            y_train = self.label_encoder.transform(y_train_raw)

        self.num_classes = len(self.class_names)
        self.class_mapping = {i: cls for i, cls in enumerate(self.class_names)}

        # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö
        print(f"\nüìã –°–ü–ò–°–û–ö –ö–ê–¢–ï–ì–û–†–ò–ô ({self.num_classes}):")
        for i, (class_name, count) in enumerate(train_dist['classes'].items()):
            percentage = train_dist['class_percentages'].get(class_name, 0)
            print(f"   {i + 1:2d}. {class_name}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤ ({percentage:.1f}%)")

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
        print("\nüìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        X_train_vec = self.vectorizer.fit_transform(X_train)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_vec.shape}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤/—Ñ—Ä–∞–∑: {len(self.vectorizer.get_feature_names_out())}")
        print(f"   –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã: {X_train_vec.nnz / (X_train_vec.shape[0] * X_train_vec.shape[1]):.4f}")

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("\nü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        self.model.fit(X_train_vec, y_train)
        self.is_trained = True

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        train_pred = self.model.predict(X_train_vec)
        train_accuracy = accuracy_score(y_train, train_pred)
        print(f"\n‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train: {train_accuracy:.3f}")

        # –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º –Ω–∞ train
        print("\nüìä –û–¢–ß–ï–¢ –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú (train):")
        print(classification_report(y_train, train_pred, target_names=self.class_names))

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        if val_data:
            val_accuracy, _ = self.evaluate(val_data, detailed=False)
            print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ val: {val_accuracy:.3f}")

        # –ü–æ–∫–∞–∂–µ–º –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        self._show_important_features(top_n=12)

        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –Ω–∞ train
        self._plot_confusion_matrix(y_train, train_pred, "Train Confusion Matrix")

    def predict(self, texts: List[str]) -> Tuple[List[str], np.ndarray]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)
        predictions_encoded = self.model.predict(X_vec)
        predictions = self.label_encoder.inverse_transform(predictions_encoded)
        probabilities = self.model.predict_proba(X_vec)

        return predictions, probabilities

    def predict_single(self, text: str) -> Dict[str, Any]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        predictions, probabilities = self.predict([text])
        pred = predictions[0]
        pred_encoded = self.label_encoder.transform([pred])[0]
        prob = probabilities[0]

        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        class_probs = {}
        for i, cls in enumerate(self.class_names):
            class_probs[cls] = prob[i]

        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-3 –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        top_n = min(3, self.num_classes)
        top_indices = np.argsort(prob)[-top_n:][::-1]
        top_classes = []
        for idx in top_indices:
            top_classes.append({
                'category': self.class_names[idx],
                'probability': prob[idx],
                'probability_percent': prob[idx] * 100
            })

        return {
            'prediction': pred,
            'category': pred,  # –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            'prediction_encoded': pred_encoded,
            'category_probabilities': class_probs,
            'top_categories': top_classes,
            'confidence': prob[pred_encoded],
            'confidence_percent': prob[pred_encoded] * 100
        }

    def evaluate(self, test_data: List[Dict[str, Any]],
                 detailed: bool = True,
                 plot_confusion_matrix: bool = True) -> Tuple[float, Dict]:
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        X_test, y_test_raw = self.prepare_data(test_data)
        y_test = self.label_encoder.transform(y_test_raw)
        X_test_vec = self.vectorizer.transform(X_test)

        y_pred_encoded = self.model.predict(X_test_vec)
        y_pred = self.label_encoder.inverse_transform(y_pred_encoded)
        accuracy = accuracy_score(y_test, y_pred_encoded)

        # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        test_dist = self.analyze_class_distribution(test_data)

        if detailed:
            print(f"\nüìä –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•:")
            print(f"   –ü—Ä–∏–º–µ—Ä–æ–≤: {len(test_data)}")
            print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {test_dist['num_classes']}")
            print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")

            print(f"\nüìà –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:")
            print(classification_report(y_test, y_pred_encoded, target_names=self.class_names, digits=3))

            # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
            print(f"\nüìä –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö:")
            cm = confusion_matrix(y_test, y_pred_encoded)
            self._print_confusion_matrix(cm)

            if plot_confusion_matrix:
                self._plot_confusion_matrix(y_test, y_pred_encoded, "Test Confusion Matrix")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        report_dict = classification_report(y_test, y_pred_encoded,
                                            target_names=self.class_names,
                                            output_dict=True)

        return accuracy, report_dict

    def _print_confusion_matrix(self, cm: np.ndarray) -> None:
        """
        –ö—Ä–∞—Å–∏–≤–æ –ø–µ—á–∞—Ç–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫
        """
        n_classes = len(self.class_names)

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        max_class_len = max(len(cls) for cls in self.class_names)
        header_padding = max(12, max_class_len + 2)

        header = " " * header_padding + " | "
        header += " ".join([f"{cls[:10]:>10}" for cls in self.class_names])
        print(header)
        print("-" * (header_padding + 3 + n_classes * 11))

        # –°—Ç—Ä–æ–∫–∏
        for i, cls in enumerate(self.class_names):
            row = f"{cls[:header_padding - 2]:>{header_padding - 2}} | "
            row += " ".join([f"{cm[i][j]:>10}" for j in range(n_classes)])
            print(row)

    def _plot_confusion_matrix(self, y_true: np.ndarray,
                               y_pred: np.ndarray,
                               title: str = "Confusion Matrix") -> None:
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
        """
        try:
            cm = confusion_matrix(y_true, y_pred)
            plt.figure(figsize=(max(10, self.num_classes), max(8, self.num_classes * 0.8)))

            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º (–ø–æ –∏—Å—Ç–∏–Ω–Ω—ã–º –º–µ—Ç–∫–∞–º)
            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                        xticklabels=self.class_names,
                        yticklabels=self.class_names,
                        vmin=0, vmax=1)
            plt.title(f"{title} (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–∞)")
            plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')
            plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')
            plt.tight_layout()

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è–º–∏
            filename = title.lower().replace(' ', '_').replace('-', '_')
            plt.savefig(f"{filename}.png", dpi=300, bbox_inches='tight')
            plt.savefig(f"{filename}_highres.png", dpi=600, bbox_inches='tight')
            plt.show()
        except Exception as e:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫: {e}")

    def _show_important_features(self, top_n: int = 12) -> None:
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        """
        if not hasattr(self.model, 'coef_'):
            return

        feature_names = self.vectorizer.get_feature_names_out()

        print(f"\nüîç –¢–û–ü-{top_n} –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø –ö–ê–ñ–î–û–ô –ö–ê–¢–ï–ì–û–†–ò–ò:")

        # –î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —É –Ω–∞—Å –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        for class_idx, class_name in enumerate(self.class_names):
            print(f"\n   –ö–ê–¢–ï–ì–û–†–ò–Ø '{class_name}':")
            coef = self.model.coef_[class_idx]

            # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –¥–∞–Ω–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é)
            pos_indices = np.argsort(coef)[-top_n:][::-1]
            print(f"      –ü–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –î–õ–Ø –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:")
            for idx in pos_indices:
                print(f"        + {feature_names[idx]}: {coef[idx]:.3f}")

            # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—É–∫–∞–∑—ã–≤–∞—é—Ç –ü–†–û–¢–ò–í –¥–∞–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)
            neg_indices = np.argsort(coef)[:top_n]
            print(f"\n      –ü–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –ü–†–û–¢–ò–í –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:")
            for idx in neg_indices:
                print(f"        - {feature_names[idx]}: {coef[idx]:.3f}")

    def predict_proba_for_category(self, texts: List[str], target_category: str) -> np.ndarray:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        """
        if target_category not in self.class_names:
            raise ValueError(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è {target_category} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {self.class_names}")

        _, probabilities = self.predict(texts)
        category_idx = list(self.class_names).index(target_category)

        return probabilities[:, category_idx]

    def get_class_distribution(self, data: List[Dict[str, Any]]) -> Dict[str, int]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö
        """
        return self.analyze_class_distribution(data)

    def save_model(self, filename: str) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        joblib.dump({
            'model': self.model,
            'vectorizer': self.vectorizer,
            'label_encoder': self.label_encoder,
            'class_names': self.class_names,
            'class_mapping': self.class_mapping,
            'num_classes': self.num_classes,
            'text_field': self.text_field,
            'label_field': self.label_field
        }, filename)
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")

    def load_model(self, filename: str) -> None:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        loaded = joblib.load(filename)
        self.model = loaded['model']
        self.vectorizer = loaded['vectorizer']
        self.label_encoder = loaded['label_encoder']
        self.class_names = loaded['class_names']
        self.class_mapping = loaded['class_mapping']
        self.num_classes = loaded['num_classes']
        self.text_field = loaded.get('text_field', 'text')
        self.label_field = loaded.get('label_field', 'category')
        self.is_trained = True

        print(f"üì• –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filename}")
        print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {self.class_names}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {self.num_classes}")
        print(f"   –ü–æ–ª–µ —Å —Ç–µ–∫—Å—Ç–æ–º: '{self.text_field}'")
        print(f"   –ü–æ–ª–µ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π: '{self.label_field}'")

    def predict_batch_with_details(self, texts: List[str]) -> List[Dict[str, Any]]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        predictions, probabilities = self.predict(texts)

        results = []
        for i, (text, pred, probs) in enumerate(zip(texts, predictions, probabilities)):
            pred_encoded = self.label_encoder.transform([pred])[0]

            # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            top_n = min(3, self.num_classes)
            top_indices = np.argsort(probs)[-top_n:][::-1]
            top_categories = []
            for idx in top_indices:
                top_categories.append({
                    'category': self.class_names[idx],
                    'probability': probs[idx],
                    'probability_percent': probs[idx] * 100
                })

            results.append({
                'text': text,
                'prediction': pred,
                'predicted_category': pred,
                'confidence': probs[pred_encoded],
                'confidence_percent': probs[pred_encoded] * 100,
                'top_categories': top_categories,
                'all_probabilities': {cls: probs[i] for i, cls in enumerate(self.class_names)}
            })

        return results


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
def quick_train(train_file: str,
                val_file: Optional[str] = None,
                test_file: Optional[str] = None,
                text_field: str = 'text',
                label_field: str = 'category',
                output_model: str = 'category_classifier.pkl') -> MultiCategoryClassifier:
    """
    –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∏–∑ —Ñ–∞–π–ª–æ–≤
    """
    import json
    import os

    def load_jsonl(filepath: str) -> List[Dict[str, Any]]:
        if not os.path.exists(filepath):
            print(f"‚ö†Ô∏è  –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
            return []
        with open(filepath, 'r', encoding='utf-8') as f:
            return [json.loads(line) for line in f]

    print("üöÄ –ó–ê–ü–£–°–ö –ë–´–°–¢–†–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê –ö–ê–¢–ï–ì–û–†–ò–ô")
    print("=" * 60)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    train_data = load_jsonl(train_file)
    if not train_data:
        print(f"‚ùå –û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {train_file}")
        return None

    print(f"   Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    if val_file:
        val_data = load_jsonl(val_file)
        print(f"   Val: {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    else:
        val_data = None
        print(f"   Val: –Ω–µ —É–∫–∞–∑–∞–Ω")

    if test_file:
        test_data = load_jsonl(test_file)
        print(f"   Test: {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    else:
        test_data = None

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
    if train_data:
        sample_item = train_data[0]
        print(f"\nüìã –°–¢–†–£–ö–¢–£–†–ê –î–ê–ù–ù–´–•:")
        print(f"   –ü–æ–ª—è –≤ –¥–∞–Ω–Ω—ã—Ö: {list(sample_item.keys())}")
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–µ —Ç–µ–∫—Å—Ç–∞: '{text_field}'")
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: '{label_field}'")

        if text_field not in sample_item:
            print(f"‚ùå –û—à–∏–±–∫–∞: –ø–æ–ª–µ '{text_field}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–∞–Ω–Ω—ã—Ö")
            return None
        if label_field not in sample_item:
            print(f"‚ùå –û—à–∏–±–∫–∞: –ø–æ–ª–µ '{label_field}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–∞–Ω–Ω—ã—Ö")
            return None

    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    print(f"\nüéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    classifier = MultiCategoryClassifier(
        regularization='l2',
        C=1.0,
        class_names=None,  # –æ–ø—Ä–µ–¥–µ–ª–∏–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        solver='lbfgs',
        max_iter=1000,
        text_field=text_field,
        label_field=label_field
    )

    classifier.train(train_data, val_data, auto_detect_classes=True, handle_imbalance=True)

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º, –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    if test_data:
        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        accuracy, report = classifier.evaluate(test_data, detailed=True)
        print(f"\nüéØ –ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {accuracy:.3f}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        import pandas as pd
        report_df = pd.DataFrame(report).transpose()
        report_df.to_csv('classification_report.csv', index=True)
        print(f"üìÑ –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ 'classification_report.csv'")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    classifier.save_model(output_model)

    # –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä
    print(f"\nüß™ –¢–ï–°–¢–û–í–´–ô –ü–†–ò–ú–ï–† –†–ê–ë–û–¢–´ –ú–û–î–ï–õ–ò:")
    if train_data:
        sample_text = train_data[0][text_field][:100] + "..." if len(train_data[0][text_field]) > 100 else \
        train_data[0][text_field]
        result = classifier.predict_single(sample_text)
        print(f"   –¢–µ–∫—Å—Ç: '{sample_text}'")
        print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {result['prediction']}")
        print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence_percent']:.1f}%")

        if result['top_categories']:
            print(f"   –¢–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:")
            for i, cat in enumerate(result['top_categories'], 1):
                print(f"     {i}. {cat['category']}: {cat['probability_percent']:.1f}%")

    return classifier


def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç —Å –ø–æ–ª—è–º–∏ 'text' –∏ 'category')
    train_data = read_jsonl_basic('../util/news_category_train.jsonl')
    val_data = read_jsonl_basic('../util/news_category_val.jsonl')
    test_data = read_jsonl_basic('../util/news_category_test.jsonl')

    print(f"üìä –î–∞–Ω–Ω—ã–µ: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
    if train_data:
        print(f"\nüìã –ü–†–ò–ú–ï–† –î–ê–ù–ù–´–•:")
        print(f"   –ü–æ–ª—è: {list(train_data[0].keys())}")
        print(f"   –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {train_data[0].get('text', 'N/A')[:100]}...")
        print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {train_data[0].get('category', 'N/A')}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ–ª–µ–π
    text_field = 'text'
    label_field = 'category'

    # 1. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    print("\n" + "=" * 60)

    classifier = MultiCategoryClassifier(
        regularization='l2',
        C=1.0,
        class_names=None,  # –æ–ø—Ä–µ–¥–µ–ª–∏–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        solver='lbfgs',
        max_iter=1000,
        text_field=text_field,
        label_field=label_field
    )

    classifier.train(train_data, val_data, auto_detect_classes=True, handle_imbalance=True)

    # 2. –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if test_data:
        print("\n" + "=" * 60)
        test_accuracy, test_report = classifier.evaluate(test_data, detailed=True)
        print(f"\nüìä –ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {test_accuracy:.3f}")

    # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    classifier.save_model("multiclass_category_classifier.pkl")

    # 5. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
    print("\n" + "=" * 60)
    print("üîÑ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ò:")
    loaded_classifier = MultiCategoryClassifier()
    loaded_classifier.load_model("multiclass_category_classifier.pkl")

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    test_result = loaded_classifier.predict_single("–¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
    print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {test_result['prediction']}")


# –ö–æ–º–∞–Ω–¥–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
if __name__ == "__main__":
    print("üöÄ –ó–ê–ü–£–°–ö –ü–†–ò–ú–ï–†–ê –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê")
    print("=" * 60)
    main()