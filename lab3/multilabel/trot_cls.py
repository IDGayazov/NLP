from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import RandomizedSearchCV, cross_val_score
from sklearn.metrics import (
    classification_report,
    accuracy_score,
    confusion_matrix,
    hamming_loss,
    f1_score,
    precision_score,
    recall_score,
    multilabel_confusion_matrix,
    make_scorer
)
import numpy as np
import joblib
import pandas as pd
from scipy.stats import randint, uniform
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from itertools import cycle
from sklearn.preprocessing import MultiLabelBinarizer

warnings.filterwarnings('ignore')


# ============================================================================
# –ö–õ–ê–°–°–ò–ß–ï–°–ö–ò–ô –ü–û–î–•–û–î –° RANDOMIZEDSEARCHCV
# ============================================================================

class MultiLabelTextClassifier:
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω–æ–π (multilabel) –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤
    """

    def __init__(self, max_training_time=300, n_iter=50, random_state=42):
        """
        Args:
            max_training_time: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
            n_iter: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            random_state: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        """
        self.vectorizer = TfidfVectorizer(
            max_features=10000,
            min_df=2,
            max_df=0.8,
            ngram_range=(1, 2),
            stop_words=None,
            analyzer='word'
        )

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º MultiLabelBinarizer –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫
        self.label_binarizer = MultiLabelBinarizer()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
        # –î–ª—è multilabel –∏—Å–ø–æ–ª—å–∑—É–µ–º OneVsRestClassifier
        self.models = {
            'logistic': {
                'model': OneVsRestClassifier(LogisticRegression(random_state=random_state, max_iter=1000)),
                'params': {
                    'estimator__C': uniform(0.001, 100),
                    'estimator__penalty': ['l1', 'l2'],
                    'estimator__solver': ['liblinear', 'saga'],
                }
            },
            'svm_linear': {
                'model': OneVsRestClassifier(LinearSVC(random_state=random_state, dual=False)),
                'params': {
                    'estimator__C': uniform(0.1, 10),
                    'estimator__loss': ['squared_hinge'],
                }
            },
            'random_forest': {
                'model': OneVsRestClassifier(RandomForestClassifier(random_state=random_state)),
                'params': {
                    'estimator__n_estimators': randint(50, 300),
                    'estimator__max_depth': [None, 10, 20, 30],
                    'estimator__min_samples_split': randint(2, 20),
                }
            },
            'naive_bayes': {
                'model': OneVsRestClassifier(MultinomialNB()),
                'params': {
                    'estimator__alpha': uniform(0.001, 2.0)
                }
            },
        }

        self.max_training_time = max_training_time
        self.n_iter = n_iter
        self.random_state = random_state
        self.is_trained = False
        self.best_model = None
        self.best_model_name = None
        self.best_score = 0
        self.label_names = None
        self.history = {
            'train_accuracy': [],
            'val_accuracy': [],
            'train_f1': [],
            'val_f1': [],
            'train_hamming': [],
            'val_hamming': []
        }

        print(f"üöÄ Multi-label Classifier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {max_training_time} —Å–µ–∫")
        print(f"   –ò—Ç–µ—Ä–∞—Ü–∏–π –ø–æ–∏—Å–∫–∞: {n_iter}")
        print(f"   –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {list(self.models.keys())}")

    def prepare_data(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏
        """
        texts = [item['text'] for item in data]

        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –º–µ—Ç–∫–∏ —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ –ø–æ–ª–µ 'binary_labels'
        labels = [item['binary_labels'] for item in data]

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        if not hasattr(self.label_binarizer, 'classes_'):
            labels_binary = self.label_binarizer.fit_transform(labels)
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏
            self.label_names = [str(cls) for cls in self.label_binarizer.classes_]
            print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(self.label_names)}")
        else:
            labels_binary = self.label_binarizer.transform(labels)

        return texts, labels_binary

    def train(self, train_data, val_data=None):
        """
        –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        """
        print("üéØ –ê–í–¢–û–ú–ê–¢–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–û–î–ë–û–† –ú–û–î–ï–õ–ï–ô...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train = self.prepare_data(train_data)

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
        print("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        X_train_vec = self.vectorizer.fit_transform(X_train)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_vec.shape}")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–µ—Ç–æ–∫: {y_train.shape}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(y_train)}")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –µ—Å—Ç—å
        if val_data:
            X_val, y_val = self.prepare_data(val_data)
            X_val_vec = self.vectorizer.transform(X_val)
        else:
            X_val_vec, y_val = None, None

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –º–æ–¥–µ–ª–µ–π
        print("\nü§ñ –ó–ê–ü–£–°–ö –°–õ–£–ß–ê–ô–ù–û–ì–û –ü–û–ò–°–ö–ê –ü–û –ú–û–î–ï–õ–Ø–ú...")

        best_models = {}

        for model_name, model_config in self.models.items():
            print(f"   üîç –ü–æ–∏—Å–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è {model_name}...")

            try:
                # –°–ª—É—á–∞–π–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
                search = RandomizedSearchCV(
                    model_config['model'],
                    model_config['params'],
                    n_iter=self.n_iter // len(self.models),
                    cv=3,
                    scoring='f1_weighted',  # –ò—Å–ø–æ–ª—å–∑—É–µ–º weighted F1 –¥–ª—è multilabel
                    random_state=self.random_state,
                    n_jobs=-1,
                    verbose=0
                )

                search.fit(X_train_vec, y_train)

                # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
                val_score = None
                if val_data:
                    y_val_pred = search.best_estimator_.predict(X_val_vec)
                    val_score = f1_score(y_val, y_val_pred, average='weighted')

                best_models[model_name] = {
                    'model': search.best_estimator_,
                    'train_score': search.best_score_,
                    'val_score': val_score,
                    'params': search.best_params_
                }

                print(f"      ‚úÖ Train F1: {search.best_score_:.3f}" +
                      (f", Val F1: {val_score:.3f}" if val_score else ""))

            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–ª—è {model_name}: {e}")
                continue

        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if best_models:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π score –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ train score
            score_key = 'val_score' if val_data else 'train_score'

            self.best_model_name = max(
                best_models.keys(),
                key=lambda x: best_models[x][score_key] if best_models[x][score_key] is not None else 0
            )

            self.best_model = best_models[self.best_model_name]['model']
            self.best_score = best_models[self.best_model_name][score_key]
            self.best_params = best_models[self.best_model_name]['params']

            print(f"\nüèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {self.best_model_name}")
            print(f"   F1 Score: {self.best_score:.3f}")

            self.is_trained = True

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
            self._show_model_comparison(best_models)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
            if val_data:
                y_train_pred = self.best_model.predict(X_train_vec)
                y_val_pred = self.best_model.predict(X_val_vec)

                self.history['train_f1'].append(f1_score(y_train, y_train_pred, average='weighted'))
                self.history['val_f1'].append(f1_score(y_val, y_val_pred, average='weighted'))
                self.history['train_accuracy'].append(accuracy_score(y_train, y_train_pred))
                self.history['val_accuracy'].append(accuracy_score(y_val, y_val_pred))

                # –í—ã—á–∏—Å–ª—è–µ–º Hamming loss
                self.history['train_hamming'] = [hamming_loss(y_train, y_train_pred)]
                self.history['val_hamming'] = [hamming_loss(y_val, y_val_pred)]

                print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train: {self.history['train_accuracy'][-1]:.3f}")
                print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ val: {self.history['val_accuracy'][-1]:.3f}")
                print(f"‚úÖ Hamming loss –Ω–∞ train: {self.history['train_hamming'][-1]:.3f}")
                print(f"‚úÖ Hamming loss –Ω–∞ val: {self.history['val_hamming'][-1]:.3f}")
        else:
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å!")

    def _show_model_comparison(self, best_models):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        print(f"\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô:")
        print("-" * 50)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º—É score –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –ø–æ train score
        sorted_models = sorted(best_models.items(),
                               key=lambda x: x[1]['val_score'] if x[1]['val_score'] is not None else x[1][
                                   'train_score'],
                               reverse=True)

        for model_name, results in sorted_models:
            train_score = results['train_score']
            val_score = results['val_score']

            score_str = f"Train F1: {train_score:.3f}"
            if val_score is not None:
                score_str += f", Val F1: {val_score:.3f}"

            print(f"   {model_name:<15}: {score_str}")

    def predict(self, texts, threshold=0.5):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)

        # –î–ª—è multilabel –ø–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥
        if hasattr(self.best_model, "predict_proba"):
            probabilities = self.best_model.predict_proba(X_vec)
            predictions = (probabilities >= threshold).astype(int)
        else:
            # –î–ª—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ predict_proba (–Ω–∞–ø—Ä–∏–º–µ—Ä, LinearSVC)
            predictions = self.best_model.predict(X_vec)
            probabilities = None

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫
        predictions_labels = self.label_binarizer.inverse_transform(predictions)

        return predictions, predictions_labels, probabilities

    def evaluate(self, test_data, threshold=0.5):
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        X_test, y_test = self.prepare_data(test_data)
        X_test_vec = self.vectorizer.transform(X_test)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π predict –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        if hasattr(self.best_model, "predict_proba"):
            probabilities = self.best_model.predict_proba(X_test_vec)
            y_pred = (probabilities >= threshold).astype(int)
        else:
            y_pred = self.best_model.predict(X_test_vec)

        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, y_pred)
        h_loss = hamming_loss(y_test, y_pred)
        f1_macro = f1_score(y_test, y_pred, average='macro')
        f1_weighted = f1_score(y_test, y_pred, average='weighted')
        f1_samples = f1_score(y_test, y_pred, average='samples')
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')

        print("\nüìä –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print("-" * 50)
        print(f"Accuracy: {accuracy:.3f}")
        print(f"Hamming Loss: {h_loss:.3f}")
        print(f"Precision (weighted): {precision:.3f}")
        print(f"Recall (weighted): {recall:.3f}")
        print(f"F1 Macro: {f1_macro:.3f}")
        print(f"F1 Weighted: {f1_weighted:.3f}")
        print(f"F1 Samples: {f1_samples:.3f}")

        # –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º
        print("\nüìà CLASSIFICATION REPORT (–ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É):")

        # –î–ª—è multilabel –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤—ã–≤–æ–¥–∏–º –æ—Ç—á–µ—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –æ—Ç–¥–µ–ª—å–Ω–æ
        for i, label_name in enumerate(self.label_names):
            print(f"\n--- –ö–ª–∞—Å—Å: {label_name} ---")
            try:
                # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –≤—ã–≤–æ–¥–∏–º –º–µ—Ç—Ä–∏–∫–∏ –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                y_test_class = y_test[:, i]
                y_pred_class = y_pred[:, i]

                # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∫–ª–∞—Å—Å–∞
                if len(np.unique(y_test_class)) > 1:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –æ–±–∞ –∫–ª–∞—Å—Å–∞
                    print(f"Precision: {precision_score(y_test_class, y_pred_class, zero_division=0):.3f}")
                    print(f"Recall: {recall_score(y_test_class, y_pred_class, zero_division=0):.3f}")
                    print(f"F1: {f1_score(y_test_class, y_pred_class, zero_division=0):.3f}")
                else:
                    print("–¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

                # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –¥–ª—è –∫–ª–∞—Å—Å–∞
                tn, fp, fn, tp = confusion_matrix(y_test_class, y_pred_class).ravel()
                print(f"TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}")

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∫–ª–∞—Å—Å–∞ {label_name}: {e}")

        return {
            'accuracy': accuracy,
            'hamming_loss': h_loss,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'f1_samples': f1_samples,
            'precision': precision,
            'recall': recall
        }

    def plot_training_history(self, save_path=None):
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è
        """
        if not self.history['train_accuracy']:
            print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è")
            return

        fig, axes = plt.subplots(1, 3, figsize=(15, 4))

        # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
        axes[0].plot(self.history['train_accuracy'], label='Train Accuracy', marker='o', linewidth=2)
        if self.history['val_accuracy']:
            axes[0].plot(self.history['val_accuracy'], label='Val Accuracy', marker='s', linewidth=2)
        axes[0].set_title('Accuracy')
        axes[0].set_xlabel('Model Selection')
        axes[0].set_ylabel('Accuracy')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # –ì—Ä–∞—Ñ–∏–∫ F1-score
        axes[1].plot(self.history['train_f1'], label='Train F1', marker='o', linewidth=2)
        if self.history['val_f1']:
            axes[1].plot(self.history['val_f1'], label='Val F1', marker='s', linewidth=2)
        axes[1].set_title('F1 Score (weighted)')
        axes[1].set_xlabel('Model Selection')
        axes[1].set_ylabel('F1 Score')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

        # –ì—Ä–∞—Ñ–∏–∫ Hamming Loss –µ—Å–ª–∏ –µ—Å—Ç—å
        if self.history.get('train_hamming'):
            axes[2].plot(self.history['train_hamming'], label='Train Hamming Loss', marker='o', linewidth=2)
            if self.history.get('val_hamming'):
                axes[2].plot(self.history['val_hamming'], label='Val Hamming Loss', marker='s', linewidth=2)
            axes[2].set_title('Hamming Loss')
            axes[2].set_xlabel('Model Selection')
            axes[2].set_ylabel('Hamming Loss')
            axes[2].legend()
            axes[2].grid(True, alpha=0.3)

        plt.suptitle(f'Best Model: {self.best_model_name}', fontsize=12)
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"üíæ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {save_path}")

        plt.show()

    def get_model_info(self):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
        """
        if not self.is_trained:
            return {"error": "–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞"}

        label_names_list = self.label_names if self.label_names is not None else []

        return {
            'model_name': self.best_model_name,
            'model_type': type(self.best_model).__name__,
            'best_score': self.best_score,
            'feature_count': len(self.vectorizer.get_feature_names_out()),
            'label_count': len(label_names_list),
        }

    def save_model(self, filename):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        label_names_to_save = list(self.label_names) if self.label_names is not None else None

        joblib.dump({
            'best_model': self.best_model,
            'vectorizer': self.vectorizer,
            'label_binarizer': self.label_binarizer,
            'best_model_name': self.best_model_name,
            'best_score': self.best_score,
            'best_params': self.best_params,
            'label_names': label_names_to_save,
            'history': self.history
        }, filename, compress=3)
        print(f"üíæ Multi-label –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")

    def load_model(self, filename):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        loaded = joblib.load(filename)
        self.best_model = loaded['best_model']
        self.vectorizer = loaded['vectorizer']
        self.label_binarizer = loaded['label_binarizer']
        self.best_model_name = loaded['best_model_name']
        self.best_score = loaded.get('best_score', 0)
        self.best_params = loaded.get('best_params', {})

        loaded_label_names = loaded.get('label_names', None)
        if loaded_label_names is not None:
            self.label_names = [str(cls) for cls in loaded_label_names]
        else:
            self.label_names = None

        self.history = loaded.get('history', {})
        self.is_trained = True
        print(f"üì• Multi-label –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filename}")


# ============================================================================
# TPOT –ü–û–î–•–û–î (–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –ú–ê–®–ò–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï)
# ============================================================================

try:
    from tpot import TPOTClassifier

    TPOT_AVAILABLE = True
    print("‚úÖ TPOT –¥–æ—Å—Ç—É–ø–µ–Ω")
except ImportError:
    print("‚ö†Ô∏è TPOT –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install tpot")
    TPOT_AVAILABLE = False


class TPOTMultiLabelClassifier:
    """
    AutoML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ TPOT –¥–ª—è –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞
    """

    def __init__(self, max_time_mins=5, generations=5, population_size=20,
                 cv=3, random_state=42, verbosity=2):
        """
        Args:
            max_time_mins: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –≤ –º–∏–Ω—É—Ç–∞—Ö
            generations: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–∫–æ–ª–µ–Ω–∏–π –≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
            population_size: —Ä–∞–∑–º–µ—Ä –ø–æ–ø—É–ª—è—Ü–∏–∏
            cv: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
            random_state: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
            verbosity: —É—Ä–æ–≤–µ–Ω—å –≤—ã–≤–æ–¥–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (0-3)
        """
        if not TPOT_AVAILABLE:
            raise ImportError("TPOT –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install tpot")

        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            min_df=2,
            max_df=0.9,
            ngram_range=(1, 2),
            stop_words=None
        )

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º MultiLabelBinarizer
        self.label_binarizer = MultiLabelBinarizer()

        # –°–æ–∑–¥–∞–µ–º scorer –¥–ª—è TPOT
        # –í –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏—è—Ö TPOT –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å scoring_func –≤–º–µ—Å—Ç–æ scoring
        f1_weighted_scorer = make_scorer(f1_score, average='weighted')

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º TPOT –¥–ª—è –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        # TPOT –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç multilabel —á–µ—Ä–µ–∑ OneVsRestClassifier
        try:
            # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–µ—Ä—Å–∏–∏ TPOT
            self.tpot = TPOTClassifier(
                generations=generations,
                population_size=population_size,
                cv=cv,
                random_state=random_state,
                verbosity=verbosity,  # –°—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è
                max_time_mins=max_time_mins,
                n_jobs=-1,
                config_dict='TPOT light',  # –ë–æ–ª–µ–µ –±—ã—Å—Ç—Ä—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                template='Selector-Transformer-Classifier'  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —à–∞–±–ª–æ–Ω
            )
        except TypeError as e:
            if "'scoring'" in str(e) or "'verbosity'" in str(e):
                # –ü—Ä–æ–±—É–µ–º —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏ TPOT...")
                self.tpot = TPOTClassifier(
                    generations=generations,
                    population_size=population_size,
                    cv=cv,
                    random_state=random_state,
                    max_time_mins=max_time_mins,
                    n_jobs=-1,
                    config_dict='TPOT light',
                    template='Selector-Transformer-Classifier',
                    scoring=f1_weighted_scorer,  # –ù–æ–≤–∞—è –≤–µ—Ä—Å–∏—è
                    verbosity=verbosity  # –ù–æ–≤–∞—è –≤–µ—Ä—Å–∏—è
                )
            else:
                raise e

        self.max_time_mins = max_time_mins
        self.random_state = random_state
        self.is_trained = False
        self.label_names = None
        self.training_history = []

        print(f"üß¨ TPOT Multi-label Classifier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {max_time_mins} –º–∏–Ω")
        print(f"   –ü–æ–∫–æ–ª–µ–Ω–∏–π: {generations}")
        print(f"   –†–∞–∑–º–µ—Ä –ø–æ–ø—É–ª—è—Ü–∏–∏: {population_size}")
        print(f"   –ú–µ—Ç—Ä–∏–∫–∞: F1 weighted")

    def prepare_data(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏
        """
        texts = [item['text'] for item in data]
        labels = [item['binary_labels'] for item in data]

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        if not hasattr(self.label_binarizer, 'classes_'):
            labels_binary = self.label_binarizer.fit_transform(labels)
            self.label_names = [str(cls) for cls in self.label_binarizer.classes_]
            print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(self.label_names)}")
        else:
            labels_binary = self.label_binarizer.transform(labels)

        return texts, labels_binary

    def train(self, train_data, val_data=None):
        """
        –û–±—É—á–µ–Ω–∏–µ TPOT –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        """
        print("üß¨ –ó–ê–ü–£–°–ö –ì–ï–ù–ï–¢–ò–ß–ï–°–ö–û–ì–û –ü–†–û–ì–†–ê–ú–ú–ò–†–û–í–ê–ù–ò–Ø TPOT...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train = self.prepare_data(train_data)

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
        print("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        X_train_vec = self.vectorizer.fit_transform(X_train)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_vec.shape}")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–µ—Ç–æ–∫: {y_train.shape}")

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–ª–æ—Ç–Ω—ã–π –º–∞—Å—Å–∏–≤ –¥–ª—è TPOT
        X_train_dense = X_train_vec.toarray()

        # –û–±—É—á–µ–Ω–∏–µ TPOT
        print("\nüéØ TPOT –∏—â–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω...")
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {self.tpot.n_jobs} —è–¥–µ—Ä CPU")

        try:
            self.tpot.fit(X_train_dense, y_train)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ TPOT: {e}")
            print("–ü–æ–ø—Ä–æ–±—É–µ–º –æ–±—É—á–∏—Ç—å —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

            # –ü—Ä–æ–±—É–µ–º —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            self.vectorizer = TfidfVectorizer(
                max_features=1000,
                min_df=2,
                max_df=0.9,
                ngram_range=(1, 1),
                stop_words=None
            )

            X_train_vec = self.vectorizer.fit_transform(X_train)
            X_train_dense = X_train_vec.toarray()
            print(f"   –ù–æ–≤–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_vec.shape}")

            self.tpot.fit(X_train_dense, y_train)

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
        if val_data:
            X_val, y_val = self.prepare_data(val_data)
            X_val_vec = self.vectorizer.transform(X_val)
            X_val_dense = X_val_vec.toarray()

            y_val_pred = self.tpot.predict(X_val_dense)
            val_score = f1_score(y_val, y_val_pred, average='weighted')
            val_accuracy = accuracy_score(y_val, y_val_pred)

            print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:")
            print(f"   F1 Weighted: {val_score:.3f}")
            print(f"   Accuracy: {val_accuracy:.3f}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
            self.training_history.append({
                'val_f1': val_score,
                'val_accuracy': val_accuracy
            })

        self.is_trained = True

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê TPOT:")
        try:
            cv_score = self.tpot.score(X_train_dense, y_train)
            print(f"   –û—Ü–µ–Ω–∫–∞ –Ω–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {cv_score:.3f}")
        except:
            print(f"   –û—Ü–µ–Ω–∫–∞ –Ω–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞")

        try:
            print(f"   –ü–æ–∫–æ–ª–µ–Ω–∏–π –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: {self.tpot.generations_}")
        except:
            pass

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ª—É—á—à–∏–π –ø–∞–π–ø–ª–∞–π–Ω
        print(f"\nüèÜ –õ–£–ß–®–ò–ô –ü–ê–ô–ü–õ–ê–ô–ù TPOT:")
        try:
            print(self.tpot.fitted_pipeline_)
        except:
            print("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–π–ø–ª–∞–π–Ω–µ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞")

        # –≠–∫—Å–ø–æ—Ä—Ç –∫–æ–¥–∞ –ª—É—á—à–µ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞
        try:
            export_filename = f'tpot_best_pipeline_{self.random_state}.py'
            self.tpot.export(export_filename)
            print(f"üíæ –ö–æ–¥ –ª—É—á—à–µ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {export_filename}")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–∞–π–ø–ª–∞–π–Ω: {e}")

    def predict(self, texts, threshold=0.5):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)
        X_dense = X_vec.toarray()

        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = self.tpot.predict(X_dense)

        # –î–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
        probabilities = None
        if hasattr(self.tpot.fitted_pipeline_, "predict_proba"):
            try:
                probabilities = self.tpot.predict_proba(X_dense)
            except:
                pass

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫
        predictions_labels = self.label_binarizer.inverse_transform(predictions)

        return predictions, predictions_labels, probabilities

    def evaluate(self, test_data, threshold=0.5):
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        X_test, y_test = self.prepare_data(test_data)
        X_test_vec = self.vectorizer.transform(X_test)
        X_test_dense = X_test_vec.toarray()

        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = self.tpot.predict(X_test_dense)

        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, y_pred)
        h_loss = hamming_loss(y_test, y_pred)
        f1_macro = f1_score(y_test, y_pred, average='macro')
        f1_weighted = f1_score(y_test, y_pred, average='weighted')
        f1_samples = f1_score(y_test, y_pred, average='samples')
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')

        print("\nüìä –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ TPOT:")
        print("-" * 50)
        print(f"Accuracy: {accuracy:.3f}")
        print(f"Hamming Loss: {h_loss:.3f}")
        print(f"Precision (weighted): {precision:.3f}")
        print(f"Recall (weighted): {recall:.3f}")
        print(f"F1 Macro: {f1_macro:.3f}")
        print(f"F1 Weighted: {f1_weighted:.3f}")
        print(f"F1 Samples: {f1_samples:.3f}")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ª—É—á—à–∏–π –ø–∞–π–ø–ª–∞–π–Ω
        print(f"\nüèÜ –õ–£–ß–®–ò–ô –ü–ê–ô–ü–õ–ê–ô–ù:")
        try:
            print(self.tpot.fitted_pipeline_)
        except:
            print("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–π–ø–ª–∞–π–Ω–µ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞")

        return {
            'accuracy': accuracy,
            'hamming_loss': h_loss,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'f1_samples': f1_samples,
            'precision': precision,
            'recall': recall
        }

    def get_model_info(self):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
        """
        if not self.is_trained:
            return {"error": "–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞"}

        label_names_list = self.label_names if self.label_names is not None else []

        info = {
            'model_type': 'TPOT AutoML',
            'feature_count': len(self.vectorizer.get_feature_names_out()),
            'label_count': len(label_names_list),
            'training_time': f"{self.max_time_mins} –º–∏–Ω"
        }

        try:
            info['best_pipeline'] = str(self.tpot.fitted_pipeline_)
        except:
            info['best_pipeline'] = "–ù–µ –¥–æ—Å—Ç—É–ø–µ–Ω"

        return info

    def save_model(self, filename):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        label_names_to_save = list(self.label_names) if self.label_names is not None else None

        joblib.dump({
            'tpot': self.tpot,
            'vectorizer': self.vectorizer,
            'label_binarizer': self.label_binarizer,
            'label_names': label_names_to_save,
            'training_history': self.training_history,
            'max_time_mins': self.max_time_mins,
            'random_state': self.random_state
        }, filename, compress=3)
        print(f"üíæ TPOT –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")

    def load_model(self, filename):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        loaded = joblib.load(filename)
        self.tpot = loaded['tpot']
        self.vectorizer = loaded['vectorizer']
        self.label_binarizer = loaded['label_binarizer']

        loaded_label_names = loaded.get('label_names', None)
        if loaded_label_names is not None:
            self.label_names = [str(cls) for cls in loaded_label_names]
        else:
            self.label_names = None

        self.training_history = loaded.get('training_history', [])
        self.max_time_mins = loaded.get('max_time_mins', 5)
        self.random_state = loaded.get('random_state', 42)
        self.is_trained = True
        print(f"üì• TPOT –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filename}")


# ============================================================================
# –£–ü–†–û–©–ï–ù–ù–´–ô TPOT –î–õ–Ø –ë–´–°–¢–†–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø
# ============================================================================

class SimpleTPOTMultiLabelClassifier:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π TPOT –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    """

    def __init__(self, max_time_mins=2, random_state=42):
        """
        Args:
            max_time_mins: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –≤ –º–∏–Ω—É—Ç–∞—Ö
            random_state: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        """
        if not TPOT_AVAILABLE:
            raise ImportError("TPOT –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install tpot")

        self.vectorizer = TfidfVectorizer(
            max_features=1000,  # –ï—â–µ –º–µ–Ω—å—à–µ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            min_df=1,
            max_df=0.95,
            ngram_range=(1, 1)
        )

        self.label_binarizer = MultiLabelBinarizer()

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º try-except –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ TPOT
        try:
            self.tpot = TPOTClassifier(
                generations=1,
                population_size=5,
                cv=2,
                random_state=random_state,
                max_time_mins=max_time_mins,
                n_jobs=1,  # –¢–æ–ª—å–∫–æ 1 —è–¥—Ä–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                config_dict='TPOT light',
                verbosity=1
            )
        except TypeError:
            # –ù–æ–≤–∞—è –≤–µ—Ä—Å–∏—è TPOT
            accuracy_scorer = make_scorer(accuracy_score)
            self.tpot = TPOTClassifier(
                generations=1,
                population_size=5,
                cv=2,
                random_state=random_state,
                max_time_mins=max_time_mins,
                n_jobs=1,
                config_dict='TPOT light',
                verbosity=1,
                scoring=accuracy_scorer
            )

        self.max_time_mins = max_time_mins
        self.random_state = random_state
        self.is_trained = False
        self.label_names = None

        print(f"‚ö° Simple TPOT Classifier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {max_time_mins} –º–∏–Ω")
        print(f"   –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")

    def prepare_data(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        """
        texts = [item['text'] for item in data]
        labels = [item['binary_labels'] for item in data]

        if not hasattr(self.label_binarizer, 'classes_'):
            labels_binary = self.label_binarizer.fit_transform(labels)
            self.label_names = [str(cls) for cls in self.label_binarizer.classes_]
            print(f"   –ö–ª–∞—Å—Å–æ–≤: {len(self.label_names)}")
        else:
            labels_binary = self.label_binarizer.transform(labels)

        return texts, labels_binary

    def train(self, train_data, val_data=None):
        """
        –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        """
        print("‚ö° –ë–´–°–¢–†–û–ï –û–ë–£–ß–ï–ù–ò–ï TPOT...")

        X_train, y_train = self.prepare_data(train_data)
        X_train_vec = self.vectorizer.fit_transform(X_train)
        X_train_dense = X_train_vec.toarray()

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {X_train_vec.shape}")
        print(f"   –ü—Ä–∏–º–µ—Ä–æ–≤: {len(X_train)}")
        print(f"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_vec.shape[1]}")
        print(f"   –ú–µ—Ç–æ–∫: {y_train.shape[1]}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –Ω–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
        if X_train_dense.shape[0] > 1000 or X_train_dense.shape[1] > 1000:
            print("   ‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –±–æ–ª—å—à–∏–µ, TPOT –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ")
            print("   –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å...")

            # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
            self.vectorizer = TfidfVectorizer(
                max_features=500,
                min_df=1,
                max_df=0.95,
                ngram_range=(1, 1)
            )
            X_train_vec = self.vectorizer.fit_transform(X_train)
            X_train_dense = X_train_vec.toarray()
            print(f"   –ù–æ–≤–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {X_train_vec.shape}")

        print("   üèÉ –ó–∞–ø—É—Å–∫ TPOT...")
        self.tpot.fit(X_train_dense, y_train)
        self.is_trained = True

        print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        try:
            cv_score = self.tpot.score(X_train_dense, y_train)
            print(f"   CV Score: {cv_score:.3f}")
        except:
            print(f"   CV Score: –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")

    def predict(self, texts, threshold=0.5):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)
        X_dense = X_vec.toarray()

        predictions = self.tpot.predict(X_dense)
        predictions_labels = self.label_binarizer.inverse_transform(predictions)

        return predictions, predictions_labels, None

    def evaluate(self, test_data):
        """
        –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞
        """
        X_test, y_test = self.prepare_data(test_data)
        X_test_vec = self.vectorizer.transform(X_test)
        X_test_dense = X_test_vec.toarray()

        y_pred = self.tpot.predict(X_test_dense)

        accuracy = accuracy_score(y_test, y_pred)
        h_loss = hamming_loss(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')

        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Simple TPOT:")
        print(f"   Accuracy: {accuracy:.3f}")
        print(f"   Hamming Loss: {h_loss:.3f}")
        print(f"   F1 Weighted: {f1:.3f}")

        return {'accuracy': accuracy, 'hamming_loss': h_loss, 'f1_weighted': f1}


# ============================================================================
# –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–´–ô –ü–û–î–•–û–î –ë–ï–ó TPOT
# ============================================================================

class LightAutoMLClassifier:
    """
    –õ–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π AutoML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –±–µ–∑ TPOT
    """

    def __init__(self, random_state=42):
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            min_df=2,
            max_df=0.9,
            ngram_range=(1, 2)
        )

        self.label_binarizer = MultiLabelBinarizer()
        self.best_model = None
        self.best_model_name = None
        self.label_names = None
        self.is_trained = False
        self.random_state = random_state

        print(f"‚ö° Light AutoML Classifier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def prepare_data(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        """
        texts = [item['text'] for item in data]
        labels = [item['binary_labels'] for item in data]

        if not hasattr(self.label_binarizer, 'classes_'):
            labels_binary = self.label_binarizer.fit_transform(labels)
            self.label_names = [str(cls) for cls in self.label_binarizer.classes_]
            print(f"   –ö–ª–∞—Å—Å–æ–≤: {len(self.label_names)}")
        else:
            labels_binary = self.label_binarizer.transform(labels)

        return texts, labels_binary

    def train(self, train_data, val_data=None):
        """
        –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        """
        print("‚ö° –ë–´–°–¢–†–û–ï –û–ë–£–ß–ï–ù–ò–ï LIGHT AUTOML...")

        X_train, y_train = self.prepare_data(train_data)
        X_train_vec = self.vectorizer.fit_transform(X_train)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {X_train_vec.shape}")

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ—Å—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π
        models = {
            'logistic': OneVsRestClassifier(LogisticRegression(max_iter=1000, random_state=self.random_state)),
            'naive_bayes': OneVsRestClassifier(MultinomialNB()),
            'random_forest': OneVsRestClassifier(
                RandomForestClassifier(n_estimators=50, random_state=self.random_state))
        }

        best_score = 0
        best_model = None
        best_name = None

        # –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö
        sample_size = min(500, X_train_vec.shape[0])
        if X_train_vec.shape[0] > sample_size:
            indices = np.random.choice(X_train_vec.shape[0], sample_size, replace=False)
            X_sample = X_train_vec[indices]
            y_sample = y_train[indices]
        else:
            X_sample = X_train_vec
            y_sample = y_train

        print(f"   –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –Ω–∞ {X_sample.shape[0]} –ø—Ä–∏–º–µ—Ä–∞—Ö...")

        for name, model in models.items():
            print(f"   üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º {name}...")
            try:
                # –ë—ã—Å—Ç—Ä–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
                scores = cross_val_score(model, X_sample, y_sample, cv=3,
                                         scoring='f1_weighted', n_jobs=1)
                score = np.mean(scores)
                print(f"      F1: {score:.3f}")

                if score > best_score:
                    best_score = score
                    best_model = model
                    best_name = name
            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞: {e}")

        # –û–±—É—á–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        if best_model:
            print(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_name} (F1: {best_score:.3f})")
            print(f"   –û–±—É—á–∞–µ–º –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...")
            best_model.fit(X_train_vec, y_train)
            self.best_model = best_model
            self.best_model_name = best_name
            self.is_trained = True

            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
            if val_data:
                X_val, y_val = self.prepare_data(val_data)
                X_val_vec = self.vectorizer.transform(X_val)
                y_val_pred = self.best_model.predict(X_val_vec)
                val_score = f1_score(y_val, y_val_pred, average='weighted')
                print(f"   F1 –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {val_score:.3f}")
        else:
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å")

    def predict(self, texts, threshold=0.5):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)

        predictions = self.best_model.predict(X_vec)
        predictions_labels = self.label_binarizer.inverse_transform(predictions)

        return predictions, predictions_labels, None

    def evaluate(self, test_data):
        """
        –û—Ü–µ–Ω–∫–∞
        """
        X_test, y_test = self.prepare_data(test_data)
        X_test_vec = self.vectorizer.transform(X_test)

        y_pred = self.best_model.predict(X_test_vec)

        accuracy = accuracy_score(y_test, y_pred)
        h_loss = hamming_loss(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')

        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Light AutoML:")
        print(f"   Accuracy: {accuracy:.3f}")
        print(f"   Hamming Loss: {h_loss:.3f}")
        print(f"   F1 Weighted: {f1:.3f}")
        print(f"   –ú–æ–¥–µ–ª—å: {self.best_model_name}")

        return {'accuracy': accuracy, 'hamming_loss': h_loss, 'f1_weighted': f1}

    def get_model_info(self):
        """
        –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        """
        if not self.is_trained:
            return {"error": "–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞"}

        return {
            'model_type': 'Light AutoML',
            'model_name': self.best_model_name,
            'feature_count': len(self.vectorizer.get_feature_names_out()),
            'label_count': len(self.label_names) if self.label_names else 0
        }


# ============================================================================
# –°–†–ê–í–ù–ï–ù–ò–ï –ü–û–î–•–û–î–û–í
# ============================================================================

def compare_automl_approaches(train_data, val_data, test_data,
                              tpot_time_mins=2, random_search_time=60):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö AutoML –ø–æ–¥—Ö–æ–¥–æ–≤ –¥–ª—è –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    """
    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï AUTOML –ü–û–î–•–û–î–û–í –î–õ–Ø MULTILABEL")
    print("=" * 60)

    approaches = {}

    # 1. –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π RandomizedSearchCV
    print("\nüéØ 1. RANDOMIZED SEARCH CV (OneVsRest + –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏):")
    try:
        random_search = MultiLabelTextClassifier(
            n_iter=10,
            max_training_time=min(random_search_time, 30),
            random_state=42
        )
        random_search.train(train_data, val_data)
        test_results = random_search.evaluate(test_data)

        approaches['random_search'] = {
            'classifier': random_search,
            'accuracy': test_results['accuracy'],
            'f1_weighted': test_results['f1_weighted'],
            'hamming_loss': test_results['hamming_loss']
        }

        print(f"   ‚úÖ Accuracy: {test_results['accuracy']:.3f}")
        print(f"   ‚úÖ F1 Weighted: {test_results['f1_weighted']:.3f}")
        print(f"   ‚úÖ Hamming Loss: {test_results['hamming_loss']:.3f}")

    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")

    # 2. Light AutoML (–±–µ–∑ TPOT)
    print("\n‚ö° 2. LIGHT AUTOML (–±—ã—Å—Ç—Ä—ã–π –ø–æ–¥–±–æ—Ä –º–æ–¥–µ–ª–µ–π):")
    try:
        light_automl = LightAutoMLClassifier(random_state=42)
        light_automl.train(train_data, val_data)
        test_results = light_automl.evaluate(test_data)

        approaches['light_automl'] = {
            'classifier': light_automl,
            'accuracy': test_results['accuracy'],
            'f1_weighted': test_results['f1_weighted'],
            'hamming_loss': test_results['hamming_loss']
        }

        print(f"   ‚úÖ Accuracy: {test_results['accuracy']:.3f}")
        print(f"   ‚úÖ F1 Weighted: {test_results['f1_weighted']:.3f}")
        print(f"   ‚úÖ Hamming Loss: {test_results['hamming_loss']:.3f}")

    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")

    # 3. TPOT AutoML (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
    if TPOT_AVAILABLE:
        print("\nüß¨ 3. SIMPLE TPOT AUTOML (–≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ):")
        try:
            tpot_classifier = SimpleTPOTMultiLabelClassifier(
                max_time_mins=min(tpot_time_mins, 2),
                random_state=42
            )
            tpot_classifier.train(train_data, val_data)
            test_results = tpot_classifier.evaluate(test_data)

            approaches['tpot_simple'] = {
                'classifier': tpot_classifier,
                'accuracy': test_results['accuracy'],
                'f1_weighted': test_results['f1_weighted'],
                'hamming_loss': test_results['hamming_loss']
            }

            print(f"   ‚úÖ Accuracy: {test_results['accuracy']:.3f}")
            print(f"   ‚úÖ F1 Weighted: {test_results['f1_weighted']:.3f}")
            print(f"   ‚úÖ Hamming Loss: {test_results['hamming_loss']:.3f}")

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")

    # 4. –ë–∞–∑–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥
    print("\nüìä 4. –ë–ê–ó–û–í–´–ô –ü–û–î–•–û–î (LogisticRegression OneVsRest):")
    try:
        from sklearn.linear_model import LogisticRegression

        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–∞–π–ø–ª–∞–π–Ω
        base_vectorizer = TfidfVectorizer(max_features=5000)
        base_model = OneVsRestClassifier(LogisticRegression(max_iter=1000, random_state=42))

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        texts_train = [item['text'] for item in train_data]
        labels_train = [item['binary_labels'] for item in train_data]

        label_binarizer = MultiLabelBinarizer()
        y_train_binary = label_binarizer.fit_transform(labels_train)

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ
        X_train_vec = base_vectorizer.fit_transform(texts_train)
        base_model.fit(X_train_vec, y_train_binary)

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ
        texts_test = [item['text'] for item in test_data]
        labels_test = [item['binary_labels'] for item in test_data]

        y_test_binary = label_binarizer.transform(labels_test)
        X_test_vec = base_vectorizer.transform(texts_test)
        y_pred = base_model.predict(X_test_vec)

        accuracy = accuracy_score(y_test_binary, y_pred)
        f1 = f1_score(y_test_binary, y_pred, average='weighted')
        h_loss = hamming_loss(y_test_binary, y_pred)

        approaches['baseline'] = {
            'classifier': None,
            'accuracy': accuracy,
            'f1_weighted': f1,
            'hamming_loss': h_loss
        }

        print(f"   ‚úÖ Accuracy: {accuracy:.3f}")
        print(f"   ‚úÖ F1 Weighted: {f1:.3f}")
        print(f"   ‚úÖ Hamming Loss: {h_loss:.3f}")

    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "=" * 60)
    print("üìä –ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï:")
    print("=" * 60)
    print(f"{'–ü–æ–¥—Ö–æ–¥':<25} {'Accuracy':<10} {'F1 Weighted':<12} {'Hamming Loss':<12}")
    print("-" * 60)

    for name, result in sorted(approaches.items(),
                               key=lambda x: x[1]['accuracy'],
                               reverse=True):
        acc = result['accuracy']
        f1 = result['f1_weighted']
        h_loss = result['hamming_loss']

        print(f"{name:<25} {acc:<10.3f} {f1:<12.3f} {h_loss:<12.3f}")

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
    print("\n" + "=" * 60)
    print("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø:")

    if approaches:
        best_approach = max(approaches.items(), key=lambda x: x[1]['accuracy'])[0]
        print(f"   –õ—É—á—à–∏–π –ø–æ–¥—Ö–æ–¥: {best_approach.upper()}")

        if 'tpot' in best_approach:
            print("   TPOT –Ω–∞—à–µ–ª –Ω–∞–∏–±–æ–ª–µ–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
        elif 'random_search' in best_approach:
            print("   Random Search CV –æ–±–µ—Å–ø–µ—á–∏–ª –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –≤—Ä–µ–º–µ–Ω–∏")
        elif 'light_automl' in best_approach:
            print("   Light AutoML –ø–æ–∫–∞–∑–∞–ª —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ –±—ã—Å—Ç—Ä–æ–º –æ–±—É—á–µ–Ω–∏–∏")
        else:
            print("   –ë–∞–∑–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –æ–∫–∞–∑–∞–ª—Å—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –¥–ª—è –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö")

    return approaches


# ============================================================================
# –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø
# ============================================================================

def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        from util.jsonl_process import read_jsonl_basic

        train_data = read_jsonl_basic('../util/news_multilabel_train_data.jsonl')
        val_data = read_jsonl_basic('../util/news_multilabel_val_data.jsonl')
        test_data = read_jsonl_basic('../util/news_multilabel_test_data.jsonl')

        print(f"üìä –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã:")
        print(f"   Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   Validation: {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   Test: {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")

        # –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤–æ–π –∑–∞–ø–∏—Å–∏
        if train_data:
            print(f"\nüìù –ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–∏:")
            print(f"   Text: {train_data[0]['text'][:100]}...")
            print(f"   Labels: {train_data[0]['binary_labels']}")
            print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫: {len(train_data[0]['binary_labels'])}")

    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ: {e}")
        print("–°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏...")

        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        train_data = [
            {"text": "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ 1 –æ —Å–ø–æ—Ä—Ç–µ –∏ –ø–æ–ª–∏—Ç–∏–∫–µ", "binary_labels": [1, 0, 1, 0, 1, 0]},
            {"text": "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ 2 –æ–± —ç–∫–æ–Ω–æ–º–∏–∫–µ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö", "binary_labels": [0, 1, 0, 1, 0, 1]},
            {"text": "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ 3 –æ –∫—É–ª—å—Ç—É—Ä–µ –∏ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏", "binary_labels": [1, 1, 0, 0, 1, 1]},
            {"text": "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ 4 –æ –∑–¥–æ—Ä–æ–≤—å–µ –∏ –Ω–∞—É–∫–µ", "binary_labels": [0, 0, 1, 1, 0, 0]},
            {"text": "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ 5 –æ –±–∏–∑–Ω–µ—Å–µ –∏ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö", "binary_labels": [1, 0, 0, 1, 1, 0]},
            {"text": "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ 6 –æ —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã—Ö —Å–æ–±—ã—Ç–∏—è—Ö", "binary_labels": [1, 0, 0, 0, 0, 0]},
            {"text": "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ 7 –æ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏—è—Ö", "binary_labels": [0, 1, 0, 0, 0, 0]},
            {"text": "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ 8 –æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∏–Ω–Ω–æ–≤–∞—Ü–∏—è—Ö", "binary_labels": [0, 0, 1, 0, 0, 0]},
        ]
        val_data = [
            {"text": "–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç 1", "binary_labels": [1, 0, 1, 0, 0, 1]},
            {"text": "–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç 2", "binary_labels": [0, 1, 0, 1, 1, 0]},
        ]
        test_data = [
            {"text": "–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç 1", "binary_labels": [1, 0, 0, 1, 0, 1]},
            {"text": "–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç 2", "binary_labels": [0, 1, 1, 0, 1, 0]},
            {"text": "–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç 3", "binary_labels": [1, 1, 0, 1, 0, 0]},
        ]
        print(f"üìä –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã:")
        print(f"   Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   Validation: {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   Test: {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    print("\n" + "=" * 60)

    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RandomizedSearchCV –ø–æ–¥—Ö–æ–¥–∞
    print("\nüéØ –í–ê–†–ò–ê–ù–¢ 1: RANDOMIZED SEARCH CV")
    print("=" * 40)

    classifier1 = MultiLabelTextClassifier(
        n_iter=8,  # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
        max_training_time=20,  # 20 —Å–µ–∫—É–Ω–¥
        random_state=42
    )

    classifier1.train(train_data, val_data)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    classifier1.save_model("multilabel_random_search.pkl")

    # –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
    classifier1.plot_training_history(save_path='random_search_history.png')

    # –û—Ü–µ–Ω–∫–∞
    print("\nüß™ –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–ï:")
    results1 = classifier1.evaluate(test_data)

    # –í–∞—Ä–∏–∞–Ω—Ç 2: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Light AutoML
    print("\n‚ö° –í–ê–†–ò–ê–ù–¢ 2: LIGHT AUTOML")
    print("=" * 40)

    classifier2 = LightAutoMLClassifier(random_state=42)
    classifier2.train(train_data, val_data)

    # –û—Ü–µ–Ω–∫–∞
    print("\nüß™ –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–ï:")
    results2 = classifier2.evaluate(test_data)

    # –í–∞—Ä–∏–∞–Ω—Ç 3: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ TPOT (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
    if TPOT_AVAILABLE:
        print("\nüß¨ –í–ê–†–ò–ê–ù–¢ 3: SIMPLE TPOT")
        print("=" * 40)

        try:
            classifier3 = SimpleTPOTMultiLabelClassifier(
                max_time_mins=1,  # 1 –º–∏–Ω—É—Ç–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
                random_state=42
            )

            classifier3.train(train_data, val_data)

            # –û—Ü–µ–Ω–∫–∞
            print("\nüß™ –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–ï:")
            results3 = classifier3.evaluate(test_data)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å TPOT: {e}")

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤
    print("\n" + "=" * 60)
    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –í–°–ï–• –ü–û–î–•–û–î–û–í")
    print("=" * 60)

    approaches = compare_automl_approaches(
        train_data,
        val_data,
        test_data,
        tpot_time_mins=1,
        random_search_time=20
    )

    # –ü—Ä–∏–º–µ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    print("\n" + "=" * 60)
    print("üîÆ –ü–†–ò–ú–ï–† –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø")
    print("=" * 60)

    if test_data and approaches:
        sample_text = test_data[0]['text']

        print(f"\n–¢–µ–∫—Å—Ç: {sample_text[:100]}...")
        print(f"–ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏: {test_data[0]['binary_labels']}")

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ª—É—á—à–µ–π –º–æ–¥–µ–ª—å—é
        best_approach_name = max(approaches.items(), key=lambda x: x[1]['accuracy'])[0]
        best_classifier = approaches[best_approach_name]['classifier']

        if best_classifier:
            predictions, pred_labels, probs = best_classifier.predict([sample_text])
            print(f"\nüìä {best_approach_name.upper()} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:")
            print(f"   –ú–µ—Ç–∫–∏: {pred_labels[0]}")

            # –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏
            print(f"\nüìã –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò:")
            model_info = best_classifier.get_model_info()
            for key, value in model_info.items():
                if key not in ['best_pipeline', 'parameters']:
                    print(f"   {key}: {value}")


if __name__ == "__main__":
    main()