from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import RandomizedSearchCV, cross_val_score
from sklearn.metrics import (
    classification_report,
    accuracy_score,
    confusion_matrix,
    hamming_loss,
    f1_score,
    precision_score,
    recall_score,
    multilabel_confusion_matrix,
    make_scorer
)
import numpy as np
import joblib
import pandas as pd
from scipy.stats import randint, uniform
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from itertools import cycle
from sklearn.preprocessing import MultiLabelBinarizer

warnings.filterwarnings('ignore')


# ============================================================================
# –ö–õ–ê–°–°–ò–ß–ï–°–ö–ò–ô –ü–û–î–•–û–î –° RANDOMIZEDSEARCHCV
# ============================================================================

class MultiLabelTextClassifier:
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω–æ–π (multilabel) –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤
    """

    def __init__(self, max_training_time=300, n_iter=50, random_state=42):
        """
        Args:
            max_training_time: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
            n_iter: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            random_state: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        """
        self.vectorizer = TfidfVectorizer(
            max_features=10000,
            min_df=2,
            max_df=0.8,
            ngram_range=(1, 2),
            stop_words=None,
            analyzer='word'
        )

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º MultiLabelBinarizer –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫
        self.label_binarizer = MultiLabelBinarizer()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
        # –î–ª—è multilabel –∏—Å–ø–æ–ª—å–∑—É–µ–º OneVsRestClassifier
        self.models = {
            'logistic': {
                'model': OneVsRestClassifier(LogisticRegression(random_state=random_state, max_iter=1000)),
                'params': {
                    'estimator__C': uniform(0.001, 100),
                    'estimator__penalty': ['l1', 'l2'],
                    'estimator__solver': ['liblinear', 'saga'],
                }
            },
            'svm_linear': {
                'model': OneVsRestClassifier(LinearSVC(random_state=random_state, dual=False)),
                'params': {
                    'estimator__C': uniform(0.1, 10),
                    'estimator__loss': ['squared_hinge'],
                }
            },
            'random_forest': {
                'model': OneVsRestClassifier(RandomForestClassifier(random_state=random_state)),
                'params': {
                    'estimator__n_estimators': randint(50, 300),
                    'estimator__max_depth': [None, 10, 20, 30],
                    'estimator__min_samples_split': randint(2, 20),
                }
            },
            'naive_bayes': {
                'model': OneVsRestClassifier(MultinomialNB()),
                'params': {
                    'estimator__alpha': uniform(0.001, 2.0)
                }
            },
        }

        self.max_training_time = max_training_time
        self.n_iter = n_iter
        self.random_state = random_state
        self.is_trained = False
        self.best_model = None
        self.best_model_name = None
        self.best_score = 0
        self.label_names = None
        self.history = {
            'train_accuracy': [],
            'val_accuracy': [],
            'train_f1': [],
            'val_f1': [],
            'train_hamming': [],
            'val_hamming': []
        }

        print(f"üöÄ Multi-label Classifier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {max_training_time} —Å–µ–∫")
        print(f"   –ò—Ç–µ—Ä–∞—Ü–∏–π –ø–æ–∏—Å–∫–∞: {n_iter}")
        print(f"   –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {list(self.models.keys())}")

    def prepare_data(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏
        """
        texts = [item['text'] for item in data]

        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –º–µ—Ç–∫–∏ —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ –ø–æ–ª–µ 'binary_labels'
        labels = [item['binary_labels'] for item in data]

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        if not hasattr(self.label_binarizer, 'classes_'):
            labels_binary = self.label_binarizer.fit_transform(labels)
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏
            self.label_names = [str(cls) for cls in self.label_binarizer.classes_]
            print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(self.label_names)}")
        else:
            labels_binary = self.label_binarizer.transform(labels)

        return texts, labels_binary

    def train(self, train_data, val_data=None):
        """
        –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        """
        print("üéØ –ê–í–¢–û–ú–ê–¢–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–û–î–ë–û–† –ú–û–î–ï–õ–ï–ô...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train = self.prepare_data(train_data)

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
        print("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        X_train_vec = self.vectorizer.fit_transform(X_train)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_vec.shape}")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–µ—Ç–æ–∫: {y_train.shape}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(y_train)}")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –µ—Å—Ç—å
        if val_data:
            X_val, y_val = self.prepare_data(val_data)
            X_val_vec = self.vectorizer.transform(X_val)
        else:
            X_val_vec, y_val = None, None

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –º–æ–¥–µ–ª–µ–π
        print("\nü§ñ –ó–ê–ü–£–°–ö –°–õ–£–ß–ê–ô–ù–û–ì–û –ü–û–ò–°–ö–ê –ü–û –ú–û–î–ï–õ–Ø–ú...")

        best_models = {}

        for model_name, model_config in self.models.items():
            print(f"   üîç –ü–æ–∏—Å–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è {model_name}...")

            try:
                # –°–ª—É—á–∞–π–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
                search = RandomizedSearchCV(
                    model_config['model'],
                    model_config['params'],
                    n_iter=self.n_iter // len(self.models),
                    cv=3,
                    scoring='f1_weighted',  # –ò—Å–ø–æ–ª—å–∑—É–µ–º weighted F1 –¥–ª—è multilabel
                    random_state=self.random_state,
                    n_jobs=-1,
                    verbose=0
                )

                search.fit(X_train_vec, y_train)

                # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
                val_score = None
                if val_data:
                    y_val_pred = search.best_estimator_.predict(X_val_vec)
                    val_score = f1_score(y_val, y_val_pred, average='weighted')

                best_models[model_name] = {
                    'model': search.best_estimator_,
                    'train_score': search.best_score_,
                    'val_score': val_score,
                    'params': search.best_params_
                }

                print(f"      ‚úÖ Train F1: {search.best_score_:.3f}" +
                      (f", Val F1: {val_score:.3f}" if val_score else ""))

            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–ª—è {model_name}: {e}")
                continue

        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if best_models:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π score –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ train score
            score_key = 'val_score' if val_data else 'train_score'

            self.best_model_name = max(
                best_models.keys(),
                key=lambda x: best_models[x][score_key] if best_models[x][score_key] is not None else 0
            )

            self.best_model = best_models[self.best_model_name]['model']
            self.best_score = best_models[self.best_model_name][score_key]
            self.best_params = best_models[self.best_model_name]['params']

            print(f"\nüèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {self.best_model_name}")
            print(f"   F1 Score: {self.best_score:.3f}")

            self.is_trained = True

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
            self._show_model_comparison(best_models)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
            if val_data:
                y_train_pred = self.best_model.predict(X_train_vec)
                y_val_pred = self.best_model.predict(X_val_vec)

                self.history['train_f1'].append(f1_score(y_train, y_train_pred, average='weighted'))
                self.history['val_f1'].append(f1_score(y_val, y_val_pred, average='weighted'))
                self.history['train_accuracy'].append(accuracy_score(y_train, y_train_pred))
                self.history['val_accuracy'].append(accuracy_score(y_val, y_val_pred))

                # –í—ã—á–∏—Å–ª—è–µ–º Hamming loss
                self.history['train_hamming'] = [hamming_loss(y_train, y_train_pred)]
                self.history['val_hamming'] = [hamming_loss(y_val, y_val_pred)]

                print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train: {self.history['train_accuracy'][-1]:.3f}")
                print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ val: {self.history['val_accuracy'][-1]:.3f}")
                print(f"‚úÖ Hamming loss –Ω–∞ train: {self.history['train_hamming'][-1]:.3f}")
                print(f"‚úÖ Hamming loss –Ω–∞ val: {self.history['val_hamming'][-1]:.3f}")
        else:
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å!")

    def _show_model_comparison(self, best_models):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        print(f"\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô:")
        print("-" * 50)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º—É score –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –ø–æ train score
        sorted_models = sorted(best_models.items(),
                               key=lambda x: x[1]['val_score'] if x[1]['val_score'] is not None else x[1][
                                   'train_score'],
                               reverse=True)

        for model_name, results in sorted_models:
            train_score = results['train_score']
            val_score = results['val_score']

            score_str = f"Train F1: {train_score:.3f}"
            if val_score is not None:
                score_str += f", Val F1: {val_score:.3f}"

            print(f"   {model_name:<15}: {score_str}")

    def predict(self, texts, threshold=0.5):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)

        # –î–ª—è multilabel –ø–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥
        if hasattr(self.best_model, "predict_proba"):
            probabilities = self.best_model.predict_proba(X_vec)
            predictions = (probabilities >= threshold).astype(int)
        else:
            # –î–ª—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ predict_proba (–Ω–∞–ø—Ä–∏–º–µ—Ä, LinearSVC)
            predictions = self.best_model.predict(X_vec)
            probabilities = None

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫
        predictions_labels = self.label_binarizer.inverse_transform(predictions)

        return predictions, predictions_labels, probabilities

    def evaluate(self, test_data, threshold=0.5):
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        X_test, y_test = self.prepare_data(test_data)
        X_test_vec = self.vectorizer.transform(X_test)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π predict –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        if hasattr(self.best_model, "predict_proba"):
            probabilities = self.best_model.predict_proba(X_test_vec)
            y_pred = (probabilities >= threshold).astype(int)
        else:
            y_pred = self.best_model.predict(X_test_vec)

        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, y_pred)
        h_loss = hamming_loss(y_test, y_pred)
        f1_macro = f1_score(y_test, y_pred, average='macro')
        f1_weighted = f1_score(y_test, y_pred, average='weighted')
        f1_samples = f1_score(y_test, y_pred, average='samples')
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')

        print("\nüìä –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print("-" * 50)
        print(f"Accuracy: {accuracy:.3f}")
        print(f"Hamming Loss: {h_loss:.3f}")
        print(f"Precision (weighted): {precision:.3f}")
        print(f"Recall (weighted): {recall:.3f}")
        print(f"F1 Macro: {f1_macro:.3f}")
        print(f"F1 Weighted: {f1_weighted:.3f}")
        print(f"F1 Samples: {f1_samples:.3f}")

        return {
            'accuracy': accuracy,
            'hamming_loss': h_loss,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'f1_samples': f1_samples,
            'precision': precision,
            'recall': recall
        }

    def plot_training_history(self, save_path=None):
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è
        """
        if not self.history['train_accuracy']:
            print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è")
            return

        fig, axes = plt.subplots(1, 3, figsize=(15, 4))

        # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
        axes[0].plot(self.history['train_accuracy'], label='Train Accuracy', marker='o', linewidth=2)
        if self.history['val_accuracy']:
            axes[0].plot(self.history['val_accuracy'], label='Val Accuracy', marker='s', linewidth=2)
        axes[0].set_title('Accuracy')
        axes[0].set_xlabel('Model Selection')
        axes[0].set_ylabel('Accuracy')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # –ì—Ä–∞—Ñ–∏–∫ F1-score
        axes[1].plot(self.history['train_f1'], label='Train F1', marker='o', linewidth=2)
        if self.history['val_f1']:
            axes[1].plot(self.history['val_f1'], label='Val F1', marker='s', linewidth=2)
        axes[1].set_title('F1 Score (weighted)')
        axes[1].set_xlabel('Model Selection')
        axes[1].set_ylabel('F1 Score')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

        # –ì—Ä–∞—Ñ–∏–∫ Hamming Loss –µ—Å–ª–∏ –µ—Å—Ç—å
        if self.history.get('train_hamming'):
            axes[2].plot(self.history['train_hamming'], label='Train Hamming Loss', marker='o', linewidth=2)
            if self.history.get('val_hamming'):
                axes[2].plot(self.history['val_hamming'], label='Val Hamming Loss', marker='s', linewidth=2)
            axes[2].set_title('Hamming Loss')
            axes[2].set_xlabel('Model Selection')
            axes[2].set_ylabel('Hamming Loss')
            axes[2].legend()
            axes[2].grid(True, alpha=0.3)

        plt.suptitle(f'Best Model: {self.best_model_name}', fontsize=12)
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"üíæ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {save_path}")

        plt.show()

    def get_model_info(self):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
        """
        if not self.is_trained:
            return {"error": "–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞"}

        label_names_list = self.label_names if self.label_names is not None else []

        return {
            'model_name': self.best_model_name,
            'model_type': type(self.best_model).__name__,
            'best_score': self.best_score,
            'feature_count': len(self.vectorizer.get_feature_names_out()),
            'label_count': len(label_names_list),
        }

    def save_model(self, filename):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        label_names_to_save = list(self.label_names) if self.label_names is not None else None

        joblib.dump({
            'best_model': self.best_model,
            'vectorizer': self.vectorizer,
            'label_binarizer': self.label_binarizer,
            'best_model_name': self.best_model_name,
            'best_score': self.best_score,
            'best_params': self.best_params,
            'label_names': label_names_to_save,
            'history': self.history
        }, filename, compress=3)
        print(f"üíæ Multi-label –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")

    def load_model(self, filename):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        loaded = joblib.load(filename)
        self.best_model = loaded['best_model']
        self.vectorizer = loaded['vectorizer']
        self.label_binarizer = loaded['label_binarizer']
        self.best_model_name = loaded['best_model_name']
        self.best_score = loaded.get('best_score', 0)
        self.best_params = loaded.get('best_params', {})

        loaded_label_names = loaded.get('label_names', None)
        if loaded_label_names is not None:
            self.label_names = [str(cls) for cls in loaded_label_names]
        else:
            self.label_names = None

        self.history = loaded.get('history', {})
        self.is_trained = True
        print(f"üì• Multi-label –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filename}")


# ============================================================================
# H2O.AI AUTOML –ü–û–î–•–û–î (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô)
# ============================================================================

try:
    import h2o
    from h2o.automl import H2OAutoML

    H2O_AVAILABLE = True
    print("‚úÖ H2O.ai –¥–æ—Å—Ç—É–ø–µ–Ω")
except ImportError:
    print("‚ö†Ô∏è H2O.ai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install h2o")
    H2O_AVAILABLE = False


class OptimizedH2OAutoMLClassifier:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π H2O AutoML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    """

    def __init__(self, max_runtime_secs=180, seed=42, n_classes_limit=5):
        """
        Args:
            max_runtime_secs: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            seed: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
            n_classes_limit: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        """
        if not H2O_AVAILABLE:
            raise ImportError("H2O.ai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install tpot")

        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Äizer —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.vectorizer = TfidfVectorizer(
            max_features=500,  # –ú–µ–Ω—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            min_df=1,  # –ë–æ–ª–µ–µ –ª–∏–±–µ—Ä–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            max_df=0.95,
            ngram_range=(1, 1),  # –¢–æ–ª—å–∫–æ —É–Ω–∏–≥—Ä–∞–º–º—ã
            stop_words=None
        )

        self.label_binarizer = MultiLabelBinarizer()
        self.max_runtime_secs = max_runtime_secs
        self.seed = seed
        self.n_classes_limit = n_classes_limit
        self.is_trained = False
        self.label_names = None
        self.models = {}
        self.h2o_initialized = False

        print(f"‚ö° Optimized H2O AutoML Classifier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {max_runtime_secs} —Å–µ–∫")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {n_classes_limit}")
        print(f"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: 500")

    def _init_h2o(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è H2O —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
        if not self.h2o_initialized:
            try:
                # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ H2O
                h2o.init(
                    min_mem_size="1G",  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å
                    max_mem_size="2G",  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å
                    nthreads=2,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–æ—Ç–æ–∫–∏
                    verbose=False,
                    enable_assertions=False  # –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                )
                self.h2o_initialized = True
                print("   ‚úÖ H2O –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏")
            except Exception as e:
                print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å H2O: {e}")
                # –ü—Ä–æ–±—É–µ–º –±–µ–∑ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
                h2o.init(verbose=False)
                self.h2o_initialized = True

    def prepare_data(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        """
        texts = [item['text'] for item in data]
        labels = [item['binary_labels'] for item in data]

        if not hasattr(self.label_binarizer, 'classes_'):
            labels_binary = self.label_binarizer.fit_transform(labels)
            self.label_names = [str(cls) for cls in self.label_binarizer.classes_]
            print(f"   –í—Å–µ–≥–æ –∫–ª–∞—Å—Å–æ–≤: {len(self.label_names)}")
        else:
            labels_binary = self.label_binarizer.transform(labels)

        return texts, labels_binary

    def train(self, train_data, val_data=None):
        """
        –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
        """
        print("‚ö° –ë–´–°–¢–†–û–ï –û–ë–£–ß–ï–ù–ò–ï H2O AUTOML –° –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø–ú–ò...")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è H2O
        self._init_h2o()

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train = self.prepare_data(train_data)
        X_train_vec = self.vectorizer.fit_transform(X_train)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö: {X_train_vec.shape}")
        print(f"   –ü—Ä–∏–º–µ—Ä–æ–≤: {len(X_train)}")

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        if len(self.label_names) > self.n_classes_limit:
            print(f"   ‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∫–ª–∞—Å—Å–æ–≤ ({len(self.label_names)}).")
            print(f"   –û–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ {self.n_classes_limit} –∫–ª–∞—Å—Å–æ–≤...")
            classes_to_train = self.label_names[:self.n_classes_limit]
        else:
            classes_to_train = self.label_names

        print(f"   –ö–ª–∞—Å—Å–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(classes_to_train)}")

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ–¥–∏–Ω —Ä–∞–∑
        feature_names = self.vectorizer.get_feature_names_out()
        X_df = pd.DataFrame(X_train_vec.toarray(), columns=feature_names)

        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –º–µ—Ç–∫–∏
        for i, label_name in enumerate(self.label_names):
            if i < len(y_train[0]):  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≥—Ä–∞–Ω–∏—Ü—ã
                X_df[label_name] = y_train[:, i]

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ H2O Frame
        h2o_df = h2o.H2OFrame(X_df)

        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
        models_trained = 0
        for i, label_name in enumerate(classes_to_train):
            if i >= len(self.label_names):
                break

            print(f"\n   üè∑Ô∏è  –ö–ª–∞—Å—Å {i + 1}/{len(classes_to_train)}: {label_name}")

            try:
                # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ AutoML –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                automl = H2OAutoML(
                    max_runtime_secs=max(30, self.max_runtime_secs // len(classes_to_train)),
                    max_models=2,  # –í—Å–µ–≥–æ 2 –º–æ–¥–µ–ª–∏
                    seed=self.seed,
                    nfolds=2,  # –¢–æ–ª—å–∫–æ 2 —Ñ–æ–ª–¥–∞
                    stopping_metric='AUC',
                    sort_metric='AUC',
                    verbosity='error',  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥
                    exclude_algos=["DeepLearning", "StackedEnsemble"],  # –ò—Å–∫–ª—é—á–∞–µ–º –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã
                    include_algos=["GLM", "GBM", "DRF"]  # –¢–æ–ª—å–∫–æ –±—ã—Å—Ç—Ä—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã
                )

                print(f"      üèÉ –ó–∞–ø—É—Å–∫ AutoML (–º–∞–∫—Å. {max(30, self.max_runtime_secs // len(classes_to_train))} —Å–µ–∫)...")

                # –ó–∞–ø—É—Å–∫–∞–µ–º AutoML
                automl.train(
                    x=list(feature_names),
                    y=label_name,
                    training_frame=h2o_df
                )

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞
                if automl.leader is not None:
                    self.models[label_name] = automl.leader
                    models_trained += 1

                    # –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞
                    try:
                        lb = automl.leaderboard
                        if lb is not None and len(lb) > 0:
                            print(f"      ‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞: {automl.leader.model_id}")
                            print(f"      üìä AUC: {automl.leader.auc():.3f}")
                    except:
                        print(f"      ‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞")
                else:
                    print(f"      ‚ö†Ô∏è AutoML –Ω–µ —Å–æ–∑–¥–∞–ª –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∞ {label_name}")

            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ –¥–ª—è –∫–ª–∞—Å—Å–∞ {label_name}: {str(e)[:100]}...")
                # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ—Å—Ç—É—é GLM –º–æ–¥–µ–ª—å
                try:
                    print(f"      üîß –ü—Ä–æ–±—É–µ–º –ø—Ä–æ—Å—Ç—É—é GLM –º–æ–¥–µ–ª—å...")
                    from h2o.estimators.glm import H2OGeneralizedLinearEstimator
                    glm = H2OGeneralizedLinearEstimator(
                        family="binomial",
                        seed=self.seed,
                        lambda_search=True,
                        alpha=0.5
                    )
                    glm.train(
                        x=list(feature_names),
                        y=label_name,
                        training_frame=h2o_df
                    )
                    self.models[label_name] = glm
                    models_trained += 1
                    print(f"      ‚úÖ GLM –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞")
                except Exception as e2:
                    print(f"      ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –¥–∞–∂–µ GLM: {str(e2)[:100]}...")

        if models_trained > 0:
            self.is_trained = True
            print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            print(f"   –£—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {models_trained}/{len(classes_to_train)}")
        else:
            print(f"\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å!")
            raise Exception("H2O AutoML –Ω–µ —Å–º–æ–≥ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏")

    def predict(self, texts, threshold=0.5):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)
        feature_names = self.vectorizer.get_feature_names_out()

        if X_vec.shape[1] != len(feature_names):
            # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç, –ø–µ—Ä–µ–æ–±—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Äizer
            print("‚ö†Ô∏è –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback...")
            return self._predict_fallback(texts, threshold)

        X_df = pd.DataFrame(X_vec.toarray(), columns=feature_names)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ H2O Frame
        try:
            h2o_df = h2o.H2OFrame(X_df)
        except:
            print("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ H2O Frame, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback...")
            return self._predict_fallback(texts, threshold)

        predictions_matrix = []

        for label_name in self.label_names:
            if label_name in self.models:
                model = self.models[label_name]
                try:
                    preds = model.predict(h2o_df)
                    if 'p1' in preds.columns:
                        probs = preds['p1'].as_data_frame().values.flatten()
                        binary_preds = (probs >= threshold).astype(int)
                    elif 'predict' in preds.columns:
                        # –î–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª–µ–π –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å p1
                        pred_vals = preds['predict'].as_data_frame().values.flatten()
                        binary_preds = pred_vals.astype(int)
                    else:
                        binary_preds = np.zeros(len(texts))

                    predictions_matrix.append(binary_preds)
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {label_name}: {str(e)[:50]}")
                    predictions_matrix.append(np.zeros(len(texts)))
            else:
                predictions_matrix.append(np.zeros(len(texts)))

        predictions = np.array(predictions_matrix).T if predictions_matrix else np.array([])

        if len(predictions) > 0:
            predictions_labels = self.label_binarizer.inverse_transform(predictions)
        else:
            predictions_labels = [[] for _ in range(len(texts))]

        return predictions, predictions_labels, None

    def _predict_fallback(self, texts, threshold=0.5):
        """Fallback –º–µ—Ç–æ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
        print("–ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...")
        predictions = np.zeros((len(texts), len(self.label_names)))
        predictions_labels = [[] for _ in range(len(texts))]
        return predictions, predictions_labels, None

    def evaluate(self, test_data):
        """
        –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞
        """
        if not self.is_trained:
            return {'accuracy': 0, 'hamming_loss': 1, 'f1_weighted': 0}

        X_test, y_test = self.prepare_data(test_data)

        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—É—á–µ–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
        predictions, pred_labels, _ = self.predict([item['text'] for item in test_data])

        if len(predictions) == 0:
            return {'accuracy': 0, 'hamming_loss': 1, 'f1_weighted': 0}

        # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ —Ç–µ–º –∫–ª–∞—Å—Å–∞–º, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å –º–æ–¥–µ–ª–∏
        trained_indices = [i for i, label_name in enumerate(self.label_names)
                           if label_name in self.models]

        if trained_indices:
            y_test_subset = y_test[:, trained_indices]
            predictions_subset = predictions[:, trained_indices]

            accuracy = accuracy_score(y_test_subset, predictions_subset)
            h_loss = hamming_loss(y_test_subset, predictions_subset)
            f1 = f1_score(y_test_subset, predictions_subset, average='weighted')
        else:
            accuracy = h_loss = f1 = 0.0

        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Optimized H2O AutoML:")
        print(f"   Accuracy: {accuracy:.3f}")
        print(f"   Hamming Loss: {h_loss:.3f}")
        print(f"   F1 Weighted: {f1:.3f}")
        print(f"   –û–±—É—á–µ–Ω–æ –∫–ª–∞—Å—Å–æ–≤: {len(self.models)}/{len(self.label_names)}")

        return {'accuracy': accuracy, 'hamming_loss': h_loss, 'f1_weighted': f1}

    def get_model_info(self):
        """
        –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        """
        if not self.is_trained:
            return {"error": "–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞"}

        return {
            'model_type': 'Optimized H2O AutoML',
            'feature_count': len(self.vectorizer.get_feature_names_out()),
            'label_count': len(self.label_names) if self.label_names else 0,
            'models_trained': len(self.models),
            'training_time': f"{self.max_runtime_secs} —Å–µ–∫"
        }

    def __del__(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ H2O —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        if H2O_AVAILABLE and self.h2o_initialized:
            try:
                h2o.shutdown(prompt=False)
            except:
                pass


# ============================================================================
# –ë–´–°–¢–†–´–ô H2O AUTOML –° GLM (–°–ê–ú–´–ô –ë–´–°–¢–†–´–ô –í–ê–†–ò–ê–ù–¢)
# ============================================================================

class FastH2OGLMClassifier:
    """
    –ë—ã—Å—Ç—Ä—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ H2O GLM (Generalized Linear Model)
    –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    """

    def __init__(self, seed=42):
        """
        Args:
            seed: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        """
        if not H2O_AVAILABLE:
            raise ImportError("H2O.ai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install h2o")

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Äizer –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        self.vectorizer = TfidfVectorizer(
            max_features=200,  # –û—á–µ–Ω—å –º–∞–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            min_df=1,
            max_df=0.95,
            ngram_range=(1, 1)
        )

        self.label_binarizer = MultiLabelBinarizer()
        self.seed = seed
        self.is_trained = False
        self.label_names = None
        self.models = {}

        print(f"üöÄ Fast H2O GLM Classifier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        print(f"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: 200 (–¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)")

    def prepare_data(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        """
        texts = [item['text'] for item in data]
        labels = [item['binary_labels'] for item in data]

        if not hasattr(self.label_binarizer, 'classes_'):
            labels_binary = self.label_binarizer.fit_transform(labels)
            self.label_names = [str(cls) for cls in self.label_binarizer.classes_]
        else:
            labels_binary = self.label_binarizer.transform(labels)

        return texts, labels_binary

    def train(self, train_data, val_data=None):
        """
        –°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ GLM –º–æ–¥–µ–ª—è–º–∏
        """
        print("üöÄ –°–í–ï–†–•–ë–´–°–¢–†–û–ï –û–ë–£–ß–ï–ù–ò–ï H2O GLM...")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è H2O
        try:
            h2o.init(verbose=False, nthreads=1)
        except:
            h2o.init(verbose=False)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train = self.prepare_data(train_data)
        X_train_vec = self.vectorizer.fit_transform(X_train)

        print(f"   –î–∞–Ω–Ω—ã–µ: {X_train_vec.shape[0]} –ø—Ä–∏–º–µ—Ä–æ–≤, {X_train_vec.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        print(f"   –ö–ª–∞—Å—Å–æ–≤: {len(self.label_names)}")

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        max_classes = min(3, len(self.label_names))
        print(f"   –û–±—É—á–∞–µ–º –ø–µ—Ä–≤—ã–µ {max_classes} –∫–ª–∞—Å—Å–æ–≤...")

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        feature_names = self.vectorizer.get_feature_names_out()
        X_df = pd.DataFrame(X_train_vec.toarray(), columns=feature_names)

        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫–∏
        for i in range(max_classes):
            if i < len(self.label_names):
                X_df[self.label_names[i]] = y_train[:, i]

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ H2O Frame
        h2o_df = h2o.H2OFrame(X_df)

        # –û–±—É—á–∞–µ–º GLM –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        from h2o.estimators.glm import H2OGeneralizedLinearEstimator

        for i in range(max_classes):
            if i >= len(self.label_names):
                break

            label_name = self.label_names[i]
            print(f"   üè∑Ô∏è  –ö–ª–∞—Å—Å {i + 1}/{max_classes}: {label_name}")

            try:
                # –ë—ã—Å—Ç—Ä–∞—è GLM –º–æ–¥–µ–ª—å
                glm = H2OGeneralizedLinearEstimator(
                    family="binomial",
                    seed=self.seed,
                    alpha=0.5,  # ElasticNet
                    lambda_search=True,  # –ê–≤—Ç–æ–ø–æ–¥–±–æ—Ä —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
                    nlambdas=5,  # –í—Å–µ–≥–æ 5 –∑–Ω–∞—á–µ–Ω–∏–π lambda
                    max_iterations=50  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏—Ç–µ—Ä–∞—Ü–∏–π
                )

                glm.train(
                    x=list(feature_names),
                    y=label_name,
                    training_frame=h2o_df
                )

                self.models[label_name] = glm
                print(f"      ‚úÖ GLM –æ–±—É—á–µ–Ω–∞")

            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞: {str(e)[:50]}...")

        self.is_trained = len(self.models) > 0

        if self.is_trained:
            print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ú–æ–¥–µ–ª–µ–π: {len(self.models)}")
        else:
            print(f"\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏")

    def predict(self, texts, threshold=0.5):
        """
        –ë—ã—Å—Ç—Ä–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        """
        if not self.is_trained:
            return np.zeros((len(texts), len(self.label_names))), [[] for _ in range(len(texts))], None

        X_vec = self.vectorizer.transform(texts)
        feature_names = self.vectorizer.get_feature_names_out()

        if X_vec.shape[1] != len(feature_names):
            return np.zeros((len(texts), len(self.label_names))), [[] for _ in range(len(texts))], None

        X_df = pd.DataFrame(X_vec.toarray(), columns=feature_names)
        h2o_df = h2o.H2OFrame(X_df)

        predictions_matrix = []

        for label_name in self.label_names:
            if label_name in self.models:
                try:
                    preds = self.models[label_name].predict(h2o_df)
                    if 'p1' in preds.columns:
                        probs = preds['p1'].as_data_frame().values.flatten()
                        binary_preds = (probs >= threshold).astype(int)
                    else:
                        binary_preds = np.zeros(len(texts))
                except:
                    binary_preds = np.zeros(len(texts))
            else:
                binary_preds = np.zeros(len(texts))

            predictions_matrix.append(binary_preds)

        predictions = np.array(predictions_matrix).T if predictions_matrix else np.array([])

        if len(predictions) > 0:
            predictions_labels = self.label_binarizer.inverse_transform(predictions)
        else:
            predictions_labels = [[] for _ in range(len(texts))]

        return predictions, predictions_labels, None

    def evaluate(self, test_data):
        """
        –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞
        """
        if not self.is_trained:
            return {'accuracy': 0, 'hamming_loss': 1, 'f1_weighted': 0}

        X_test, y_test = self.prepare_data(test_data)
        predictions, pred_labels, _ = self.predict([item['text'] for item in test_data])

        if len(predictions) == 0:
            return {'accuracy': 0, 'hamming_loss': 1, 'f1_weighted': 0}

        # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ –æ–±—É—á–µ–Ω–Ω—ã–º –∫–ª–∞—Å—Å–∞–º
        trained_indices = [i for i, label_name in enumerate(self.label_names)
                           if label_name in self.models]

        if trained_indices:
            y_test_subset = y_test[:, trained_indices]
            predictions_subset = predictions[:, trained_indices]

            if y_test_subset.size > 0 and predictions_subset.size > 0:
                accuracy = accuracy_score(y_test_subset, predictions_subset)
                h_loss = hamming_loss(y_test_subset, predictions_subset)
                f1 = f1_score(y_test_subset, predictions_subset, average='weighted', zero_division=0)
            else:
                accuracy = h_loss = f1 = 0.0
        else:
            accuracy = h_loss = f1 = 0.0

        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Fast H2O GLM:")
        print(f"   Accuracy: {accuracy:.3f}")
        print(f"   Hamming Loss: {h_loss:.3f}")
        print(f"   F1 Weighted: {f1:.3f}")

        return {'accuracy': accuracy, 'hamming_loss': h_loss, 'f1_weighted': f1}


# ============================================================================
# –°–†–ê–í–ù–ï–ù–ò–ï –ü–û–î–•–û–î–û–í (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï)
# ============================================================================

def compare_automl_approaches_optimized(train_data, val_data, test_data):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ AutoML –ø–æ–¥—Ö–æ–¥–æ–≤
    """
    print("üî¨ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï AUTOML –ü–û–î–•–û–î–û–í")
    print("=" * 60)

    approaches = {}

    # 1. –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π RandomizedSearchCV (–±—ã—Å—Ç—Ä—ã–π)
    print("\nüéØ 1. RANDOMIZED SEARCH CV (–±—ã—Å—Ç—Ä—ã–π):")
    try:
        random_search = MultiLabelTextClassifier(
            n_iter=5,  # –ú–∏–Ω–∏–º—É–º –∏—Ç–µ—Ä–∞—Ü–∏–π
            max_training_time=30,
            random_state=42
        )
        random_search.train(train_data, val_data)
        test_results = random_search.evaluate(test_data)

        approaches['random_search'] = {
            'classifier': random_search,
            'accuracy': test_results['accuracy'],
            'f1_weighted': test_results['f1_weighted'],
            'hamming_loss': test_results['hamming_loss']
        }

        print(f"   ‚úÖ Accuracy: {test_results['accuracy']:.3f}")
        print(f"   ‚úÖ F1 Weighted: {test_results['f1_weighted']:.3f}")
        print(f"   ‚úÖ Hamming Loss: {test_results['hamming_loss']:.3f}")

    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")

    # 2. Fast H2O GLM (—Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π H2O –≤–∞—Ä–∏–∞–Ω—Ç)
    print("\nüöÄ 2. FAST H2O GLM (—Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π):")
    if H2O_AVAILABLE:
        try:
            h2o_classifier = FastH2OGLMClassifier(seed=42)
            h2o_classifier.train(train_data, val_data)
            test_results = h2o_classifier.evaluate(test_data)

            approaches['h2o_fast_glm'] = {
                'classifier': h2o_classifier,
                'accuracy': test_results['accuracy'],
                'f1_weighted': test_results['f1_weighted'],
                'hamming_loss': test_results['hamming_loss']
            }

            print(f"   ‚úÖ Accuracy: {test_results['accuracy']:.3f}")
            print(f"   ‚úÖ F1 Weighted: {test_results['f1_weighted']:.3f}")
            print(f"   ‚úÖ Hamming Loss: {test_results['hamming_loss']:.3f}")

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
    else:
        print("   ‚ö†Ô∏è H2O.ai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º...")

    # 3. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π H2O AutoML
    print("\n‚ö° 3. OPTIMIZED H2O AUTOML (–±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞):")
    if H2O_AVAILABLE:
        try:
            h2o_optimized = OptimizedH2OAutoMLClassifier(
                max_runtime_secs=120,  # –ë–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏
                seed=42,
                n_classes_limit=3  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–ª–∞—Å—Å—ã
            )
            h2o_optimized.train(train_data, val_data)
            test_results = h2o_optimized.evaluate(test_data)

            approaches['h2o_optimized'] = {
                'classifier': h2o_optimized,
                'accuracy': test_results['accuracy'],
                'f1_weighted': test_results['f1_weighted'],
                'hamming_loss': test_results['hamming_loss']
            }

            print(f"   ‚úÖ Accuracy: {test_results['accuracy']:.3f}")
            print(f"   ‚úÖ F1 Weighted: {test_results['f1_weighted']:.3f}")
            print(f"   ‚úÖ Hamming Loss: {test_results['hamming_loss']:.3f}")

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
    else:
        print("   ‚ö†Ô∏è H2O.ai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º...")

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "=" * 60)
    print("üìä –ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï:")
    print("=" * 60)
    print(f"{'–ü–æ–¥—Ö–æ–¥':<25} {'Accuracy':<10} {'F1 Weighted':<12} {'Hamming Loss':<12}")
    print("-" * 60)

    for name, result in sorted(approaches.items(),
                               key=lambda x: x[1]['accuracy'],
                               reverse=True):
        acc = result['accuracy']
        f1 = result['f1_weighted']
        h_loss = result['hamming_loss']

        print(f"{name:<25} {acc:<10.3f} {f1:<12.3f} {h_loss:<12.3f}")

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
    print("\n" + "=" * 60)
    print("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø:")

    if approaches:
        best_approach = max(approaches.items(), key=lambda x: x[1]['accuracy'])[0]
        print(f"   –õ—É—á—à–∏–π –ø–æ–¥—Ö–æ–¥: {best_approach.upper()}")

        if 'h2o' in best_approach:
            print("   H2O –ø–æ–∫–∞–∑–∞–ª —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö")
        else:
            print("   Random Search CV –æ–±–µ—Å–ø–µ—á–∏–ª –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏")

    return approaches


# ============================================================================
# –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô)
# ============================================================================

def main_optimized():
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        from util.jsonl_process import read_jsonl_basic

        train_data = read_jsonl_basic('../util/news_multilabel_train_data.jsonl')
        val_data = read_jsonl_basic('../util/news_multilabel_val_data.jsonl')
        test_data = read_jsonl_basic('../util/news_multilabel_test_data.jsonl')

        print(f"üìä –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã:")
        print(f"   Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   Validation: {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   Test: {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")

        if train_data:
            print(f"\nüìù –ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–∏:")
            print(f"   Text: {train_data[0]['text'][:100]}...")
            print(f"   Labels: {train_data[0]['binary_labels']}")
            print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫: {len(train_data[0]['binary_labels'])}")

    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ: {e}")
        print("–°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏...")

        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        train_data = [
            {"text": "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ 1 –æ —Å–ø–æ—Ä—Ç–µ –∏ –ø–æ–ª–∏—Ç–∏–∫–µ", "binary_labels": [1, 0, 1, 0]},
            {"text": "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ 2 –æ–± —ç–∫–æ–Ω–æ–º–∏–∫–µ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö", "binary_labels": [0, 1, 0, 1]},
            {"text": "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ 3 –æ –∫—É–ª—å—Ç—É—Ä–µ –∏ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏", "binary_labels": [1, 1, 0, 0]},
            {"text": "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ 4 –æ –∑–¥–æ—Ä–æ–≤—å–µ –∏ –Ω–∞—É–∫–µ", "binary_labels": [0, 0, 1, 1]},
        ]
        val_data = [
            {"text": "–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç 1", "binary_labels": [1, 0, 1, 0]},
            {"text": "–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç 2", "binary_labels": [0, 1, 0, 1]},
        ]
        test_data = [
            {"text": "–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç 1", "binary_labels": [1, 0, 0, 1]},
            {"text": "–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç 2", "binary_labels": [0, 1, 1, 0]},
        ]
        print(f"üìä –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ):")
        print(f"   Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   Validation: {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   Test: {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    print("\n" + "=" * 60)

    # –í–∞—Ä–∏–∞–Ω—Ç 1: RandomizedSearchCV
    print("\nüéØ –í–ê–†–ò–ê–ù–¢ 1: RANDOMIZED SEARCH CV")
    print("=" * 40)

    try:
        classifier1 = MultiLabelTextClassifier(
            n_iter=3,  # –ú–∏–Ω–∏–º—É–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            max_training_time=20,
            random_state=42
        )

        classifier1.train(train_data, val_data)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        classifier1.save_model("multilabel_fast_random_search.pkl")

        # –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        classifier1.plot_training_history(save_path='fast_random_search_history.png')

        # –û—Ü–µ–Ω–∫–∞
        print("\nüß™ –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–ï:")
        results1 = classifier1.evaluate(test_data)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

    # –í–∞—Ä–∏–∞–Ω—Ç 2: Fast H2O GLM
    if H2O_AVAILABLE:
        print("\nüöÄ –í–ê–†–ò–ê–ù–¢ 2: FAST H2O GLM")
        print("=" * 40)

        try:
            classifier2 = FastH2OGLMClassifier(seed=42)
            classifier2.train(train_data, val_data)

            # –û—Ü–µ–Ω–∫–∞
            print("\nüß™ –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–ï:")
            results2 = classifier2.evaluate(test_data)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤
    print("\n" + "=" * 60)
    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –í–°–ï–• –ü–û–î–•–û–î–û–í")
    print("=" * 60)

    approaches = compare_automl_approaches_optimized(train_data, val_data, test_data)

    # –ü—Ä–∏–º–µ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    print("\n" + "=" * 60)
    print("üîÆ –ü–†–ò–ú–ï–† –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø")
    print("=" * 60)

    if test_data and approaches:
        sample_text = test_data[0]['text']

        print(f"\n–¢–µ–∫—Å—Ç: {sample_text[:100]}...")
        print(f"–ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏: {test_data[0]['binary_labels']}")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π —É—Å–ø–µ—à–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        for approach_name, result in approaches.items():
            classifier = result['classifier']
            if classifier:
                try:
                    predictions, pred_labels, probs = classifier.predict([sample_text])
                    print(f"\nüìä {approach_name.upper()} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:")
                    print(f"   –ú–µ—Ç–∫–∏: {pred_labels[0]}")
                    break
                except:
                    continue

    # –ó–∞–∫—Ä—ã—Ç–∏–µ H2O
    if H2O_AVAILABLE:
        try:
            h2o.shutdown(prompt=False)
            print("\n‚úÖ H2O —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –∑–∞–∫—Ä—ã—Ç—ã")
        except:
            pass


if __name__ == "__main__":
    main_optimized()