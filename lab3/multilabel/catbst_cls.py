from catboost import CatBoostClassifier, Pool
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score, multilabel_confusion_matrix, hamming_loss
from sklearn.dummy import DummyClassifier
import numpy as np
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import json
import warnings

warnings.filterwarnings('ignore')


class MultiLabelCatBoostClassifier:
    """
    –ú–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞ CatBoost
    """

    def __init__(self, n_labels=14, iterations=500, depth=6, learning_rate=0.1,
                 random_state=42, task_type="CPU", verbose=100):
        """
        Args:
            n_labels: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫/–∫–ª–∞—Å—Å–æ–≤
            iterations: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –±—É—Å—Ç–∏–Ω–≥–∞
            depth: –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤
            learning_rate: —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            random_state: seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
            task_type: "CPU" –∏–ª–∏ "GPU" (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
            verbose: —á–∞—Å—Ç–æ—Ç–∞ –≤—ã–≤–æ–¥–∞ –ª–æ–≥–æ–≤ (0 - –±–µ–∑ –≤—ã–≤–æ–¥–∞)
        """
        self.vectorizer = None  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–∑–∂–µ
        self.n_labels = n_labels
        self.is_trained = False
        self.estimators_ = []  # –±—É–¥–µ–º —Ö—Ä–∞–Ω–∏—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç–∫–∏ –æ—Ç–¥–µ–ª—å–Ω–æ
        self.single_class_labels = set()  # –º–µ—Ç–∫–∏ —Å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º –∫–ª–∞—Å—Å–æ–º
        self.loss_history = []  # –∏—Å—Ç–æ—Ä–∏—è –ø–æ—Ç–µ—Ä—å
        self.val_loss_history = []  # –∏—Å—Ç–æ—Ä–∏—è –ø–æ—Ç–µ—Ä—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        self.accuracy_history = []  # –∏—Å—Ç–æ—Ä–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
        self.val_accuracy_history = []  # –∏—Å—Ç–æ—Ä–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        self.iterations = iterations
        self.depth = depth
        self.learning_rate = learning_rate
        self.random_state = random_state
        self.task_type = task_type
        self.verbose = verbose
        self.feature_importances_ = None  # –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.eval_history = []  # –∏—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è

    def _create_catboost_classifier(self):
        """–°–æ–∑–¥–∞–µ—Ç CatBoost –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä"""
        return CatBoostClassifier(
            iterations=self.iterations,
            depth=self.depth,
            learning_rate=self.learning_rate,
            random_seed=self.random_state,
            task_type=self.task_type,
            verbose=self.verbose,
            loss_function='Logloss',
            eval_metric='Accuracy',
            early_stopping_rounds=50,
            use_best_model=True,
            od_type='Iter',
            l2_leaf_reg=3,
            border_count=254
        )

    def prepare_data(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω—ã–µ –º–µ—Ç–∫–∏
        """
        texts = [item['text'] for item in data]
        labels = [item['binary_labels'] for item in data]
        return texts, np.array(labels)

    def train(self, train_data, val_data=None):
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫
        """
        print("üéØ –û–ë–£–ß–ï–ù–ò–ï –ú–ù–û–ì–û–ú–ï–¢–û–ß–ù–û–ì–û CATBOOST –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train = self.prepare_data(train_data)

        print(f"üìä –†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {len(X_train)}")
        print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫: {self.n_labels}")
        print(f"üìä –§–æ—Ä–º–∞—Ç –º–µ—Ç–æ–∫: {y_train.shape}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞
        print("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            min_df=2,
            max_df=0.8,
            ngram_range=(1, 2)
        )

        X_train_vec = self.vectorizer.fit_transform(X_train)
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ dense —Ñ–æ—Ä–º–∞—Ç –¥–ª—è CatBoost
        X_train_dense = X_train_vec.toarray()
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_dense.shape}")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –µ—Å—Ç—å
        eval_set = None
        if val_data:
            X_val, y_val = self.prepare_data(val_data)
            X_val_vec = self.vectorizer.transform(X_val)
            X_val_dense = X_val_vec.toarray()
            eval_set = [(X_val_dense, y_val)]

        # –û–±—É—á–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç–∫–∏
        print("üê± –û–±—É—á–µ–Ω–∏–µ CatBoost –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç–∫–∏...")

        # –°–±—Ä–æ—Å —Å–ø–∏—Å–∫–æ–≤
        self.estimators_ = []
        self.single_class_labels = set()
        self.feature_importances_ = np.zeros((self.n_labels, X_train_dense.shape[1]))
        self.eval_history = []

        for label_idx in range(self.n_labels):
            y_single = y_train[:, label_idx]
            unique_classes = np.unique(y_single)

            if len(unique_classes) < 2:
                # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å, –∏—Å–ø–æ–ª—å–∑—É–µ–º DummyClassifier
                print(f"   –ú–µ—Ç–∫–∞ {label_idx}: —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å ({unique_classes[0]}) - –∏—Å–ø–æ–ª—å–∑—É–µ–º DummyClassifier")
                clf = DummyClassifier(strategy='constant', constant=unique_classes[0])
                clf.fit(X_train_dense, y_single)
                self.single_class_labels.add(label_idx)
            else:
                # –ï—Å–ª–∏ –¥–≤–∞ –∫–ª–∞—Å—Å–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º CatBoost
                print(f"   –ú–µ—Ç–∫–∞ {label_idx}: –æ–±—É—á–µ–Ω–∏–µ CatBoost...")
                clf = self._create_catboost_classifier()

                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ eval_set –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–µ—Ç–∫–∏
                if eval_set:
                    X_val_dense, y_val = eval_set[0]
                    y_val_single = y_val[:, label_idx]
                    val_pool = Pool(X_val_dense, label=y_val_single)
                    train_pool = Pool(X_train_dense, label=y_single)

                    clf.fit(
                        train_pool,
                        eval_set=val_pool,
                        verbose_eval=self.verbose,
                        plot=False
                    )
                else:
                    train_pool = Pool(X_train_dense, label=y_single)
                    clf.fit(train_pool, verbose_eval=self.verbose, plot=False)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
                if hasattr(clf, 'get_evals_result'):
                    history = clf.get_evals_result()
                    if history:
                        self.eval_history.append(history)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                if hasattr(clf, 'get_feature_importance'):
                    self.feature_importances_[label_idx] = clf.get_feature_importance()

            self.estimators_.append(clf)

            # –ü—Ä–æ–≥—Ä–µ—Å—Å
            if (label_idx + 1) % 5 == 0 or (label_idx + 1) == self.n_labels:
                print(f"   –ü—Ä–æ–≥—Ä–µ—Å—Å: {label_idx + 1}/{self.n_labels} –º–µ—Ç–æ–∫ –æ–±—É—á–µ–Ω–æ")

        self.is_trained = True

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        y_pred_train = self._predict_from_estimators(X_train_dense)
        train_accuracy = accuracy_score(y_train, y_pred_train)
        train_hamming = hamming_loss(y_train, y_pred_train)

        print(f"\n‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train: {train_accuracy:.4f}")
        print(f"‚úÖ –ü–æ—Ç–µ—Ä—è –•—ç–º–º–∏–Ω–≥–∞ –Ω–∞ train: {train_hamming:.4f}")
        print(f"‚úÖ –ú–µ—Ç–∫–∏ —Å –æ–¥–Ω–∏–º –∫–ª–∞—Å—Å–æ–º: {sorted(self.single_class_labels)}")

        self.loss_history.append(train_hamming)
        self.accuracy_history.append(train_accuracy)

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        if val_data:
            val_accuracy, val_hamming = self.evaluate(val_data, verbose=False)
            print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ val: {val_accuracy:.4f}")
            print(f"‚úÖ –ü–æ—Ç–µ—Ä—è –•—ç–º–º–∏–Ω–≥–∞ –Ω–∞ val: {val_hamming:.4f}")
            self.val_loss_history.append(val_hamming)
            self.val_accuracy_history.append(val_accuracy)

        # –ü–æ–∫–∞–∂–µ–º –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–µ—Ç–æ–∫ —Å –¥–≤—É–º—è –∫–ª–∞—Å—Å–∞–º–∏
        self._show_important_features(top_n=10)

        # –ê–Ω–∞–ª–∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        self._analyze_model()

    def _predict_from_estimators(self, X_dense):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö –æ–±—É—á–µ–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        predictions = []
        for clf in self.estimators_:
            if hasattr(clf, 'predict'):
                pred = clf.predict(X_dense)
                predictions.append(pred)
            else:
                # –î–ª—è DummyClassifier
                pred = clf.predict(X_dense)
                predictions.append(pred)
        return np.array(predictions).T

    def _predict_proba_from_estimators(self, X_dense):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö –æ–±—É—á–µ–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        probabilities = []
        for idx, clf in enumerate(self.estimators_):
            if idx in self.single_class_labels:
                # –î–ª—è DummyClassifier –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º [1, 0] –∏–ª–∏ [0, 1]
                pred = clf.predict(X_dense)
                prob = np.zeros((len(pred), 2))
                for i in range(len(pred)):
                    if pred[i] == 1:
                        prob[i] = [0, 1]  # [P(0), P(1)]
                    else:
                        prob[i] = [1, 0]  # [P(0), P(1)]
                probabilities.append(prob[:, 1])
            else:
                # CatBoost –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
                prob = clf.predict_proba(X_dense)
                probabilities.append(prob[:, 1])  # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ 1
        return np.array(probabilities).T

    def predict(self, texts):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        if self.vectorizer is None:
            raise Exception("–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω!")

        X_vec = self.vectorizer.transform(texts)
        X_dense = X_vec.toarray()
        predictions = self._predict_from_estimators(X_dense)
        probabilities = self._predict_proba_from_estimators(X_dense)

        return predictions, probabilities

    def predict_single(self, text, threshold=0.5):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        predictions, probabilities = self.predict([text])
        pred = predictions[0]
        prob = probabilities[0]

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        binary_pred = (prob > threshold).astype(int)

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –∞–∫—Ç–∏–≤–Ω—ã—Ö –º–µ—Ç–æ–∫
        active_labels = [i for i, val in enumerate(binary_pred) if val == 1]

        return {
            'prediction': pred.tolist(),
            'binary_prediction': binary_pred.tolist(),
            'probabilities': prob.tolist(),
            'active_labels': active_labels,
            'confidence': np.mean(prob) if len(prob) > 0 else 0
        }

    def evaluate(self, test_data, verbose=True):
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        if self.vectorizer is None:
            raise Exception("–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω! –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å.")

        X_test, y_test = self.prepare_data(test_data)
        X_test_vec = self.vectorizer.transform(X_test)
        X_test_dense = X_test_vec.toarray()

        y_pred = self._predict_from_estimators(X_test_dense)
        accuracy = accuracy_score(y_test, y_pred)
        hamming = hamming_loss(y_test, y_pred)

        if verbose:
            print(f"\nüìä –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò CATBOOST:")
            print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f}")
            print(f"   –ü–æ—Ç–µ—Ä—è –•—ç–º–º–∏–Ω–≥–∞: {hamming:.4f}")

            # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–∫ —Å –æ–¥–Ω–∏–º –∫–ª–∞—Å—Å–æ–º
            if self.single_class_labels:
                print(f"\n‚ö†Ô∏è  –ú–µ—Ç–∫–∏ —Å –æ–¥–Ω–∏–º –∫–ª–∞—Å—Å–æ–º –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {sorted(self.single_class_labels)}")
                print("   (–¥–ª—è —ç—Ç–∏—Ö –º–µ—Ç–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è DummyClassifier)")

            print("\nüìä –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –ö–õ–ê–°–°–ê–ú:")

            # –û—Ç—á–µ—Ç –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É
            for i in range(self.n_labels):
                y_true = y_test[:, i]
                y_pred_single = y_pred[:, i]
                unique_classes = np.unique(y_true)

                if len(unique_classes) < 2:
                    print(f"\n   –ö–ª–∞—Å—Å {i} (–æ–¥–∏–Ω –∫–ª–∞—Å—Å –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {unique_classes[0]}):")
                    print(f"      –í—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {np.unique(y_pred_single)[0]}")
                    print(f"      Accuracy: {np.mean(y_true == y_pred_single):.4f}")
                elif i in self.single_class_labels:
                    print(f"\n   –ö–ª–∞—Å—Å {i} (–æ–¥–∏–Ω –∫–ª–∞—Å—Å –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö):")
                    print(f"      –í—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {np.unique(y_pred_single)[0]}")
                    print(f"      Accuracy: {np.mean(y_true == y_pred_single):.4f}")
                else:
                    print(f"\n   –ö–ª–∞—Å—Å {i}:")
                    try:
                        print(classification_report(y_true, y_pred_single,
                                                    target_names=[f'–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç({i})', f'–ü—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç({i})'],
                                                    zero_division=0, digits=4))
                    except:
                        print("      –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å classification_report")

            # –ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ —Å –¥–≤—É–º—è –∫–ª–∞—Å—Å–∞–º–∏ –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            print("\nüìà –ú–ê–¢–†–ò–¶–´ –û–®–ò–ë–û–ö –ü–û –ö–õ–ê–°–°–ê–ú (—Ç–æ–ª—å–∫–æ –¥–ª—è –º–µ—Ç–æ–∫ —Å –¥–≤—É–º—è –∫–ª–∞—Å—Å–∞–º–∏ –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö):")
            valid_labels = []
            for i in range(self.n_labels):
                if len(np.unique(y_test[:, i])) >= 2 and i not in self.single_class_labels:
                    valid_labels.append(i)

            if valid_labels:
                y_test_valid = y_test[:, valid_labels]
                y_pred_valid = y_pred[:, valid_labels]
                try:
                    cm = multilabel_confusion_matrix(y_test_valid, y_pred_valid)

                    for idx, label_idx in enumerate(valid_labels[:5]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5
                        print(f"\n   –ö–ª–∞—Å—Å {label_idx}:")
                        print(f"               –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ 0  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ 1")
                        print(f"   –†–µ–∞–ª—å–Ω–æ 0:     {cm[idx][0][0]:^10}        {cm[idx][0][1]:^10}")
                        print(f"   –†–µ–∞–ª—å–Ω–æ 1:     {cm[idx][1][0]:^10}        {cm[idx][1][1]:^10}")
                except:
                    print("   –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫")
            else:
                print("   –ù–µ—Ç –º–µ—Ç–æ–∫ —Å –¥–≤—É–º—è –∫–ª–∞—Å—Å–∞–º–∏ –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü –æ—à–∏–±–æ–∫")

        return accuracy, hamming

    def _show_important_features(self, top_n=10):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–µ—Ç–æ–∫ —Å –¥–≤—É–º—è –∫–ª–∞—Å—Å–∞–º–∏
        """
        if self.vectorizer is None or self.feature_importances_ is None:
            return

        feature_names = self.vectorizer.get_feature_names_out()

        print(f"\nüê± –¢–û–ü-{top_n} –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø –ú–ï–¢–û–ö –° –î–í–£–ú–Ø –ö–õ–ê–°–°–ê–ú–ò:")

        for label_idx in range(self.n_labels):
            if label_idx not in self.single_class_labels and np.sum(self.feature_importances_[label_idx]) > 0:

                print(f"\n   –ö–õ–ê–°–° {label_idx}:")

                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                importance_scores = self.feature_importances_[label_idx]
                top_indices = np.argsort(importance_scores)[-top_n:][::-1]

                print(f"      –°–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
                for i, idx in enumerate(top_indices[:top_n]):
                    if idx < len(feature_names):
                        print(f"        {i + 1}. {feature_names[idx]}: {importance_scores[idx]:.6f}")

    def _analyze_model(self):
        """
        –ê–Ω–∞–ª–∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ CatBoost
        """
        print("\nüê± –ê–ù–ê–õ–ò–ó –ú–û–î–ï–õ–ò CATBOOST:")
        print("=" * 50)

        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –º–æ–¥–µ–ª—è–º
        tree_counts = []

        for idx, clf in enumerate(self.estimators_):
            if idx not in self.single_class_labels:
                if hasattr(clf, 'tree_count_'):
                    tree_counts.append(clf.tree_count_)

        if tree_counts:
            print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤: {np.mean(tree_counts):.1f}")
            print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤: {np.min(tree_counts)}")
            print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤: {np.max(tree_counts)}")
            print(f"   –í—Å–µ–≥–æ –æ–±—É—á–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(self.estimators_)}")
            print(f"   –ò–∑ –Ω–∏—Ö CatBoost –º–æ–¥–µ–ª–µ–π: {len(tree_counts)}")
        else:
            print("   –ù–µ—Ç CatBoost –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (—Ç–æ–ª—å–∫–æ DummyClassifiers)")

    def save_model(self, filename):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        if not self.is_trained:
            print("‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –ù–µ—á–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å.")
            return

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º CatBoost –º–æ–¥–µ–ª–∏ –æ—Ç–¥–µ–ª—å–Ω–æ
        catboost_models = {}
        for idx, clf in enumerate(self.estimators_):
            if idx not in self.single_class_labels:
                catboost_models[idx] = clf

        joblib.dump({
            'estimators': self.estimators_,
            'vectorizer': self.vectorizer,
            'n_labels': self.n_labels,
            'single_class_labels': self.single_class_labels,
            'feature_importances': self.feature_importances_,
            'loss_history': self.loss_history,
            'val_loss_history': self.val_loss_history,
            'accuracy_history': self.accuracy_history,
            'val_accuracy_history': self.val_accuracy_history,
            'iterations': self.iterations,
            'depth': self.depth,
            'learning_rate': self.learning_rate,
            'random_state': self.random_state,
            'task_type': self.task_type,
            'verbose': self.verbose,
            'eval_history': self.eval_history
        }, filename)
        print(f"üíæ –ú–æ–¥–µ–ª—å CatBoost —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")

    def load_model(self, filename):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        loaded = joblib.load(filename)
        self.estimators_ = loaded['estimators']
        self.vectorizer = loaded['vectorizer']
        self.n_labels = loaded.get('n_labels', 14)
        self.single_class_labels = loaded.get('single_class_labels', set())
        self.feature_importances_ = loaded.get('feature_importances', None)
        self.loss_history = loaded.get('loss_history', [])
        self.val_loss_history = loaded.get('val_loss_history', [])
        self.accuracy_history = loaded.get('accuracy_history', [])
        self.val_accuracy_history = loaded.get('val_accuracy_history', [])
        self.iterations = loaded.get('iterations', 500)
        self.depth = loaded.get('depth', 6)
        self.learning_rate = loaded.get('learning_rate', 0.1)
        self.random_state = loaded.get('random_state', 42)
        self.task_type = loaded.get('task_type', "CPU")
        self.verbose = loaded.get('verbose', 100)
        self.eval_history = loaded.get('eval_history', [])
        self.is_trained = True
        print(f"üì• –ú–æ–¥–µ–ª—å CatBoost –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filename}")

    def plot_training_history(self):
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è
        """
        if not self.loss_history:
            print("–ù–µ—Ç –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤")
            return

        fig, axes = plt.subplots(1, 2, figsize=(12, 4))

        # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
        axes[0].plot(self.loss_history, label='Train Loss', marker='o')
        if self.val_loss_history:
            axes[0].plot(self.val_loss_history, label='Val Loss', marker='s')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Hamming Loss')
        axes[0].set_title('Loss History')
        axes[0].legend()
        axes[0].grid(True)

        # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
        axes[1].plot(self.accuracy_history, label='Train Accuracy', marker='o')
        if self.val_accuracy_history:
            axes[1].plot(self.val_accuracy_history, label='Val Accuracy', marker='s')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Accuracy')
        axes[1].set_title('Accuracy History')
        axes[1].legend()
        axes[1].grid(True)

        plt.tight_layout()
        plt.show()

    def plot_feature_importances(self, top_n=20):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        if self.feature_importances_ is None:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            return

        # –°—É–º–º–∏—Ä—É–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –≤—Å–µ–º –º–µ—Ç–∫–∞–º
        total_importance = np.sum(self.feature_importances_, axis=0)

        if np.sum(total_importance) == 0:
            print("–í—Å–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–∞–≤–Ω—ã –Ω—É–ª—é")
            return

        feature_names = self.vectorizer.get_feature_names_out()

        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-N —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        top_indices = np.argsort(total_importance)[-top_n:][::-1]
        top_features = [feature_names[i] for i in top_indices]
        top_importances = total_importance[top_indices]

        fig, ax = plt.subplots(figsize=(10, 6))
        y_pos = np.arange(len(top_features))

        ax.barh(y_pos, top_importances, align='center', color='mediumseagreen')
        ax.set_yticks(y_pos)
        ax.set_yticklabels(top_features)
        ax.invert_yaxis()  # –°–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ —Å–≤–µ—Ä—Ö—É
        ax.set_xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞')
        ax.set_title(f'–¢–æ–ø-{top_n} —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (CatBoost)')
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def plot_learning_curves(self, max_labels=5):
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫—Ä–∏–≤—ã—Ö –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –º–µ—Ç–æ–∫
        """
        if not self.eval_history:
            print("–ù–µ—Ç –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫—Ä–∏–≤—ã—Ö")
            return

        valid_labels = [i for i in range(self.n_labels)
                        if i not in self.single_class_labels and i < len(self.eval_history)]

        if not valid_labels:
            print("–ù–µ—Ç CatBoost –º–æ–¥–µ–ª–µ–π —Å –∏—Å—Ç–æ—Ä–∏–µ–π –æ–±—É—á–µ–Ω–∏—è")
            return

        n_labels = min(max_labels, len(valid_labels))

        fig, axes = plt.subplots(1, n_labels, figsize=(5 * n_labels, 4))
        if n_labels == 1:
            axes = [axes]

        for idx, label_idx in enumerate(valid_labels[:n_labels]):
            history = self.eval_history[label_idx]

            if 'learn' in history and 'validation' in history:
                learn_metric = history['learn']['Accuracy']
                val_metric = history['validation']['Accuracy']

                axes[idx].plot(learn_metric, label='Train', linewidth=2)
                axes[idx].plot(val_metric, label='Validation', linewidth=2)
                axes[idx].set_xlabel('–ò—Ç–µ—Ä–∞—Ü–∏–∏')
                axes[idx].set_ylabel('Accuracy')
                axes[idx].set_title(f'–ú–µ—Ç–∫–∞ {label_idx}')
                axes[idx].legend()
                axes[idx].grid(True, alpha=0.3)

        plt.suptitle('–ö—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è CatBoost', fontsize=14)
        plt.tight_layout()
        plt.show()

    def plot_confusion_matrices(self, test_data, max_classes=4):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü –æ—à–∏–±–æ–∫ –¥–ª—è –ø–µ—Ä–≤—ã—Ö N –∫–ª–∞—Å—Å–æ–≤ —Å –¥–≤—É–º—è –∫–ª–∞—Å—Å–∞–º–∏
        """
        if self.vectorizer is None:
            print("‚ö†Ô∏è  –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω. –ù–µ –º–æ–≥—É –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫.")
            return

        X_test, y_test = self.prepare_data(test_data)
        X_test_vec = self.vectorizer.transform(X_test)
        X_test_dense = X_test_vec.toarray()
        y_pred = self._predict_from_estimators(X_test_dense)

        # –§–∏–ª—å—Ç—Ä—É–µ–º –º–µ—Ç–∫–∏ —Å –¥–≤—É–º—è –∫–ª–∞—Å—Å–∞–º–∏ –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        valid_labels = []
        for i in range(self.n_labels):
            if len(np.unique(y_test[:, i])) >= 2 and i not in self.single_class_labels:
                valid_labels.append(i)

        if not valid_labels:
            print("–ù–µ—Ç –º–µ—Ç–æ–∫ —Å –¥–≤—É–º—è –∫–ª–∞—Å—Å–∞–º–∏ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü –æ—à–∏–±–æ–∫")
            return

        n_classes = min(max_classes, len(valid_labels))

        if n_classes == 0:
            print("–ù–µ—Ç –º–µ—Ç–æ–∫ —Å –¥–≤—É–º—è –∫–ª–∞—Å—Å–∞–º–∏ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü –æ—à–∏–±–æ–∫")
            return

        fig, axes = plt.subplots(1, n_classes, figsize=(4 * n_classes, 4))

        if n_classes == 1:
            axes = [axes]

        for i in range(n_classes):
            label_idx = valid_labels[i]
            cm = multilabel_confusion_matrix(y_test[:, [label_idx]], y_pred[:, [label_idx]])[0]

            sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',
                        xticklabels=['Pred 0', 'Pred 1'],
                        yticklabels=['True 0', 'True 1'],
                        ax=axes[i])
            axes[i].set_title(f'Confusion Matrix - Class {label_idx}')
            axes[i].set_ylabel('True Label')
            axes[i].set_xlabel('Predicted Label')

        plt.tight_layout()
        plt.show()

    def analyze_label_distribution(self, data):
        """
        –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Ç–æ–∫ –≤ –¥–∞–Ω–Ω—ã—Ö
        """
        _, y = self.prepare_data(data)

        print("\nüìä –ê–ù–ê–õ–ò–ó –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –ú–ï–¢–û–ö:")
        print("=" * 50)

        for i in range(self.n_labels):
            unique, counts = np.unique(y[:, i], return_counts=True)
            print(f"–ú–µ—Ç–∫–∞ {i}:")
            for val, count in zip(unique, counts):
                percentage = count / len(y) * 100
                print(f"  –ö–ª–∞—Å—Å {val}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤ ({percentage:.1f}%)")
            if len(unique) < 2:
                print(f"  ‚ö†Ô∏è  –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å!")
            print()

    def compare_hyperparameters(self, train_data, val_data=None,
                                iterations_list=[100, 300, 500],
                                depth_list=[4, 6, 8],
                                learning_rate_list=[0.01, 0.05, 0.1]):
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ CatBoost
        """
        print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –†–ê–ó–ù–´–• –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í CATBOOST:")
        print("=" * 60)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train = self.prepare_data(train_data)

        # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
        vectorizer_temp = TfidfVectorizer(
            max_features=5000,
            min_df=2,
            max_df=0.8,
            ngram_range=(1, 2)
        )
        X_train_vec = vectorizer_temp.fit_transform(X_train)
        X_train_dense = X_train_vec.toarray()

        if val_data:
            X_val, y_val = self.prepare_data(val_data)
            X_val_vec = vectorizer_temp.transform(X_val)
            X_val_dense = X_val_vec.toarray()

        results = []

        # –û–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞ –ø–µ—Ä–≤–æ–π –º–µ—Ç–∫–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        test_label = 0
        y_single = y_train[:, test_label]

        if len(np.unique(y_single)) < 2:
            print("–ü–µ—Ä–≤–∞—è –º–µ—Ç–∫–∞ –∏–º–µ–µ—Ç —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å, –≤—ã–±–∏—Ä–∞–µ–º –¥—Ä—É–≥—É—é...")
            for i in range(1, self.n_labels):
                if len(np.unique(y_train[:, i])) >= 2:
                    test_label = i
                    y_single = y_train[:, test_label]
                    break

        print(f"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ –º–µ—Ç–∫–µ {test_label}")

        for iterations in iterations_list:
            for depth in depth_list:
                for lr in learning_rate_list:
                    print(f"\nüìä iterations={iterations}, depth={depth}, lr={lr}:")

                    # –û–±—É—á–∞–µ–º CatBoost
                    clf = CatBoostClassifier(
                        iterations=iterations,
                        depth=depth,
                        learning_rate=lr,
                        random_seed=self.random_state,
                        task_type=self.task_type,
                        verbose=0,
                        loss_function='Logloss',
                        eval_metric='Accuracy'
                    )

                    if val_data:
                        y_val_single = y_val[:, test_label]
                        train_pool = Pool(X_train_dense, label=y_single)
                        val_pool = Pool(X_val_dense, label=y_val_single)

                        clf.fit(
                            train_pool,
                            eval_set=val_pool,
                            verbose_eval=False
                        )

                        # –û—Ü–µ–Ω–∫–∞
                        y_pred_train = clf.predict(X_train_dense)
                        train_accuracy = accuracy_score(y_single, y_pred_train)

                        y_pred_val = clf.predict(X_val_dense)
                        val_accuracy = accuracy_score(y_val_single, y_pred_val)

                        print(f"   Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}")
                    else:
                        train_pool = Pool(X_train_dense, label=y_single)
                        clf.fit(train_pool, verbose_eval=False)

                        y_pred_train = clf.predict(X_train_dense)
                        train_accuracy = accuracy_score(y_single, y_pred_train)
                        print(f"   Train Accuracy: {train_accuracy:.4f}")

                    results.append({
                        'iterations': iterations,
                        'depth': depth,
                        'learning_rate': lr,
                        'train_accuracy': train_accuracy,
                        'val_accuracy': val_accuracy if val_data else None
                    })

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self._plot_catboost_hyperparameter_comparison(results)

    def _plot_catboost_hyperparameter_comparison(self, results):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        if not results:
            print("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤")
            return

        # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç–∫—É –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        iterations_list = sorted(list(set(r['iterations'] for r in results)))
        depth_list = sorted(list(set(r['depth'] for r in results)))
        lr_list = sorted(list(set(r['learning_rate'] for r in results)))

        # –î–ª—è –∫–∞–∂–¥–æ–π learning rate —Å–æ–∑–¥–∞–µ–º heatmap
        fig, axes = plt.subplots(1, len(lr_list), figsize=(5 * len(lr_list), 4))
        if len(lr_list) == 1:
            axes = [axes]

        for idx, lr in enumerate(lr_list):
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ learning rate
            lr_results = [r for r in results if r['learning_rate'] == lr]

            # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è heatmap
            accuracy_matrix = np.zeros((len(depth_list), len(iterations_list)))

            for r in lr_results:
                i = depth_list.index(r['depth'])
                j = iterations_list.index(r['iterations'])
                accuracy_matrix[i, j] = r['train_accuracy']

            # Heatmap
            im = axes[idx].imshow(accuracy_matrix, cmap='YlGn', aspect='auto')

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Å–µ–π
            axes[idx].set_xticks(np.arange(len(iterations_list)))
            axes[idx].set_xticklabels(iterations_list)
            axes[idx].set_yticks(np.arange(len(depth_list)))
            axes[idx].set_yticklabels(depth_list)
            axes[idx].set_xlabel('Iterations')
            axes[idx].set_ylabel('Depth')
            axes[idx].set_title(f'Learning Rate = {lr}')

            # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ —è—á–µ–π–∫–∏
            for i in range(len(depth_list)):
                for j in range(len(iterations_list)):
                    text = axes[idx].text(j, i, f'{accuracy_matrix[i, j]:.3f}',
                                          ha="center", va="center", color="black")

        plt.suptitle('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ CatBoost', fontsize=14)
        plt.tight_layout()
        plt.show()


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω–æ–≥–æ CatBoost –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    """

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —á—Ç–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    def read_jsonl_basic(filepath):
        data = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                data.append(json.loads(line.strip()))
        return data

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        train_data = read_jsonl_basic('../util/news_multilabel_train_data.jsonl')
        val_data = read_jsonl_basic('../util/news_multilabel_val_data.jsonl')
        test_data = read_jsonl_basic('../util/news_multilabel_test_data.jsonl')
    except FileNotFoundError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}")
        return

    print(f"üìä –î–∞–Ω–Ω—ã–µ: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫ –∏–∑ –¥–∞–Ω–Ω—ã—Ö
    if train_data:
        n_labels = len(train_data[0]['binary_labels'])
        print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫: {n_labels}")
    else:
        print("‚ùå –ù–µ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
        return

    # 1. –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Ç–æ–∫
    print("\n" + "=" * 50)
    classifier = MultiLabelCatBoostClassifier(n_labels=n_labels, iterations=300, verbose=100)
    classifier.analyze_label_distribution(train_data)

    # 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    print("\n" + "=" * 50)
    print("üî¨ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ CatBoost (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —à–∞–≥)...")
    try:
        classifier.compare_hyperparameters(
            train_data[:1000],  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            val_data[:200] if val_data else None,
            iterations_list=[100, 200],
            depth_list=[4, 6],
            learning_rate_list=[0.05, 0.1]
        )
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}")
        print("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å iterations=300, depth=6, learning_rate=0.1")

    # 3. –û–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    print("\n" + "=" * 50)
    print("üê± –û–ë–£–ß–ï–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò CATBOOST...")
    classifier = MultiLabelCatBoostClassifier(
        n_labels=n_labels,
        iterations=300,
        depth=6,
        learning_rate=0.1,
        verbose=100
    )
    classifier.train(train_data, val_data)

    # 5. –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\n" + "=" * 50)
    print("üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•:")
    try:
        accuracy, hamming = classifier.evaluate(test_data)
        print(f"   –ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f}")
        print(f"   –ò—Ç–æ–≥–æ–≤–∞—è –ø–æ—Ç–µ—Ä—è –•—ç–º–º–∏–Ω–≥–∞: {hamming:.4f}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏: {e}")

    # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    try:
        classifier.save_model("multilabel_catboost.pkl")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")

    # 7. –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫–∏
    print("\n" + "=" * 50)
    print("üìà –ü–û–°–¢–†–û–ï–ù–ò–ï –ì–†–ê–§–ò–ö–û–í:")
    try:
        classifier.plot_training_history()
        classifier.plot_feature_importances(top_n=20)
        classifier.plot_learning_curves(max_labels=3)
        classifier.plot_confusion_matrices(test_data, max_classes=4)
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤: {e}")


# –ü—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± –±—ã—Å—Ç—Ä–æ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å
def quick_train_catboost(train_file, val_file=None, n_labels=14, iterations=300):
    """
    –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ CatBoost –º–æ–¥–µ–ª–∏ –∏–∑ —Ñ–∞–π–ª–æ–≤
    """

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    def load_jsonl(filepath):
        data = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                data.append(json.loads(line))
        return data

    train_data = load_jsonl(train_file)
    val_data = load_jsonl(val_file) if val_file else None

    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    classifier = MultiLabelCatBoostClassifier(
        n_labels=n_labels,
        iterations=iterations,
        verbose=0  # –ë–µ–∑ –≤—ã–≤–æ–¥–∞ –ª–æ–≥–æ–≤
    )
    classifier.train(train_data, val_data)

    return classifier


if __name__ == "__main__":
    main()