import torch
import numpy as np
from typing import List, Dict, Any, Union
from scipy.spatial.distance import cosine

from util.jsonl_process import read_jsonl_basic


class DocumentEmbeddingAggregator:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
    """

    def __init__(self, model_name: str = "cointegrated/rubert-tiny2"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.load_model(model_name)

    def load_model(self, model_name: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        from transformers import AutoTokenizer, AutoModel

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {self.device}")

    def get_document_embedding(self, text: str,
                               method: str = "mean",
                               layer: int = -1,
                               remove_special_tokens: bool = True) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤

        Args:
            text: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
            method: –º–µ—Ç–æ–¥ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ ('mean', 'max', 'cls', 'pooler', 'weighted')
            layer: —Å–ª–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            remove_special_tokens: —É–¥–∞–ª—è—Ç—å –ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã ([CLS], [SEP], [PAD])
        """
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        encoded = self.tokenizer(
            text,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors="pt"
        )

        encoded = {k: v.to(self.device) for k, v in encoded.items()}

        # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        with torch.no_grad():
            outputs = self.model(**encoded, output_hidden_states=True)

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω—É–∂–Ω–æ–≥–æ —Å–ª–æ—è
        hidden_states = outputs.hidden_states
        layer_idx = layer if layer >= 0 else len(hidden_states) + layer
        token_embeddings = hidden_states[layer_idx]  # [batch_size, seq_len, hidden_size]

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
        if method == "mean":
            doc_embedding = self._mean_pooling(token_embeddings, encoded['attention_mask'], remove_special_tokens)
        elif method == "max":
            doc_embedding = self._max_pooling(token_embeddings, encoded['attention_mask'], remove_special_tokens)
        elif method == "cls":
            doc_embedding = self._cls_pooling(token_embeddings)
        elif method == "pooler":
            doc_embedding = self._pooler_output(outputs)
        elif method == "weighted":
            doc_embedding = self._weighted_pooling(token_embeddings, encoded['attention_mask'], remove_special_tokens)
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥: {method}")

        return {
            'document_embedding': doc_embedding.cpu().numpy(),
            'method': method,
            'layer': layer_idx,
            'token_embeddings': token_embeddings.cpu().numpy(),
            'tokens': self.tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])
        }

    def _mean_pooling(self, token_embeddings: torch.Tensor,
                      attention_mask: torch.Tensor,
                      remove_special_tokens: bool = True) -> torch.Tensor:
        """
        –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤
        """
        if remove_special_tokens:
            # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
            input_mask = attention_mask.clone()
            special_tokens_mask = self._get_special_tokens_mask(attention_mask)
            input_mask[special_tokens_mask] = 0
        else:
            input_mask = attention_mask

        # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –º–∞—Å–∫–∏
        input_mask_expanded = input_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
        return sum_embeddings / sum_mask

    def _max_pooling(self, token_embeddings: torch.Tensor,
                     attention_mask: torch.Tensor,
                     remove_special_tokens: bool = True) -> torch.Tensor:
        """
        –ú–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è –ø–æ –∫–∞–∂–¥–æ–º—É –∏–∑–º–µ—Ä–µ–Ω–∏—é
        """
        if remove_special_tokens:
            input_mask = attention_mask.clone()
            special_tokens_mask = self._get_special_tokens_mask(attention_mask)
            input_mask[special_tokens_mask] = 0
            input_mask_expanded = input_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        else:
            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()

        # –ó–∞–º–µ–Ω—è–µ–º padding —Ç–æ–∫–µ–Ω—ã –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        token_embeddings[input_mask_expanded == 0] = -1e9
        return torch.max(token_embeddings, 1)[0]

    def _cls_pooling(self, token_embeddings: torch.Tensor) -> torch.Tensor:
        """
        –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ [CLS] —Ç–æ–∫–µ–Ω–∞ –∫–∞–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
        """
        return token_embeddings[:, 0, :]

    def _pooler_output(self, outputs) -> torch.Tensor:
        """
        –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ pooler output (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        """
        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:
            return outputs.pooler_output
        else:
            raise ValueError("Pooler output –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏")

    def _weighted_pooling(self, token_embeddings: torch.Tensor,
                          attention_mask: torch.Tensor,
                          remove_special_tokens: bool = True) -> torch.Tensor:
        """
        –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º IDF –≤–µ—Å–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        """
        if remove_special_tokens:
            input_mask = attention_mask.clone()
            special_tokens_mask = self._get_special_tokens_mask(attention_mask)
            input_mask[special_tokens_mask] = 0
        else:
            input_mask = attention_mask

        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –≤–µ—Å–∞ –æ–±—Ä–∞—Ç–Ω–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã —á–∞—Å—Ç–æ—Ç–µ —Ç–æ–∫–µ–Ω–∞
        # –í —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ IDF –≤–µ—Å–∞
        weights = torch.ones_like(input_mask).float()

        # –£–º–µ–Ω—å—à–∞–µ–º –≤–µ—Å –¥–ª—è —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞—Ü–∏–∏
        tokens = self.tokenizer.convert_ids_to_tokens(attention_mask.nonzero()[:, 1])
        for i, token in enumerate(tokens):
            if token in ['.', ',', '!', '?', '–∏', '–≤', '–Ω–∞', '—Å']:
                weights[0, i] = 0.1

        input_mask_expanded = input_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        weights_expanded = weights.unsqueeze(-1).expand(token_embeddings.size()).float()

        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded * weights_expanded, 1)
        sum_weights = torch.clamp((input_mask_expanded * weights_expanded).sum(1), min=1e-9)
        return sum_embeddings / sum_weights

    def _get_special_tokens_mask(self, attention_mask: torch.Tensor) -> torch.Tensor:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å–∫–∏ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        """
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –ø–µ—Ä–≤—ã–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ç–æ–∫–µ–Ω—ã —á–∞—Å—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ
        batch_size, seq_len = attention_mask.shape
        special_mask = torch.zeros_like(attention_mask, dtype=torch.bool)

        # –ü–æ–º–µ—á–∞–µ–º –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω ([CLS]) –∏ padding —Ç–æ–∫–µ–Ω—ã
        special_mask[:, 0] = True  # [CLS]

        # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω ([SEP] –∏–ª–∏ –∫–æ–Ω–µ—Ü –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
        for i in range(batch_size):
            real_tokens = attention_mask[i].nonzero()
            if len(real_tokens) > 0:
                last_token_idx = real_tokens[-1]
                special_mask[i, last_token_idx] = True  # [SEP] –∏–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω

        return special_mask

    def compare_aggregation_methods(self, text: str) -> Dict[str, Any]:
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
        """
        methods = ["mean", "max", "cls", "weighted"]
        results = {}

        reference_method = "mean"
        reference_embedding = None

        for method in methods:
            try:
                result = self.get_document_embedding(text, method=method)
                embedding = result['document_embedding'][0]  # [hidden_size]
                results[method] = {
                    'embedding': embedding,
                    'shape': embedding.shape,
                    'norm': np.linalg.norm(embedding)
                }

                if method == reference_method:
                    reference_embedding = embedding

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è –º–µ—Ç–æ–¥–∞ {method}: {e}")
                results[method] = None

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å reference –º–µ—Ç–æ–¥–æ–º
        if reference_embedding is not None:
            for method, data in results.items():
                if data is not None and method != reference_method:
                    similarity = 1 - cosine(reference_embedding, data['embedding'])
                    data['similarity_with_mean'] = similarity

        return results


def demo_aggregation_methods():
    """
    –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
    """
    aggregator = DocumentEmbeddingAggregator()

    file_name1 = "../dataset/old_dataset.jsonl"
    file_name2 = "../dataset/new_dataset.jsonl"

    data1 = read_jsonl_basic(file_name1)
    data2 = read_jsonl_basic(file_name2)

    data = data1 + data2

    test_texts = []
    for item in data:
        test_texts.append(item['text'])

    print("üéØ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ú–ï–¢–û–î–û–í –ê–ì–†–ï–ì–ê–¶–ò–ò –≠–ú–ë–ï–î–î–ò–ù–ì–û–í")
    print("=" * 60)

    # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    print("\n1. üîç –°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–û–î–û–í –î–õ–Ø –û–î–ù–û–ì–û –¢–ï–ö–°–¢–ê:")
    sample_text = test_texts[0]
    methods_comparison = aggregator.compare_aggregation_methods(sample_text)

    for method, data in methods_comparison.items():
        if data:
            print(f"   {method:>10}: –Ω–æ—Ä–º–∞={data['norm']:.3f}", end="")
            if 'similarity_with_mean' in data:
                print(f", —Å—Ö–æ–∂–µ—Å—Ç—å —Å mean={data['similarity_with_mean']:.3f}")
            else:
                print()

    # 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
    print("\n2. üìä –°–•–û–ñ–ï–°–¢–¨ –¢–ï–ö–°–¢–û–í –° –†–ê–ó–ù–´–ú–ò –ú–ï–¢–û–î–ê–ú–ò:")
    methods = ["mean", "max", "cls"]

    for method in methods:
        print(f"\n   –ú–µ—Ç–æ–¥: {method}")
        embeddings = []

        for text in test_texts:
            result = aggregator.get_document_embedding(text, method=method)
            embeddings.append(result['document_embedding'][0])

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç–∏
        for i in range(len(test_texts)):
            for j in range(i + 1, len(test_texts)):
                similarity = 1 - cosine(embeddings[i], embeddings[j])
                print(f"      '{test_texts[i][:20]}...' vs '{test_texts[j][:20]}...': {similarity:.3f}")

    # 3. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤
    print("\n3. üî§ –ê–ù–ê–õ–ò–ó –¢–û–ö–ï–ù–û–í –ò –ò–• –í–ö–õ–ê–î–ê:")
    analyze_token_contributions(aggregator, sample_text)


def analyze_token_contributions(aggregator: DocumentEmbeddingAggregator, text: str):
    """
    –ê–Ω–∞–ª–∏–∑ –≤–∫–ª–∞–¥–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
    """
    print(f"   –¢–µ–∫—Å—Ç: '{text}'")

    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–∫–µ–Ω–æ–≤
    result = aggregator.get_document_embedding(text, method="mean")
    token_embeddings = result['token_embeddings'][0]  # [seq_len, hidden_size]
    doc_embedding = result['document_embedding'][0]  # [hidden_size]
    tokens = result['tokens']

    print(f"   –¢–æ–∫–µ–Ω—ã: {tokens}")
    print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤: {token_embeddings.shape}")
    print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞: {doc_embedding.shape}")

    # –í—ã—á–∏—Å–ª—è–µ–º –≤–∫–ª–∞–¥ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
    contributions = []
    for i, (token, token_emb) in enumerate(zip(tokens, token_embeddings)):
        if token not in ['[CLS]', '[SEP]', '[PAD]']:
            # –°—Ö–æ–∂–µ—Å—Ç—å —Ç–æ–∫–µ–Ω–∞ —Å –æ–±—â–∏–º –≤–µ–∫—Ç–æ—Ä–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞
            similarity = 1 - cosine(token_emb, doc_embedding)
            contributions.append((token, similarity))

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∫–ª–∞–¥—É
    contributions.sort(key=lambda x: x[1], reverse=True)

    print(f"   –¢–æ–ø-5 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:")
    for token, contribution in contributions[:5]:
        print(f"      '{token}': {contribution:.3f}")


class AdvancedDocumentEmbeddings:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    """

    def __init__(self, model_name: str = "cointegrated/rubert-tiny2"):
        self.aggregator = DocumentEmbeddingAggregator(model_name)

    def get_batch_embeddings(self, texts: List[str],
                             method: str = "mean",
                             batch_size: int = 8) -> np.ndarray:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        all_embeddings = []

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_embeddings = []

            for text in batch_texts:
                result = self.aggregator.get_document_embedding(text, method=method)
                batch_embeddings.append(result['document_embedding'][0])

            all_embeddings.extend(batch_embeddings)

        return np.array(all_embeddings)

    def find_similar_documents(self, query: str,
                               documents: List[str],
                               method: str = "mean",
                               top_k: int = 5) -> List[tuple]:
        """
        –ü–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å
        """
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_result = self.aggregator.get_document_embedding(query, method=method)
        query_embedding = query_result['document_embedding'][0]

        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        doc_embeddings = self.get_batch_embeddings(documents, method=method)

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarities = []
        for i, doc_emb in enumerate(doc_embeddings):
            similarity = 1 - cosine(query_embedding, doc_emb)
            similarities.append((documents[i], similarity))

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarities.sort(key=lambda x: x[1], reverse=True)

        return similarities[:top_k]

    def document_clustering(self, texts: List[str],
                            method: str = "mean",
                            n_clusters: int = 3) -> Dict[str, Any]:
        """
        –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º
        """
        from sklearn.cluster import KMeans
        from sklearn.metrics import silhouette_score

        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        embeddings = self.get_batch_embeddings(texts, method=method)

        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        clusters = kmeans.fit_predict(embeddings)

        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        silhouette_avg = silhouette_score(embeddings, clusters)

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        clustered_docs = {}
        for i, (text, cluster) in enumerate(zip(texts, clusters)):
            if cluster not in clustered_docs:
                clustered_docs[cluster] = []
            clustered_docs[cluster].append(text)

        return {
            'clusters': clustered_docs,
            'embeddings': embeddings,
            'cluster_centers': kmeans.cluster_centers_,
            'silhouette_score': silhouette_avg
        }


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
def demo_news_analysis():
    """
    –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø–æ–º–æ—â—å—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    """
    # news_articles = [
    #     "–í –ö–∞–∑–∞–Ω–∏ –æ—Ç–∫—Ä—ã–ª—Å—è –Ω–æ–≤—ã–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–∞—Ä–∫ –¥–ª—è IT-–∫–æ–º–ø–∞–Ω–∏–π. –ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ —Å–æ—Å—Ç–∞–≤–∏–ª–∏ 2 –º–∏–ª–ª–∏–∞—Ä–¥–∞ —Ä—É–±–ª–µ–π.",
    #     "–¢–∞—Ç–∞—Ä—Å—Ç–∞–Ω –∞–∫—Ç–∏–≤–Ω–æ –∏–Ω–≤–µ—Å—Ç–∏—Ä—É–µ—Ç –≤ —Ä–∞–∑–≤–∏—Ç–∏–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.",
    #     "–ü–æ–≥–æ–¥–∞ –≤ –ö–∞–∑–∞–Ω–∏: –Ω–∞ —ç—Ç–æ–π –Ω–µ–¥–µ–ª–µ –æ–∂–∏–¥–∞–µ—Ç—Å—è —Å–Ω–µ–≥ –∏ –ø–æ—Ö–æ–ª–æ–¥–∞–Ω–∏–µ –¥–æ -15 –≥—Ä–∞–¥—É—Å–æ–≤.",
    #     "–£—á–µ–Ω—ã–µ –ö–∞–∑–∞–Ω—Å–∫–æ–≥–æ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤.",
    #     "–í –¢–∞—Ç–∞—Ä—Å—Ç–∞–Ω–µ –ø—Ä–æ—Ö–æ–¥–∏—Ç –µ–∂–µ–≥–æ–¥–Ω—ã–π —Ñ–µ—Å—Ç–∏–≤–∞–ª—å —Ç–∞—Ç–∞—Ä—Å–∫–æ–π –∫—É–ª—å—Ç—É—Ä—ã –∏ —è–∑—ã–∫–∞.",
    #     "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –ø–æ–º–æ–≥–∞—é—Ç –≤—Ä–∞—á–∞–º –≤ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π –ø–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–º —Å–Ω–∏–º–∫–∞–º.",
    #     "–ö–∞–∑–∞–Ω—å —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Ü–µ–Ω—Ç—Ä–æ–º IT-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –≤ –ü–æ–≤–æ–ª–∂—å–µ, –ø—Ä–∏–≤–ª–µ–∫–∞—è –º–æ–ª–æ–¥—ã—Ö —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤."
    # ]

    file_name1 = "../dataset/old_dataset.jsonl"
    file_name2 = "../dataset/new_dataset.jsonl"

    data1 = read_jsonl_basic(file_name1)
    data2 = read_jsonl_basic(file_name2)

    data = data1 + data2

    news_articles = []
    for item in data:
        news_articles.append(item['text'])

    advanced_emb = AdvancedDocumentEmbeddings()

    print("üì∞ –ê–ù–ê–õ–ò–ó –ù–û–í–û–°–¢–ï–ô –° –ü–û–ú–û–©–¨–Æ –≠–ú–ë–ï–î–î–ò–ù–ì–û–í –î–û–ö–£–ú–ï–ù–¢–û–í")
    print("=" * 60)

    # 1. –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    print("\n1. üîç –ü–û–ò–°–ö –ü–û–•–û–ñ–ò–• –ù–û–í–û–°–¢–ï–ô:")
    query = "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"
    similar_news = advanced_emb.find_similar_documents(query, news_articles, top_k=3)

    print(f"   –ó–∞–ø—Ä–æ—Å: '{query}'")
    for i, (news, similarity) in enumerate(similar_news, 1):
        print(f"   {i}. {similarity:.3f} - {news[:60]}...")

    # 2. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π
    print("\n2. üéØ –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø –ù–û–í–û–°–¢–ï–ô:")
    clustering_result = advanced_emb.document_clustering(news_articles, n_clusters=3)

    print(f"   –ö–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (silhouette): {clustering_result['silhouette_score']:.3f}")

    for cluster_id, docs in clustering_result['clusters'].items():
        print(f"\n   –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}:")
        for doc in docs[:2]:  # –ü–æ–∫–∞–∂–µ–º –ø–æ 2 –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
            print(f"      - {doc[:50]}...")


if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
    demo_aggregation_methods()

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
    demo_news_analysis()