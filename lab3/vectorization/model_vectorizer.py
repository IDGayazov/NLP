import logging
import pickle
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

from gensim.models import Word2Vec, FastText

from util.load_tokenize_dataset import load_tokenize_ds


class EmbeddingModels:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    """

    def __init__(self, vector_size: int = 100, window: int = 5,
                 min_count: int = 2, workers: int = 4, epochs: int = 10):
        self.vector_size = vector_size
        self.window = window
        self.min_count = min_count
        self.workers = workers
        self.epochs = epochs

        # –ú–æ–¥–µ–ª–∏
        self.word2vec_model = None
        self.fasttext_model = None
        self.glove_vectors = None

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è
        self.training_metadata = {}

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        logging.basicConfig(
            format='%(asctime)s : %(levelname)s : %(message)s',
            level=logging.INFO
        )

    def train_word2vec(self, tokenized_texts: List[List[str]],
                       sg: int = 1, **kwargs) -> Word2Vec:
        """
        –û–±—É—á–µ–Ω–∏–µ Word2Vec –º–æ–¥–µ–ª–∏
        """
        print("üéØ –û–±—É—á–µ–Ω–∏–µ Word2Vec –º–æ–¥–µ–ª–∏...")

        self.word2vec_model = Word2Vec(
            sentences=tokenized_texts,
            vector_size=self.vector_size,
            window=self.window,
            min_count=self.min_count,
            workers=self.workers,
            sg=sg,
            epochs=self.epochs,
            **kwargs
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.training_metadata['word2vec'] = {
            'vocab_size': len(self.word2vec_model.wv),
            'training_date': datetime.now().isoformat(),
            'parameters': {
                'vector_size': self.vector_size,
                'window': self.window,
                'min_count': self.min_count,
                'sg': sg,
                'epochs': self.epochs
            }
        }

        print(f"‚úÖ Word2Vec –æ–±—É—á–µ–Ω–∞. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(self.word2vec_model.wv)}")
        return self.word2vec_model

    def train_fasttext(self, tokenized_texts: List[List[str]],
                       sg: int = 1, **kwargs) -> FastText:
        """
        –û–±—É—á–µ–Ω–∏–µ FastText –º–æ–¥–µ–ª–∏
        """
        print("üéØ –û–±—É—á–µ–Ω–∏–µ FastText –º–æ–¥–µ–ª–∏...")

        self.fasttext_model = FastText(
            sentences=tokenized_texts,
            vector_size=self.vector_size,
            window=self.window,
            min_count=self.min_count,
            workers=self.workers,
            sg=sg,
            epochs=self.epochs,
            **kwargs
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.training_metadata['fasttext'] = {
            'vocab_size': len(self.fasttext_model.wv),
            'training_date': datetime.now().isoformat(),
            'parameters': {
                'vector_size': self.vector_size,
                'window': self.window,
                'min_count': self.min_count,
                'sg': sg,
                'epochs': self.epochs
            }
        }

        print(f"‚úÖ FastText –æ–±—É—á–µ–Ω–∞. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(self.fasttext_model.wv)}")
        return self.fasttext_model

    def train_glove(self, tokenized_texts: List[List[str]],
                    corpus_file: str = "corpus.txt", **kwargs):
        """
        –û–±—É—á–µ–Ω–∏–µ GloVe –º–æ–¥–µ–ª–∏ (—á–µ—Ä–µ–∑ glove-python)
        """
        try:
            from glove import Corpus, Glove
        except ImportError:
            print("‚ùå glove-python –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install glove-python")
            return None

        print("üéØ –û–±—É—á–µ–Ω–∏–µ GloVe –º–æ–¥–µ–ª–∏...")

        # –°–æ–∑–¥–∞–µ–º –∫–æ—Ä–ø—É—Å
        corpus = Corpus()
        corpus.fit(tokenized_texts, window=self.window)

        # –û–±—É—á–∞–µ–º GloVe
        self.glove_model = Glove(no_components=self.vector_size,
                                 learning_rate=0.05)
        self.glove_model.fit(corpus.matrix, epochs=self.epochs,
                             no_threads=self.workers, verbose=True)
        self.glove_model.add_dictionary(corpus.dictionary)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.training_metadata['glove'] = {
            'vocab_size': len(corpus.dictionary),
            'training_date': datetime.now().isoformat(),
            'parameters': {
                'vector_size': self.vector_size,
                'window': self.window,
                'epochs': self.epochs
            }
        }

        print(f"‚úÖ GloVe –æ–±—É—á–µ–Ω–∞. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(corpus.dictionary)}")
        return self.glove_model

    def save_models(self, base_path: str = "embeddings") -> Dict[str, str]:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø—É—Ç—è–º–∏ –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º
        """
        Path(base_path).mkdir(exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        saved_paths = {}

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º Word2Vec
        if self.word2vec_model:
            w2v_path = f"{base_path}/word2vec_{timestamp}.model"
            self.word2vec_model.save(w2v_path)

            # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ word2vec
            w2v_vectors_path = f"{base_path}/word2vec_vectors_{timestamp}.kv"
            self.word2vec_model.wv.save(w2v_vectors_path)

            saved_paths['word2vec'] = w2v_path
            saved_paths['word2vec_vectors'] = w2v_vectors_path
            print(f"üíæ Word2Vec —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {w2v_path}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º FastText
        if self.fasttext_model:
            ft_path = f"{base_path}/fasttext_{timestamp}.model"
            self.fasttext_model.save(ft_path)

            ft_vectors_path = f"{base_path}/fasttext_vectors_{timestamp}.kv"
            self.fasttext_model.wv.save(ft_vectors_path)

            saved_paths['fasttext'] = ft_path
            saved_paths['fasttext_vectors'] = ft_vectors_path
            print(f"üíæ FastText —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {ft_path}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º GloVe
        if hasattr(self, 'glove_model') and self.glove_model:
            glove_path = f"{base_path}/glove_{timestamp}.model"

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ –≤–µ–∫—Ç–æ—Ä—ã
            with open(glove_path, 'wb') as f:
                pickle.dump(self.glove_model, f)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            glove_vectors_path = f"{base_path}/glove_vectors_{timestamp}.txt"
            self._save_glove_vectors(glove_vectors_path)

            saved_paths['glove'] = glove_path
            saved_paths['glove_vectors'] = glove_vectors_path
            print(f"üíæ GloVe —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {glove_path}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata_path = f"{base_path}/training_metadata_{timestamp}.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(self.training_metadata, f, ensure_ascii=False, indent=2)

        saved_paths['metadata'] = metadata_path
        print(f"üíæ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")

        return saved_paths

    def _save_glove_vectors(self, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ GloVe –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
        with open(filepath, 'w', encoding='utf-8') as f:
            for word, idx in self.glove_model.dictionary.items():
                vector = self.glove_model.word_vectors[idx]
                vector_str = ' '.join([str(x) for x in vector])
                f.write(f"{word} {vector_str}\n")

    def load_models(self, base_path: str = "embeddings") -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–Ω–µ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

        Returns:
            True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞
        """
        try:
            # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ñ–∞–π–ª—ã –ø–æ —à–∞–±–ª–æ–Ω—É
            embedding_files = list(Path(base_path).glob("*_*.model"))

            for file_path in embedding_files:
                filename = file_path.stem

                if filename.startswith('word2vec'):
                    self.word2vec_model = Word2Vec.load(str(file_path))
                    print(f"üì• Word2Vec –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {file_path}")

                elif filename.startswith('fasttext'):
                    self.fasttext_model = FastText.load(str(file_path))
                    print(f"üì• FastText –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {file_path}")

                elif filename.startswith('glove'):
                    with open(file_path, 'rb') as f:
                        self.glove_model = pickle.load(f)
                    print(f"üì• GloVe –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {file_path}")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata_files = list(Path(base_path).glob("training_metadata_*.json"))
            if metadata_files:
                latest_metadata = max(metadata_files, key=lambda x: x.stat().st_mtime)
                with open(latest_metadata, 'r', encoding='utf-8') as f:
                    self.training_metadata = json.load(f)
                print(f"üì• –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {latest_metadata}")

            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            return False

    def get_model_info(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤—Å–µ—Ö –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö
        """
        info = {}

        if self.word2vec_model:
            info['word2vec'] = {
                'vocab_size': len(self.word2vec_model.wv),
                'vector_size': self.vector_size,
                'most_frequent': list(self.word2vec_model.wv.index_to_key[:10])
            }

        if self.fasttext_model:
            info['fasttext'] = {
                'vocab_size': len(self.fasttext_model.wv),
                'vector_size': self.vector_size,
                'most_frequent': list(self.fasttext_model.wv.index_to_key[:10])
            }

        if hasattr(self, 'glove_model') and self.glove_model:
            info['glove'] = {
                'vocab_size': len(self.glove_model.dictionary),
                'vector_size': self.vector_size,
                'most_frequent': list(self.glove_model.dictionary.keys())[:10]
            }

        return info

    def train_all_models(self, tokenized_texts: List[List[str]],
                         save_path: str = "embeddings") -> Dict[str, Any]:
        """
        –û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø—É—Ç—è–º–∏ –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º –º–æ–¥–µ–ª—è–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        print("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")

        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
        self.train_word2vec(tokenized_texts)
        self.train_fasttext(tokenized_texts)
        self.train_glove(tokenized_texts)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        saved_paths = self.save_models(save_path)

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö
        model_info = self.get_model_info()

        results = {
            'saved_paths': saved_paths,
            'model_info': model_info,
            'training_metadata': self.training_metadata
        }

        print("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")
        return results


def main():
    # tokenized_texts = [
    #     ["–º–∞—à–∏–Ω–Ω–æ–µ", "–æ–±—É—á–µ–Ω–∏–µ", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π", "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"],
    #     ["–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ", "—Å–µ—Ç–∏", "–≥–ª—É–±–æ–∫–æ–µ", "–æ–±—É—á–µ–Ω–∏–µ"],
    #     ["—Ç–∞—Ç–∞—Ä—Å–∫–∏–π", "—è–∑—ã–∫", "–ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∞", "–æ–±—Ä–∞–±–æ—Ç–∫–∞", "—Ç–µ–∫—Å—Ç–∞"],
    #     ["–º–∞—à–∏–Ω–Ω–æ–µ", "–æ–±—É—á–µ–Ω–∏–µ", "–∞–ª–≥–æ—Ä–∏—Ç–º", "–¥–∞–Ω–Ω—ã–µ"],
    #     ["—Ç–∞—Ç–∞—Ä—Å–∫–∏–π", "–∫—É–ª—å—Ç—É—Ä–∞", "—Ç—Ä–∞–¥–∏—Ü–∏—è", "—è–∑—ã–∫"]
    # ]

    tokenized_texts = load_tokenize_ds(size=600)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ
    embedding_trainer = EmbeddingModels(
        vector_size=100,
        window=5,
        min_count=1,
        epochs=10
    )

    # –û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    results = embedding_trainer.train_all_models(tokenized_texts)

    # –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    print("\nüìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ú–û–î–ï–õ–Ø–•:")
    for model_name, info in results['model_info'].items():
        print(f"\n{model_name.upper()}:")
        print(f"  –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {info['vocab_size']}")
        print(f"  –†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞: {info['vector_size']}")
        print(f"  –ß–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞: {info['most_frequent']}")

    print(f"\nüíæ –ü—É—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è:")
    for file_type, path in results['saved_paths'].items():
        print(f"  {file_type}: {path}")

    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤
    if embedding_trainer.word2vec_model:
        print(f"\nüîç –ü—Ä–∏–º–µ—Ä Word2Vec:")
        try:
            similar = embedding_trainer.word2vec_model.wv.most_similar("–º–∞—à–∏–Ω–Ω–æ–µ", topn=3)
            for word, score in similar:
                print(f"  {word}: {score:.3f}")
        except KeyError:
            print("  –°–ª–æ–≤–æ '–º–∞—à–∏–Ω–Ω–æ–µ' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Å–ª–æ–≤–∞—Ä–µ")


if __name__ == "__main__":
    main()