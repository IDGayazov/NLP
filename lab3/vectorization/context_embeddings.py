import torch
from transformers import AutoTokenizer, AutoModel
import numpy as np
from typing import List, Dict, Any, Union
from scipy.spatial.distance import cosine


class ContextualEmbeddings:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π CUDA –æ—à–∏–±–æ–∫
    """

    def __init__(self, model_name: str = "cointegrated/rubert-tiny2", force_cpu: bool = True):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.force_cpu = force_cpu

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        self.device = self._get_device()

        self.load_model(model_name)

    def _get_device(self):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        if self.force_cpu:
            print("üîß –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU")
            return torch.device("cpu")

        try:
            if torch.cuda.is_available():
                test_tensor = torch.tensor([1.0]).cuda()
                del test_tensor
                torch.cuda.empty_cache()
                print("‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç")
                return torch.device("cuda")
            else:
                print("‚ö†Ô∏è CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
                return torch.device("cpu")

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ CUDA: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
            return torch.device("cpu")

    def load_model(self, model_name: str) -> None:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        """
        print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
            self.model = AutoModel.from_pretrained(model_name)
            self.model.to(self.device)
            self.model.eval()

            print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {self.device}")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            try:
                print("üîÑ –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞ CPU...")
                self.model = AutoModel.from_pretrained(model_name)
                self.model.to(torch.device("cpu"))
                self.model.eval()
                self.device = torch.device("cpu")
                print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ CPU")
            except Exception as e2:
                print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e2}")

    def get_embeddings(self, texts: Union[str, List[str]],
                       pooling: str = "mean",
                       layers: Union[int, List[int]] = -1,
                       max_length: int = 512) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤
        """
        if self.model is None or self.tokenizer is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

        if isinstance(texts, str):
            texts = [texts]

        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            encoded = self.tokenizer(
                texts,
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors="pt"
            )

            # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            encoded = {k: v.to(self.device) for k, v in encoded.items()}

            # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            with torch.no_grad():
                outputs = self.model(**encoded, output_hidden_states=True)

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            hidden_states = outputs.hidden_states

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å–ª–æ–∏
            available_layers = len(hidden_states)
            print(f"üìä –î–æ—Å—Ç—É–ø–Ω–æ —Å–ª–æ–µ–≤: {available_layers}")

            if isinstance(layers, int):
                layers = [layers]

            embeddings = {}
            for layer in layers:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –≤ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ
                if layer < 0:
                    layer_idx = available_layers + layer
                else:
                    layer_idx = layer

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–ª–æ–π —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                if layer_idx < 0 or layer_idx >= available_layers:
                    print(f"‚ö†Ô∏è –°–ª–æ–π {layer} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π.")
                    layer_idx = available_layers - 1

                layer_output = hidden_states[layer_idx]

                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—É–ª–∏–Ω–≥
                if pooling == "mean":
                    attention_mask = encoded['attention_mask']
                    input_mask_expanded = attention_mask.unsqueeze(-1).expand(layer_output.size()).float()
                    sum_embeddings = torch.sum(layer_output * input_mask_expanded, 1)
                    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
                    layer_embeddings = sum_embeddings / sum_mask

                elif pooling == "max":
                    attention_mask = encoded['attention_mask']
                    input_mask_expanded = attention_mask.unsqueeze(-1).expand(layer_output.size()).float()
                    layer_output[input_mask_expanded == 0] = -1e9
                    layer_embeddings = torch.max(layer_output, 1)[0]

                elif pooling == "cls":
                    layer_embeddings = layer_output[:, 0, :]

                elif pooling == "pooler" and hasattr(outputs, 'pooler_output'):
                    layer_embeddings = outputs.pooler_output
                else:
                    raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –ø—É–ª–∏–Ω–≥–∞: {pooling}")

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–Ω—è—Ç–Ω—ã–µ –∫–ª—é—á–∏
                embeddings[f"layer_{layer_idx}"] = layer_embeddings.cpu().numpy()
                embeddings["last_layer"] = layer_embeddings.cpu().numpy()  # –ü—Å–µ–≤–¥–æ–Ω–∏–º –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è

            return {
                'embeddings': embeddings,
                'tokens': [self.tokenizer.convert_ids_to_tokens(ids) for ids in encoded['input_ids']],
                'pooling': pooling,
                'layers': layers,
                'available_layers': available_layers
            }

        except RuntimeError as e:
            if "CUDA" in str(e):
                print("‚ö†Ô∏è –û—à–∏–±–∫–∞ CUDA, –ø—Ä–æ–±—É–µ–º –Ω–∞ CPU...")
                self.device = torch.device("cpu")
                self.model.to(self.device)
                return self.get_embeddings(texts, pooling, layers, max_length)
            else:
                raise e


class SafeRussianBERTEmbeddings(ContextualEmbeddings):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""

    def __init__(self, model_size: str = "tiny"):
        """
        Args:
            model_size: 'tiny', 'base', 'large'
        """
        models = {
            "tiny": "cointegrated/rubert-tiny2",
            "base": "DeepPavlov/rubert-base-cased",
            "large": "sberbank-ai/ruBert-large"
        }

        if model_size not in models:
            raise ValueError(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã: {list(models.keys())}")

        # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        super().__init__(models[model_size], force_cpu=True)

    def get_sentence_similarity(self, text1: str, text2: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–≤—É—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        emb1 = self.get_embeddings(text1, pooling="mean")
        emb2 = self.get_embeddings(text2, pooling="mean")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π (–±–æ–ª–µ–µ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –¥–æ—Å—Ç—É–ø)
        vec1 = list(emb1['embeddings'].values())[0][0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π —Å–ª–æ–π
        vec2 = list(emb2['embeddings'].values())[0][0]

        # –ö–æ—Å–∏–Ω—É—Å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
        similarity = 1 - cosine(vec1, vec2)
        return similarity


def demo_safe_embeddings():
    """
    –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –Ω–∞ CPU
    """
    print("üéØ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ö–û–ù–¢–ï–ö–°–¢–ù–´–• –≠–ú–ë–ï–î–î–ò–ù–ì–û–í –ù–ê CPU")
    print("=" * 60)

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å –Ω–∞ CPU
    embedder = SafeRussianBERTEmbeddings("tiny")

    # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã
    texts = [
        "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ ‚Äî —ç—Ç–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
        "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –æ–±—Ä–∞–∑–æ–≤",
        "–°–µ–≥–æ–¥–Ω—è —Ö–æ—Ä–æ—à–∞—è –ø–æ–≥–æ–¥–∞ –≤ –ö–∞–∑–∞–Ω–∏",
        "–¢–∞—Ç–∞—Ä—Å–∫–∏–π —è–∑—ã–∫ –æ—á–µ–Ω—å –∫—Ä–∞—Å–∏–≤—ã–π"
    ]

    print("\n1. üìä –ü–û–õ–£–ß–ï–ù–ò–ï –≠–ú–ë–ï–î–î–ò–ù–ì–û–í:")
    for text in texts:
        result = embedder.get_embeddings(text, pooling="mean")

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞
        emb_key = list(result['embeddings'].keys())[0]
        emb_vector = result['embeddings'][emb_key][0]

        print(f"   '{text[:40]}...'")
        print(f"      –ö–ª—é—á: {emb_key}, –†–∞–∑–º–µ—Ä: {emb_vector.shape}, –ù–æ—Ä–º–∞: {np.linalg.norm(emb_vector):.3f}")

    print("\n2. üîç –°–†–ê–í–ù–ï–ù–ò–ï –¢–ï–ö–°–¢–û–í:")
    text_pairs = [
        ("–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"),
        ("–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–•–æ—Ä–æ—à–∞—è –ø–æ–≥–æ–¥–∞"),
        ("–ö–∞–∑–∞–Ω—å", "–¢–∞—Ç–∞—Ä—Å–∫–∏–π —è–∑—ã–∫")
    ]

    for text1, text2 in text_pairs:
        similarity = embedder.get_sentence_similarity(text1, text2)
        print(f"   '{text1}' vs '{text2}': {similarity:.3f}")

    print("\n3. üéØ –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ú–û–î–ï–õ–ò:")
    result = embedder.get_embeddings("—Ç–µ—Å—Ç", pooling="mean")
    print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ —Å–ª–æ–∏: {result['available_layers']}")
    print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {list(result['embeddings'].keys())}")

class NewsEmbeddingAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π (–±–µ–∑–æ–ø–∞—Å–Ω–∞—è –≤–µ—Ä—Å–∏—è)
    """

    def __init__(self, model_size: str = "tiny"):
        self.embedder = SafeRussianBERTEmbeddings(model_size)

    def analyze_news_batch(self, news_texts: List[str]) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –±–∞—Ç—á–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
        """
        print(f"üìä –ê–Ω–∞–ª–∏–∑ {len(news_texts)} –Ω–æ–≤–æ—Å—Ç–µ–π...")

        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        embeddings_result = self.embedder.get_embeddings(news_texts, pooling="mean")

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤
        emb_key = list(embeddings_result['embeddings'].keys())[0]
        news_vectors = embeddings_result['embeddings'][emb_key]

        # –ê–Ω–∞–ª–∏–∑ —Å—Ö–æ–∂–µ—Å—Ç–∏
        analysis = self._analyze_similarity(news_texts, news_vectors)
        analysis['embeddings'] = news_vectors

        return analysis

    def _analyze_similarity(self, texts: List[str], vectors: np.ndarray) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ–ø–∞—Ä–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤"""
        n_texts = len(texts)
        similarity_matrix = np.zeros((n_texts, n_texts))

        # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–ø–∞—Ä–Ω—ã–µ —Å—Ö–æ–∂–µ—Å—Ç–∏
        for i in range(n_texts):
            for j in range(n_texts):
                if i != j:
                    similarity_matrix[i][j] = 1 - cosine(vectors[i], vectors[j])

        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ –ø–∞—Ä—ã
        similar_pairs = []
        for i in range(n_texts):
            for j in range(i + 1, n_texts):
                similarity = similarity_matrix[i][j]
                similar_pairs.append({
                    'text1_index': i,
                    'text2_index': j,
                    'text1_preview': texts[i][:50] + "..." if len(texts[i]) > 50 else texts[i],
                    'text2_preview': texts[j][:50] + "..." if len(texts[j]) > 50 else texts[j],
                    'similarity': similarity
                })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–∂–µ—Å—Ç–∏
        similar_pairs.sort(key=lambda x: x['similarity'], reverse=True)

        return {
            'similarity_matrix': similarity_matrix,
            'top_similar_pairs': similar_pairs[:5],
            'avg_similarity': np.mean(similarity_matrix[np.triu_indices(n_texts, k=1)]),
            'min_similarity': np.min(similarity_matrix[np.triu_indices(n_texts, k=1)]),
            'max_similarity': np.max(similarity_matrix[np.triu_indices(n_texts, k=1)])
        }

    def find_similar_news(self, target_text: str, candidate_texts: List[str], top_k: int = 3) -> List[tuple]:
        """
        –ü–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Ü–µ–ª–µ–≤—É—é
        """
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ü–µ–ª–µ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        target_emb = self.embedder.get_embeddings(target_text, pooling="mean")
        target_key = list(target_emb['embeddings'].keys())[0]
        target_vector = target_emb['embeddings'][target_key][0]

        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        candidates_emb = self.embedder.get_embeddings(candidate_texts, pooling="mean")
        candidate_key = list(candidates_emb['embeddings'].keys())[0]
        candidate_vectors = candidates_emb['embeddings'][candidate_key]

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarities = []
        for i, cand_vector in enumerate(candidate_vectors):
            similarity = 1 - cosine(target_vector, cand_vector)
            similarities.append((candidate_texts[i], similarity))

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarities.sort(key=lambda x: x[1], reverse=True)

        return similarities[:top_k]


def demo_with_sample_news():
    """
    –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å –ø—Ä–∏–º–µ—Ä–æ–º –Ω–æ–≤–æ—Å—Ç–µ–π
    """
    # –ü—Ä–∏–º–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π
    sample_news = [
        "–í –ö–∞–∑–∞–Ω–∏ –æ—Ç–∫—Ä—ã–ª—Å—è –Ω–æ–≤—ã–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–∞—Ä–∫ –¥–ª—è IT-–∫–æ–º–ø–∞–Ω–∏–π",
        "–¢–∞—Ç–∞—Ä—Å—Ç–∞–Ω –∏–Ω–≤–µ—Å—Ç–∏—Ä—É–µ—Ç –≤ —Ä–∞–∑–≤–∏—Ç–∏–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞",
        "–ü–æ–≥–æ–¥–∞ –≤ –ö–∞–∑–∞–Ω–∏: –æ–∂–∏–¥–∞–µ—Ç—Å—è —Å–Ω–µ–≥ –∏ –ø–æ—Ö–æ–ª–æ–¥–∞–Ω–∏–µ",
        "–£—á–µ–Ω—ã–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤",
        "–í –¢–∞—Ç–∞—Ä—Å—Ç–∞–Ω–µ –ø—Ä–æ—Ö–æ–¥–∏—Ç —Ñ–µ—Å—Ç–∏–≤–∞–ª—å —Ç–∞—Ç–∞—Ä—Å–∫–æ–π –∫—É–ª—å—Ç—É—Ä—ã –∏ —è–∑—ã–∫–∞"
    ]

    analyzer = NewsEmbeddingAnalyzer("tiny")

    print("üì∞ –ê–ù–ê–õ–ò–ó –ù–û–í–û–°–¢–ï–ô –° –ü–û–ú–û–©–¨–Æ BERT")
    print("=" * 50)

    # –ê–Ω–∞–ª–∏–∑ –≤—Å–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
    analysis = analyzer.analyze_news_batch(sample_news)

    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–•–û–ñ–ï–°–¢–ò:")
    print(f"   –°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å: {analysis['avg_similarity']:.3f}")
    print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {analysis['min_similarity']:.3f}")
    print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {analysis['max_similarity']:.3f}")

    print(f"\nüîç –°–ê–ú–´–ï –ü–û–•–û–ñ–ò–ï –ù–û–í–û–°–¢–ò:")
    for i, pair in enumerate(analysis['top_similar_pairs'], 1):
        print(f"   {i}. –°—Ö–æ–∂–µ—Å—Ç—å: {pair['similarity']:.3f}")
        print(f"      üìù {pair['text1_preview']}")
        print(f"      üìù {pair['text2_preview']}")


if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ CPU
    demo_safe_embeddings()

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
    demo_with_sample_news()

    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
    # interactive_safe_demo()