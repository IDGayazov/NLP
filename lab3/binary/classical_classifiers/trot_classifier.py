from tpot import TPOTClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score
import numpy as np
import joblib
import time
import signal


def read_jsonl_basic(filepath):
    """
    –ß—Ç–µ–Ω–∏–µ JSONL —Ñ–∞–π–ª–∞
    """
    import json
    data = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line.strip())
            data.append(item)
    return data


class TimeoutTPOT:
    """
    TPOT —Å —Ç–∞–π–º–∞—É—Ç–æ–º —á—Ç–æ–±—ã –Ω–µ –∑–∞–≤–∏—Å–∞–ª
    """

    def __init__(self, time_minutes=1):
        self.vectorizer = TfidfVectorizer(
            max_features=500,  # –ï—â–µ –º–µ–Ω—å—à–µ —Ñ–∏—á–µ–π
            min_df=1,
            max_df=1.0
        )
        self.time_minutes = time_minutes
        self.is_trained = False
        print(f"‚è∞ TPOT —Å —Ç–∞–π–º–∞—É—Ç–æ–º: {time_minutes} –º–∏–Ω")

    def train_with_timeout(self, train_data):
        """–û–±—É—á–µ–Ω–∏–µ —Å —Ç–∞–π–º–∞—É—Ç–æ–º"""
        texts = [item['text'] for item in train_data]
        labels = [item['sentiment'] for item in train_data]

        X = self.vectorizer.fit_transform(texts).toarray()
        y = np.array(labels)

        print(f"üìä –î–∞–Ω–Ω—ã–µ: {X.shape}")

        # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–∞–π–º–∞—É—Ç–∞
        def timeout_handler(signum, frame):
            raise TimeoutError("TPOT –ø—Ä–µ–≤—ã—Å–∏–ª –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è!")

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(self.time_minutes * 60 + 10)  # +10 —Å–µ–∫—É–Ω–¥ –±—É—Ñ–µ—Ä

        try:
            print("üîÑ –ó–∞–ø—É—Å–∫–∞–µ–º TPOT...")
            self.tpot = TPOTClassifier(
                max_time_mins=self.time_minutes,
                random_state=42,
                population_size=5,  # –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∞—è –ø–æ–ø—É–ª—è—Ü–∏—è
                generations=2  # –í—Å–µ–≥–æ 2 –ø–æ–∫–æ–ª–µ–Ω–∏—è
            )
            self.tpot.fit(X, y)
            self.is_trained = True
            signal.alarm(0)  # –û—Ç–∫–ª—é—á–∞–µ–º —Ç–∞–π–º–∞—É—Ç
            print("‚úÖ TPOT —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–∏–ª—Å—è!")

        except TimeoutError:
            print("‚è∞ TPOT –ø—Ä–µ–≤—ã—Å–∏–ª –≤—Ä–µ–º—è! –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback...")
            self._fallback_training(X, y)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            self._fallback_training(X, y)

    def _fallback_training(self, X, y):
        """–†–µ–∑–µ—Ä–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –µ—Å–ª–∏ TPOT –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"""
        from sklearn.linear_model import LogisticRegression

        print("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º LogisticRegression –∫–∞–∫ fallback...")
        self.fallback_model = LogisticRegression(random_state=42)
        self.fallback_model.fit(X, y)
        self.is_trained = True
        self.use_fallback = True

        score = self.fallback_model.score(X, y)
        print(f"‚úÖ Fallback –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞. –¢–æ—á–Ω–æ—Å—Ç—å: {score:.3f}")

    def evaluate(self, test_data):
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
        texts = [item['text'] for item in test_data]
        labels = [item['sentiment'] for item in test_data]

        X = self.vectorizer.transform(texts).toarray()
        y = np.array(labels)

        if hasattr(self, 'use_fallback') and self.use_fallback:
            predictions = self.fallback_model.predict(X)
            model_type = "LogisticRegression (fallback)"
        else:
            predictions = self.tpot.predict(X)
            model_type = "TPOT"

        accuracy = accuracy_score(y, predictions)

        print(f"üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ({model_type}):")
        print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")
        print(f"   –û—Ç—á–µ—Ç:")
        print(classification_report(y, predictions))

        return accuracy


# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ LogisticRegression
class SimpleClassifier:
    """
    –ü—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –±–µ–∑ TPOT
    """

    def __init__(self):
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            min_df=1,
            max_df=1.0
        )
        from sklearn.linear_model import LogisticRegression
        self.model = LogisticRegression(random_state=42)
        print("ü§ñ –ü—Ä–æ—Å—Ç–æ–π LogisticRegression –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä")

    def train(self, train_data):
        """–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
        texts = [item['text'] for item in train_data]
        labels = [item['sentiment'] for item in train_data]

        print(f"üìö –û–±—É—á–∞–µ–º –Ω–∞ {len(texts)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")

        X = self.vectorizer.fit_transform(texts)
        y = np.array(labels)

        self.model.fit(X, y)

        train_score = self.model.score(X, y)
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –¢–æ—á–Ω–æ—Å—Ç—å: {train_score:.3f}")

    def evaluate(self, test_data):
        """–û—Ü–µ–Ω–∫–∞"""
        texts = [item['text'] for item in test_data]
        labels = [item['sentiment'] for item in test_data]

        X = self.vectorizer.transform(texts)
        y = np.array(labels)

        predictions = self.model.predict(X)
        accuracy = accuracy_score(y, predictions)

        print(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {accuracy:.3f}")
        print("\nüìà –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:")
        print(classification_report(y, predictions))

        return accuracy

    def predict_text(self, text):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        X = self.vectorizer.transform([text])
        prediction = self.model.predict(X)[0]
        probability = self.model.predict_proba(X)[0]

        return {
            'prediction': prediction,
            'confidence': f"{max(probability):.3f}",
            'probabilities': {
                f'class_{i}': f"{prob:.3f}" for i, prob in enumerate(probability)
            }
        }


# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def main():
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    train_data = read_jsonl_basic('../../util/news_sentiment_train.jsonl')
    test_data = read_jsonl_basic('../../util/news_sentiment_test.jsonl')

    print("=" * 50)
    print("üöÄ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–´ –î–õ–Ø –¢–ï–ö–°–¢–ê")
    print("=" * 50)
    print(f"üìÅ Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"üìÅ Test: {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ü—Ä–æ—Å—Ç–æ–π –∏ –±—ã—Å—Ç—Ä—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    print("\nüéØ –í–ê–†–ò–ê–ù–¢ 1: –ü–†–û–°–¢–û–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†")
    simple_model = SimpleClassifier()
    simple_model.train(train_data)
    simple_accuracy = simple_model.evaluate(test_data)

    # –í–∞—Ä–∏–∞–Ω—Ç 2: TPOT —Å —Ç–∞–π–º–∞—É—Ç–æ–º (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    use_tpot = input("\nü§î –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å TPOT? (y/n): ").lower().strip() == 'y'

    if use_tpot:
        print("\nüéØ –í–ê–†–ò–ê–ù–¢ 2: TPOT –° –¢–ê–ô–ú–ê–£–¢–û–ú")
        try:
            tpot_model = TimeoutTPOT(time_minutes=1)  # –í—Å–µ–≥–æ 1 –º–∏–Ω—É—Ç–∞
            tpot_model.train_with_timeout(train_data)
            tpot_accuracy = tpot_model.evaluate(test_data)
        except Exception as e:
            print(f"‚ùå TPOT –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
            tpot_accuracy = 0
    else:
        tpot_accuracy = 0

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
    print(f"   SimpleClassifier: {simple_accuracy:.3f}")
    if tpot_accuracy > 0:
        print(f"   TPOT: {tpot_accuracy:.3f}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å
    joblib.dump(simple_model, "models/simple_classifier.pkl")
    print("üíæ –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'simple_classifier.pkl'")


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –≤–µ—Ä—Å–∏—é
    main()