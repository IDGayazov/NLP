from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.model_selection import cross_val_predict
from catboost import CatBoostClassifier
import numpy as np
import joblib
import warnings

from util.jsonl_process import read_jsonl_basic

warnings.filterwarnings('ignore')


class StackingSentimentClassifier:
    """
    –°—Ç–µ–∫–∏–Ω–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å –∫–æ–º–±–∏–Ω–∞—Ü–∏–µ–π SVM, LR –∏ CatBoost
    """

    def __init__(self, use_blending=True, meta_model='logistic',
                 positive_label=1, negative_label=0, random_state=42):
        """
        Args:
            use_blending: True –¥–ª—è –±–ª–µ–Ω–¥–∏–Ω–≥–∞, False –¥–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞
            meta_model: —Ç–∏–ø –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ ('logistic', 'svm', 'random_forest')
            positive_label: –º–µ—Ç–∫–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
            negative_label: –º–µ—Ç–∫–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
            random_state: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        """
        self.vectorizer = TfidfVectorizer(
            max_features=8000,
            min_df=2,
            max_df=0.85,
            ngram_range=(1, 3),  # –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–∏–≥—Ä–∞–º–º—ã –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            stop_words=None
        )

        # –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (level-0)
        self.base_models = {
            'svm': LinearSVC(
                C=1.0,
                random_state=random_state,
                max_iter=2000,
                dual=True
            ),
            'logistic': LogisticRegression(
                C=1.0,
                random_state=random_state,
                max_iter=1000,
                solver='liblinear'
            ),
            'catboost': CatBoostClassifier(
                iterations=500,
                learning_rate=0.1,
                depth=6,
                random_seed=random_state,
                verbose=0,
                thread_count=-1
            )
        }

        # –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å (level-1)
        if meta_model == 'logistic':
            self.meta_model = LogisticRegression(
                C=1.0,
                random_state=random_state,
                max_iter=1000
            )
        elif meta_model == 'svm':
            self.meta_model = LinearSVC(
                C=1.0,
                random_state=random_state,
                max_iter=1000
            )
        elif meta_model == 'random_forest':
            self.meta_model = RandomForestClassifier(
                n_estimators=100,
                random_state=random_state,
                max_depth=None
            )
        else:
            raise ValueError("meta_model –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 'logistic', 'svm' –∏–ª–∏ 'random_forest'")

        self.use_blending = use_blending
        self.meta_model_type = meta_model
        self.positive_label = positive_label
        self.negative_label = negative_label
        self.is_trained = False
        self.random_state = random_state

        # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø—Ä–∏ –±–ª–µ–Ω–¥–∏–Ω–≥–µ
        self.base_predictions = {}
        self.base_probabilities = {}

    def prepare_data(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏
        """
        texts = [item['text'] for item in data]
        labels = [item['sentiment'] for item in data]
        return texts, labels

    def train_blending(self, train_data, val_data):
        """
        –û–±—É—á–µ–Ω–∏–µ —Å –±–ª–µ–Ω–¥–∏–Ω–≥–æ–º (–∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π validation set)
        """
        print("üéØ –û–ë–£–ß–ï–ù–ò–ï –° –ë–õ–ï–ù–î–ò–ù–ì–û–ú...")

        X_train, y_train = self.prepare_data(train_data)
        X_val, y_val = self.prepare_data(val_data)

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        print("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        X_train_vec = self.vectorizer.fit_transform(X_train)
        X_val_vec = self.vectorizer.transform(X_val)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_vec.shape}")
        print(f"   –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: {list(self.base_models.keys())}")
        print(f"   –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å: {self.meta_model_type}")

        # –û–±—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        base_val_predictions = []
        base_val_probabilities = []

        print("\nü§ñ –û–ë–£–ß–ï–ù–ò–ï –ë–ê–ó–û–í–´–• –ú–û–î–ï–õ–ï–ô:")
        for name, model in self.base_models.items():
            print(f"   –û–±—É—á–µ–Ω–∏–µ {name}...")

            if name == 'catboost':
                # CatBoost —Ç—Ä–µ–±—É–µ—Ç –ø–ª–æ—Ç–Ω—ã–µ –º–∞—Å—Å–∏–≤—ã
                X_train_dense = X_train_vec.toarray()
                X_val_dense = X_val_vec.toarray()
                model.fit(X_train_dense, y_train)

                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ validation set
                val_pred = model.predict(X_val_dense)
                val_prob = model.predict_proba(X_val_dense)
            else:
                model.fit(X_train_vec, y_train)
                val_pred = model.predict(X_val_vec)

                # –î–ª—è SVM –±–µ–∑ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º decision function
                if hasattr(model, 'predict_proba'):
                    val_prob = model.predict_proba(X_val_vec)
                else:
                    decision_scores = model.decision_function(X_val_vec)
                    val_prob = self._decision_to_probability(decision_scores)

            accuracy = accuracy_score(y_val, val_pred)
            print(f"      ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ val: {accuracy:.3f}")

            base_val_predictions.append(val_pred.reshape(-1, 1))
            base_val_probabilities.append(val_prob)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
            self.base_predictions[name] = val_pred
            self.base_probabilities[name] = val_prob

        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        meta_features = np.hstack(base_val_probabilities)

        print(f"\nüìä –ú–ï–¢–ê-–ü–†–ò–ó–ù–ê–ö–ò:")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {meta_features.shape}")
        print(f"   –û–±—É—á–µ–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –Ω–∞ {len(y_val)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")

        # –û–±—É—á–∞–µ–º –º–µ—Ç–∞-–º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        self.meta_model.fit(meta_features, y_val)

        # –û—Ü–µ–Ω–∫–∞ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –Ω–∞ validation set
        meta_pred = self.meta_model.predict(meta_features)
        meta_accuracy = accuracy_score(y_val, meta_pred)
        print(f"   ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –Ω–∞ val: {meta_accuracy:.3f}")

        self.is_trained = True

        return {
            'base_accuracies': {name: accuracy_score(y_val, pred)
                                for name, pred in self.base_predictions.items()},
            'meta_accuracy': meta_accuracy
        }

    def train_stacking(self, train_data, val_data=None):
        """
        –û–±—É—á–µ–Ω–∏–µ —Å–æ —Å—Ç–µ–∫–∏–Ω–≥–æ–º (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é)
        """
        print("üéØ –û–ë–£–ß–ï–ù–ò–ï –°–û –°–¢–ï–ö–ò–ù–ì–û–ú...")

        X_train, y_train = self.prepare_data(train_data)

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        print("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        X_train_vec = self.vectorizer.fit_transform(X_train)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_vec.shape}")
        print(f"   –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: {list(self.base_models.keys())}")
        print(f"   –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å: {self.meta_model_type}")

        # –î–ª—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ —É–±–µ–¥–∏–º—Å—è, —á—Ç–æ –æ–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        if self.meta_model_type == 'svm':
            # –ó–∞–º–µ–Ω—è–µ–º LinearSVC –Ω–∞ SVC —Å probability=True –¥–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞
            from sklearn.svm import SVC
            meta_model = SVC(
                C=1.0,
                random_state=self.random_state,
                probability=True,  # –í–∫–ª—é—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
                kernel='linear'
            )
        else:
            meta_model = self.meta_model

        # –°–æ–∑–¥–∞–µ–º —Å—Ç–µ–∫–∏–Ω–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        estimators = [(name, model) for name, model in self.base_models.items()]

        self.stacking_model = StackingClassifier(
            estimators=estimators,
            final_estimator=meta_model,
            cv=3,
            passthrough=False,
            n_jobs=-1
        )

        print("\nü§ñ –û–ë–£–ß–ï–ù–ò–ï –°–¢–ï–ö–ò–ù–ì –ú–û–î–ï–õ–ò...")

        # –î–ª—è CatBoost –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–ª–æ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        if any(name == 'catboost' for name in self.base_models.keys()):
            X_train_dense = X_train_vec.toarray()
            self.stacking_model.fit(X_train_dense, y_train)
        else:
            self.stacking_model.fit(X_train_vec, y_train)

        self.is_trained = True

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if any(name == 'catboost' for name in self.base_models.keys()):
            train_pred = self.stacking_model.predict(X_train_dense)
        else:
            train_pred = self.stacking_model.predict(X_train_vec)

        train_accuracy = accuracy_score(y_train, train_pred)
        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train: {train_accuracy:.3f}")

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        if val_data:
            val_accuracy = self.evaluate(val_data)
            print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ val: {val_accuracy:.3f}")

        return train_accuracy

    def train(self, train_data, val_data=None):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è
        """
        if self.use_blending:
            if val_data is None:
                raise ValueError("–î–ª—è –±–ª–µ–Ω–¥–∏–Ω–≥–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º validation set")
            return self.train_blending(train_data, val_data)
        else:
            return self.train_stacking(train_data, val_data)

    def predict(self, texts):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)

        if self.use_blending:
            return self._predict_blending(X_vec)
        else:
            return self._predict_stacking(X_vec)

    def _predict_blending(self, X_vec):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –±–ª–µ–Ω–¥–∏–Ω–≥–∞
        """
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –≤—Å–µ—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        base_probabilities = []

        for name, model in self.base_models.items():
            if name == 'catboost':
                X_dense = X_vec.toarray()
                prob = model.predict_proba(X_dense)
            else:
                if hasattr(model, 'predict_proba'):
                    prob = model.predict_proba(X_vec)
                else:
                    decision_scores = model.decision_function(X_vec)
                    prob = self._decision_to_probability(decision_scores)

            base_probabilities.append(prob)

        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏
        meta_features = np.hstack(base_probabilities)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
        predictions = self.meta_model.predict(meta_features)
        probabilities = self._get_meta_probabilities(meta_features)

        return predictions, probabilities

    def _predict_stacking(self, X_vec):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞
        """
        # –î–ª—è CatBoost –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–ª–æ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        if any(name == 'catboost' for name in self.base_models.keys()):
            X_dense = X_vec.toarray()
            predictions = self.stacking_model.predict(X_dense)
        else:
            predictions = self.stacking_model.predict(X_vec)

        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        if hasattr(self.stacking_model, 'predict_proba'):
            if any(name == 'catboost' for name in self.base_models.keys()):
                probabilities = self.stacking_model.predict_proba(X_dense)
            else:
                probabilities = self.stacking_model.predict_proba(X_vec)
        else:
            # –ï—Å–ª–∏ predict_proba –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, —Å–æ–∑–¥–∞–µ–º –ø—Å–µ–≤–¥–æ-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            if any(name == 'catboost' for name in self.base_models.keys()):
                decision_scores = self.stacking_model.decision_function(X_dense)
            else:
                decision_scores = self.stacking_model.decision_function(X_vec)
            probabilities = self._decision_to_probability(decision_scores)

        return predictions, probabilities

    def _decision_to_probability(self, decision_scores):
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ decision function –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è SVM
        """
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy array –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏–∏
        decision_scores = np.array(decision_scores)

        # –ü—Ä–æ—Å—Ç–∞—è —Å–∏–≥–º–æ–∏–¥–∞–ª—å–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è
        probabilities = 1 / (1 + np.exp(-decision_scores))
        prob_matrix = np.zeros((len(probabilities), 2))
        prob_matrix[:, 1] = probabilities
        prob_matrix[:, 0] = 1 - probabilities
        return prob_matrix

    def _get_meta_probabilities(self, meta_features):
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –æ—Ç –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
        """
        if hasattr(self.meta_model, 'predict_proba'):
            return self.meta_model.predict_proba(meta_features)
        else:
            # –î–ª—è SVM –±–µ–∑ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            decision_scores = self.meta_model.decision_function(meta_features)
            return self._decision_to_probability(decision_scores)

    def predict_single(self, text):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        predictions, probabilities = self.predict([text])
        pred = predictions[0]
        prob = probabilities[0]

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        if self.meta_model.classes_[0] == self.positive_label:
            pos_prob = prob[0]
            neg_prob = prob[1]
        else:
            pos_prob = prob[1]
            neg_prob = prob[0]

        sentiment = "POSITIVE" if pred == self.positive_label else "NEGATIVE"

        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        base_predictions = self._get_base_predictions(text)

        return {
            'prediction': pred,
            'sentiment': sentiment,
            'positive_prob': pos_prob,
            'negative_prob': neg_prob,
            'confidence': max(pos_prob, neg_prob),
            'base_predictions': base_predictions,
            'consensus': self._get_consensus(base_predictions)
        }

    def _get_base_predictions(self, text):
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Å–µ—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        X_vec = self.vectorizer.transform([text])
        base_results = {}

        if self.use_blending:
            # –î–ª—è –±–ª–µ–Ω–¥–∏–Ω–≥–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
            for name, model in self.base_models.items():
                try:
                    if name == 'catboost':
                        X_dense = X_vec.toarray()
                        pred = model.predict(X_dense)[0]
                        prob = model.predict_proba(X_dense)[0]
                    else:
                        pred = model.predict(X_vec)[0]
                        if hasattr(model, 'predict_proba'):
                            prob = model.predict_proba(X_vec)[0]
                        else:
                            # –î–ª—è SVM –±–µ–∑ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
                            decision_score = model.decision_function(X_vec)
                            # decision_score –º–æ–∂–µ—Ç –±—ã—Ç—å –º–∞—Å—Å–∏–≤–æ–º, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç
                            if isinstance(decision_score, np.ndarray) and len(decision_score) == 1:
                                decision_score = decision_score[0]
                            prob = self._decision_to_probability([decision_score])[0]

                    base_results[name] = {
                        'prediction': pred,
                        'probability': prob,
                        'sentiment': "POSITIVE" if pred == self.positive_label else "NEGATIVE"
                    }
                except Exception as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ –º–æ–¥–µ–ª–∏ {name}: {e}")
                    continue
        else:
            # –î–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞ - –ø–æ–ª—É—á–∞–µ–º –∏–∑ named_estimators_
            try:
                for name, model in self.stacking_model.named_estimators_.items():
                    try:
                        if name == 'catboost':
                            X_dense = X_vec.toarray()
                            pred = model.predict(X_dense)[0]
                            prob = model.predict_proba(X_dense)[0]
                        else:
                            pred = model.predict(X_vec)[0]
                            if hasattr(model, 'predict_proba'):
                                prob = model.predict_proba(X_vec)[0]
                            else:
                                decision_score = model.decision_function(X_vec)
                                if isinstance(decision_score, np.ndarray) and len(decision_score) == 1:
                                    decision_score = decision_score[0]
                                prob = self._decision_to_probability([decision_score])[0]

                        base_results[name] = {
                            'prediction': pred,
                            'probability': prob,
                            'sentiment': "POSITIVE" if pred == self.positive_label else "NEGATIVE"
                        }
                    except Exception as e:
                        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ –º–æ–¥–µ–ª–∏ {name}: {e}")
                        continue
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: {e}")

        return base_results

    def _get_consensus(self, base_predictions):
        """
        –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        predictions = [data['prediction'] for data in base_predictions.values()]
        positive_votes = sum(1 for p in predictions if p == self.positive_label)
        total_votes = len(predictions)

        return {
            'positive_votes': positive_votes,
            'negative_votes': total_votes - positive_votes,
            'total_votes': total_votes,
            'consensus_ratio': max(positive_votes, total_votes - positive_votes) / total_votes,
            'unanimous': positive_votes == total_votes or positive_votes == 0
        }

    def evaluate(self, test_data):
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        X_test, y_test = self.prepare_data(test_data)

        predictions, probabilities = self.predict(X_test)
        accuracy = accuracy_score(y_test, predictions)

        print("\nüìä –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        target_names = [f'NEGATIVE({self.negative_label})', f'POSITIVE({self.positive_label})']
        print(classification_report(y_test, predictions, target_names=target_names))

        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        print("\nüìà –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö:")
        cm = confusion_matrix(y_test, predictions)
        print(f"               –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ {self.negative_label}  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ {self.positive_label}")
        print(f"–†–µ–∞–ª—å–Ω–æ {self.negative_label}:     {cm[0][0]:^10}        {cm[0][1]:^10}")
        print(f"–†–µ–∞–ª—å–Ω–æ {self.positive_label}:     {cm[1][0]:^10}        {cm[1][1]:^10}")

        return accuracy

    def analyze_model_performance(self, data):
        """
        –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        """
        X, y = self.prepare_data(data)
        X_vec = self.vectorizer.transform(X)

        print("\nüìä –ê–ù–ê–õ–ò–ó –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò –ú–û–î–ï–õ–ï–ô:")
        print("=" * 60)

        base_accuracies = {}

        if self.use_blending:
            # –î–ª—è –±–ª–µ–Ω–¥–∏–Ω–≥–∞ - –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã –æ—Ç–¥–µ–ª—å–Ω–æ
            for name, model in self.base_models.items():
                if name == 'catboost':
                    X_dense = X_vec.toarray()
                    pred = model.predict(X_dense)
                else:
                    pred = model.predict(X_vec)

                accuracy = accuracy_score(y, pred)
                base_accuracies[name] = accuracy
                print(f"   {name.upper():<12}: {accuracy:.3f}")
        else:
            # –î–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞ - –ø–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ estimators
            try:
                # –ü–æ–ª—É—á–∞–µ–º –æ–±—É—á–µ–Ω–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ stacking classifier
                for name, model in self.stacking_model.named_estimators_.items():
                    if name == 'catboost':
                        X_dense = X_vec.toarray()
                        pred = model.predict(X_dense)
                    else:
                        pred = model.predict(X_vec)

                    accuracy = accuracy_score(y, pred)
                    base_accuracies[name] = accuracy
                    print(f"   {name.upper():<12}: {accuracy:.3f}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: {e}")
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –æ–±—É—á–µ–Ω–∏—è
                return {'ensemble_accuracy': None, 'improvement': None}

        # –û—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è
        ensemble_pred, _ = self.predict(X)
        ensemble_accuracy = accuracy_score(y, ensemble_pred)
        print(f"   {'ENSEMBLE':<12}: {ensemble_accuracy:.3f}")

        # –£–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ª—É—á—à–µ–π –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
        if base_accuracies:
            best_base_accuracy = max(base_accuracies.values())
            improvement = ensemble_accuracy - best_base_accuracy
            print(f"\n   üìà –£–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–¥ –ª—É—á—à–µ–π –º–æ–¥–µ–ª—å—é: {improvement:.3f}")
            print(f"   üìà –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ: {improvement / best_base_accuracy * 100:.1f}%")
        else:
            improvement = 0
            print(f"\n   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å —É–ª—É—á—à–µ–Ω–∏–µ")

        return {
            'base_accuracies': base_accuracies,
            'ensemble_accuracy': ensemble_accuracy,
            'improvement': improvement
        }

    def save_model(self, filename):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        if self.use_blending:
            to_save = {
                'base_models': self.base_models,
                'meta_model': self.meta_model,
                'vectorizer': self.vectorizer,
                'use_blending': self.use_blending,
                'meta_model_type': self.meta_model_type,
                'positive_label': self.positive_label,
                'negative_label': self.negative_label
            }
        else:
            to_save = {
                'stacking_model': self.stacking_model,
                'vectorizer': self.vectorizer,
                'use_blending': self.use_blending,
                'meta_model_type': self.meta_model_type,
                'positive_label': self.positive_label,
                'negative_label': self.negative_label
            }

        joblib.dump(to_save, filename)
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")

    def load_model(self, filename):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        loaded = joblib.load(filename)

        self.vectorizer = loaded['vectorizer']
        self.use_blending = loaded['use_blending']
        self.meta_model_type = loaded['meta_model_type']
        self.positive_label = loaded.get('positive_label', 1)
        self.negative_label = loaded.get('negative_label', 0)

        if self.use_blending:
            self.base_models = loaded['base_models']
            self.meta_model = loaded['meta_model']
        else:
            self.stacking_model = loaded['stacking_model']

        self.is_trained = True
        print(f"üì• –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filename}")


# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
def compare_ensemble_strategies(train_data, val_data, test_data):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç–µ–∫–∏–Ω–≥–∞ –∏ –±–ª–µ–Ω–¥–∏–Ω–≥–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–∞-–º–æ–¥–µ–ª—è–º–∏
    """
    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –°–¢–†–ê–¢–ï–ì–ò–ô –ê–ù–°–ê–ú–ë–õ–ò–†–û–í–ê–ù–ò–Ø")
    print("=" * 60)

    strategies = [
        ('blending_logistic', True, 'logistic'),
        ('blending_svm', True, 'svm'),
        ('blending_rf', True, 'random_forest'),
        ('stacking_logistic', False, 'logistic'),
        ('stacking_svm', False, 'svm'),  # –¢–µ–ø–µ—Ä—å –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SVC —Å probability=True
    ]

    results = {}

    for name, use_blending, meta_model in strategies:
        print(f"\nüéØ {name.upper()}:")
        ensemble = StackingSentimentClassifier(
            use_blending=use_blending,
            meta_model=meta_model
        )

        if use_blending:
            ensemble.train(train_data, val_data)
        else:
            ensemble.train(train_data, val_data)

        test_accuracy = ensemble.evaluate(test_data)
        results[name] = {
            'model': ensemble,
            'accuracy': test_accuracy
        }

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüìä –ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï:")
    print("=" * 40)
    for name, result in sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True):
        print(f"   {name:<20}: {result['accuracy']:.3f}")

    return results


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å—Ç–µ–∫–∏–Ω–≥/–±–ª–µ–Ω–¥–∏–Ω–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    """
    train_data = read_jsonl_basic('../../util/news_sentiment_train.jsonl')
    val_data = read_jsonl_basic('../../util/news_sentiment_val.jsonl')
    test_data = read_jsonl_basic('../../util/news_sentiment_test.jsonl')

    print(f"üìä –î–∞–Ω–Ω—ã–µ: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test")

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–ª–µ–Ω–¥–∏–Ω–≥–∞
    print("\n" + "=" * 50)
    print("üéØ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ë–õ–ï–ù–î–ò–ù–ì–ê")
    blending_classifier = StackingSentimentClassifier(
        use_blending=True,
        meta_model='logistic'
    )

    blending_results = blending_classifier.train(train_data, val_data)

    blending_classifier.analyze_model_performance(test_data)

    print("\n" + "=" * 50)
    print("üéØ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –°–¢–ï–ö–ò–ù–ì–ê")
    stacking_classifier = StackingSentimentClassifier(
        use_blending=False,
        meta_model='logistic'
    )

    stacking_classifier.train(train_data, val_data)
    stacking_classifier.analyze_model_performance(test_data)

    blending_classifier.save_model("blending_classifier.pkl")
    stacking_classifier.save_model("stacking_classifier.pkl")

    print("\n" + "=" * 50)
    results = compare_ensemble_strategies(train_data, val_data, test_data)


if __name__ == "__main__":
    main()