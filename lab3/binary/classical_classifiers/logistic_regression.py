from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import numpy as np
import joblib

from util.jsonl_process import read_jsonl_basic


class BinarySentimentClassifier:
    """
    –ë–∏–Ω–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
    """

    def __init__(self, regularization='l2', C=1.0, positive_label=1, negative_label=0):
        """
        Args:
            regularization: 'l1' –∏–ª–∏ 'l2' —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
            C: –ø–∞—Ä–∞–º–µ—Ç—Ä —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (–º–µ–Ω—å—à–µ = —Å–∏–ª—å–Ω–µ–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
            positive_label: –º–µ—Ç–∫–∞ –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
            negative_label: –º–µ—Ç–∫–∞ –¥–ª—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
        """
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            min_df=2,
            max_df=0.8,
            ngram_range=(1, 2)
        )

        self.model = LogisticRegression(
            penalty=regularization,
            C=C,
            random_state=42,
            solver='liblinear' if regularization == 'l1' else 'lbfgs'
        )

        self.positive_label = positive_label
        self.negative_label = negative_label
        self.is_trained = False

    def prepare_data(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏
        """
        texts = [item['text'] for item in data]
        labels = [item['sentiment'] for item in data]
        return texts, labels

    def train(self, train_data, val_data=None):
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        print("üéØ –û–ë–£–ß–ï–ù–ò–ï –ë–ò–ù–ê–†–ù–û–ô –õ–û–ì–ò–°–¢–ò–ß–ï–°–ö–û–ô –†–ï–ì–†–ï–°–°–ò–ò...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train = self.prepare_data(train_data)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É –Ω–∞—Å —Ç–æ–ª—å–∫–æ 2 –∫–ª–∞—Å—Å–∞
        unique_labels = set(y_train)
        if len(unique_labels) != 2:
            print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(unique_labels)} –∫–ª–∞—Å—Å–æ–≤: {unique_labels}")
            print("   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–æ–ª—å–∫–æ –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏")

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
        print("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        X_train_vec = self.vectorizer.fit_transform(X_train)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_vec.shape}")
        print(f"   –ö–ª–∞—Å—Å—ã: {unique_labels}")
        print(f"   –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å: {self.positive_label}")
        print(f"   –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å: {self.negative_label}")

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("ü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        self.model.fit(X_train_vec, y_train)
        self.is_trained = True

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        train_pred = self.model.predict(X_train_vec)
        train_accuracy = accuracy_score(y_train, train_pred)
        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train: {train_accuracy:.3f}")

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        if val_data:
            val_accuracy = self.evaluate(val_data)
            print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ val: {val_accuracy:.3f}")

        # –ü–æ–∫–∞–∂–µ–º –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        self._show_important_features(top_n=10)

    def predict(self, texts):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)
        predictions = self.model.predict(X_vec)
        probabilities = self.model.predict_proba(X_vec)

        return predictions, probabilities

    def predict_single(self, text):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        predictions, probabilities = self.predict([text])
        pred = predictions[0]
        prob = probabilities[0]

        # –î–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —É –Ω–∞—Å —Ç–æ–ª—å–∫–æ 2 –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        if self.model.classes_[0] == self.positive_label:
            pos_prob = prob[0]
            neg_prob = prob[1]
        else:
            pos_prob = prob[1]
            neg_prob = prob[0]

        sentiment = "POSITIVE" if pred == self.positive_label else "NEGATIVE"

        return {
            'prediction': pred,
            'sentiment': sentiment,
            'positive_prob': pos_prob,
            'negative_prob': neg_prob,
            'confidence': max(pos_prob, neg_prob)
        }

    def evaluate(self, test_data):
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        X_test, y_test = self.prepare_data(test_data)
        X_test_vec = self.vectorizer.transform(X_test)

        y_pred = self.model.predict(X_test_vec)
        accuracy = accuracy_score(y_test, y_pred)

        print("\nüìä –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        target_names = [f'NEGATIVE({self.negative_label})', f'POSITIVE({self.positive_label})']
        print(classification_report(y_test, y_pred, target_names=target_names))

        print("\nüìà –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö:")
        cm = confusion_matrix(y_test, y_pred)
        print(f"               –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ {self.negative_label}  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ {self.positive_label}")
        print(f"–†–µ–∞–ª—å–Ω–æ {self.negative_label}:     {cm[0][0]:^10}        {cm[0][1]:^10}")
        print(f"–†–µ–∞–ª—å–Ω–æ {self.positive_label}:     {cm[1][0]:^10}        {cm[1][1]:^10}")

        return accuracy

    def _show_important_features(self, top_n=10):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        """
        if not hasattr(self.model, 'coef_'):
            return

        feature_names = self.vectorizer.get_feature_names_out()

        print(f"\nüîç –¢–û–ü-{top_n} –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í:")

        # –î–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —É –Ω–∞—Å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤
        coef = self.model.coef_[0]

        # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å)
        print(f"\n   –ü–û–õ–û–ñ–ò–¢–ï–õ–¨–ù–´–ï (—É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –∫–ª–∞—Å—Å {self.positive_label}):")
        pos_indices = np.argsort(coef)[-top_n:][::-1]
        for idx in pos_indices:
            print(f"      {feature_names[idx]}: {coef[idx]:.3f}")

        # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å)
        print(f"\n   –û–¢–†–ò–¶–ê–¢–ï–õ–¨–ù–´–ï (—É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –∫–ª–∞—Å—Å {self.negative_label}):")
        neg_indices = np.argsort(coef)[:top_n]
        for idx in neg_indices:
            print(f"      {feature_names[idx]}: {coef[idx]:.3f}")

    def save_model(self, filename):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        joblib.dump({
            'model': self.model,
            'vectorizer': self.vectorizer,
            'positive_label': self.positive_label,
            'negative_label': self.negative_label
        }, filename)
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")

    def load_model(self, filename):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        loaded = joblib.load(filename)
        self.model = loaded['model']
        self.vectorizer = loaded['vectorizer']
        self.positive_label = loaded.get('positive_label', 1)
        self.negative_label = loaded.get('negative_label', 0)
        self.is_trained = True
        print(f"üì• –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filename}")


# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–π
def compare_regularizations(train_data, val_data):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ L1 –∏ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–π
    """
    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï L1 vs L2 –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¶–ò–ò")
    print("=" * 50)

    # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    print("\n1. L2 –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¶–ò–Ø (Ridge):")
    l2_model = BinarySentimentClassifier(regularization='l2', C=1.0)
    l2_model.train(train_data, val_data)

    # L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Lasso)
    print("\n2. L1 –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¶–ò–Ø (Lasso):")
    l1_model = BinarySentimentClassifier(regularization='l1', C=1.0)
    l1_model.train(train_data, val_data)

    # –°—Ä–∞–≤–Ω–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–Ω—É–ª–µ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    l2_nonzero = np.sum(l2_model.model.coef_ != 0)
    l1_nonzero = np.sum(l1_model.model.coef_ != 0)

    print(f"\nüìä –°–†–ê–í–ù–ï–ù–ò–ï:")
    print(f"   L2 - –Ω–µ–Ω—É–ª–µ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {l2_nonzero}")
    print(f"   L1 - –Ω–µ–Ω—É–ª–µ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {l1_nonzero}")
    print(f"   L1 –æ—Ç–±–∏—Ä–∞–µ—Ç {l1_nonzero / l2_nonzero * 100:.1f}% –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—Ç L2")

    return l2_model, l1_model

def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–∏–Ω–∞—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    """

    train_data = read_jsonl_basic('../../util/news_sentiment_train.jsonl')
    val_data = read_jsonl_basic('../../util/news_sentiment_val.jsonl')
    test_data = read_jsonl_basic('../../util/news_sentiment_test.jsonl')

    print(f"üìä –î–∞–Ω–Ω—ã–µ: {len(train_data)} train, {len(val_data)} val")

    # 2. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Å L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
    print("\n" + "=" * 50)
    classifier = BinarySentimentClassifier(regularization='l2', C=1.0)
    classifier.train(train_data, val_data)

    # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    classifier.save_model("binary_sentiment_classifier.pkl")

    # 4. –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    print("\n" + "=" * 50)
    l2_model, l1_model = compare_regularizations(train_data, val_data)


if __name__ == "__main__":
    main()