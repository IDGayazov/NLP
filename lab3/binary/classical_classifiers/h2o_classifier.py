import h2o
from h2o.automl import H2OAutoML
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score
import numpy as np
import pandas as pd
import joblib
import warnings

warnings.filterwarnings('ignore')


def read_jsonl_basic(filepath):
    """
    –ß—Ç–µ–Ω–∏–µ JSONL —Ñ–∞–π–ª–∞
    """
    import json
    data = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line.strip())
            data.append(item)
    return data


class H2OSentimentClassifier:
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ H2O.ai AutoML
    """

    def __init__(self, max_runtime_secs=300, max_models=10, nfolds=5):
        """
        Args:
            max_runtime_secs: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã AutoML –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            max_models: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è
            nfolds: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è H2O –∫–ª–∞—Å—Ç–µ—Ä–∞
        h2o.init()

        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            min_df=2,
            max_df=0.8,
            ngram_range=(1, 2),
            stop_words=None
        )

        self.aml = H2OAutoML(
            max_runtime_secs=max_runtime_secs,
            max_models=max_models,
            nfolds=nfolds,
            seed=42,
            sort_metric="AUC"
        )

        self.max_runtime_secs = max_runtime_secs
        self.max_models = max_models
        self.is_trained = False
        self.leader_model = None

        print(f"üöÄ H2O.AI AUTOML –ò–ù–ò–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù:")
        print(f"   –í—Ä–µ–º—è: {max_runtime_secs} —Å–µ–∫")
        print(f"   –ú–∞–∫—Å. –º–æ–¥–µ–ª–µ–π: {max_models}")
        print(f"   CV —Ñ–æ–ª–¥–æ–≤: {nfolds}")
        print(f"   H2O –≤–µ—Ä—Å–∏—è: {h2o.__version__}")

    def prepare_features(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö
        """
        texts = [item['text'] for item in data]
        return texts

    def prepare_labels(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–æ–∫ –∏–∑ –¥–∞–Ω–Ω—ã—Ö
        """
        labels = [item['sentiment'] for item in data]
        return labels

    def _create_h2o_frame(self, texts, labels=None):
        """
        –°–æ–∑–¥–∞–µ—Ç H2O Frame –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ –∏ –º–µ—Ç–æ–∫
        """
        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
        X_vec = self.vectorizer.transform(texts)
        X_dense = X_vec.toarray()

        # –°–æ–∑–¥–∞–µ–º DataFrame
        feature_names = [f"feature_{i}" for i in range(X_dense.shape[1])]
        df = pd.DataFrame(X_dense, columns=feature_names)

        if labels is not None:
            df['sentiment'] = labels

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ H2O Frame
        h2o_frame = h2o.H2OFrame(df)

        if labels is not None:
            # –£–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ sentiment - –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
            h2o_frame['sentiment'] = h2o_frame['sentiment'].asfactor()

        return h2o_frame

    def train(self, train_data, val_data=None):
        """
        –û–±—É—á–µ–Ω–∏–µ H2O AutoML
        """
        print("üéØ –ê–í–¢–û–ú–ê–¢–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–û–ò–°–ö –ú–û–î–ï–õ–ï–ô –° H2O.AI...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train_texts = self.prepare_features(train_data)
        y_train = self.prepare_labels(train_data)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª–∞—Å—Å—ã
        unique_labels = set(y_train)
        print(f"üìä –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∫–ª–∞—Å—Å—ã: {unique_labels}")
        print(f"üìä –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(X_train_texts)} –ø—Ä–∏–º–µ—Ä–æ–≤")

        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–π–∑–µ—Ä –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
        print("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        self.vectorizer.fit(X_train_texts)

        # –°–æ–∑–¥–∞–µ–º H2O Frame –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        print("üìä –°–æ–∑–¥–∞–Ω–∏–µ H2O Frame...")
        train_frame = self._create_h2o_frame(X_train_texts, y_train)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {train_frame.shape}")
        print(f"   –ö–æ–ª–æ–Ω–∫–∏: {train_frame.columns}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º features –∏ target
        x = train_frame.columns[:-1]  # –í—Å–µ –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∫–æ–ª–æ–Ω–∫–∏ (target)
        y = 'sentiment'

        print(f"   Features: {len(x)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        print(f"   Target: {y}")

        # –ó–∞–ø—É—Å–∫ AutoML
        print(f"\nü§ñ –ó–ê–ü–£–°–ö H2O AUTOML...")
        print(f"   –≠—Ç–æ –∑–∞–π–º–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ {self.max_runtime_secs} —Å–µ–∫—É–Ω–¥")

        self.aml.train(x=x, y=y, training_frame=train_frame)
        self.is_trained = True

        # –ü–æ–ª—É—á–∞–µ–º –ª–∏–¥–µ—Ä-–º–æ–¥–µ–ª—å
        self.leader_model = self.aml.leader

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._show_training_summary()

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
        if val_data:
            print("\nüîç –û–¶–ï–ù–ö–ê –ù–ê –í–ê–õ–ò–î–ê–¶–ò–ò:")
            self.evaluate(val_data)

    def _show_training_summary(self):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è AutoML
        """
        print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ H2O AUTOML:")
        print("=" * 60)

        # –õ–∏–¥–µ—Ä–±–æ—Ä–¥
        lb = self.aml.leaderboard
        print("üèÜ –õ–ò–î–ï–†–ë–û–†–î –ú–û–î–ï–õ–ï–ô:")
        print(lb.head(rows=10))

        # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
        print(f"\nüéØ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨:")
        print(f"   –ê–ª–≥–æ—Ä–∏—Ç–º: {self.leader_model.algo}")
        print(f"   Model ID: {self.leader_model.model_id}")

        # –ú–µ—Ç—Ä–∏–∫–∏ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        try:
            performance = self.leader_model.model_performance()
            print(f"   AUC: {performance.auc():.4f}")
            print(f"   Logloss: {performance.logloss():.4f}")
            print(f"   Accuracy: {performance.accuracy():.4f}")
        except Exception as e:
            print(f"   –ú–µ—Ç—Ä–∏–∫–∏: {e}")

    def predict(self, data):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        texts = self.prepare_features(data)

        # –°–æ–∑–¥–∞–µ–º H2O Frame –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predict_frame = self._create_h2o_frame(texts)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = self.leader_model.predict(predict_frame)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy arrays
        pred_array = predictions['predict'].as_data_frame().values.flatten()
        prob_array = predictions[['p0', 'p1']].as_data_frame().values

        return pred_array, prob_array

    def predict_single(self, text):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        # –°–æ–∑–¥–∞–µ–º H2O Frame –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        predict_frame = self._create_h2o_frame([text])

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = self.leader_model.predict(predict_frame)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        pred = prediction['predict'].as_data_frame().values[0][0]
        probabilities = prediction[['p0', 'p1']].as_data_frame().values[0]

        result = {
            'prediction': pred,
            'probabilities': {
                'class_0': f"{probabilities[0]:.3f}",
                'class_1': f"{probabilities[1]:.3f}"
            },
            'confidence': f"{max(probabilities):.3f}",
            'model_type': self.leader_model.algo
        }

        return result

    def evaluate(self, test_data):
        """
        –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        X_test_texts = self.prepare_features(test_data)
        y_test = self.prepare_labels(test_data)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions, probabilities = self.predict(test_data)

        # –¢–æ—á–Ω–æ—Å—Ç—å
        accuracy = accuracy_score(y_test, predictions)

        print(f"üìä –¢–ï–°–¢–û–í–ê–Ø –¢–û–ß–ù–û–°–¢–¨: {accuracy:.4f}")

        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        print("\nüìà –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢:")
        print(classification_report(y_test, predictions))

        return accuracy

    def get_model_info(self):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö
        """
        if not self.is_trained:
            return {"error": "–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞"}

        lb = self.aml.leaderboard.as_data_frame()
        top_models = lb.head(5)[['model_id', 'auc', 'logloss']].to_dict('records')

        return {
            "leader_model": self.leader_model.model_id,
            "leader_algorithm": self.leader_model.algo,
            "top_models": top_models,
            "total_models_trained": len(lb),
            "feature_count": len(self.vectorizer.get_feature_names_out())
        }

    def save_model(self, filename):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        if not self.is_trained:
            print("‚ùå –ù–µ–ª—å–∑—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–µ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å")
            return

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º H2O –º–æ–¥–µ–ª—å
        model_path = h2o.save_model(model=self.leader_model, path=filename, force=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–π–∑–µ—Ä –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        joblib.dump({
            'vectorizer': self.vectorizer,
            'model_path': model_path,
            'model_id': self.leader_model.model_id
        }, f"{filename}_meta.pkl")

        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")

    def load_model(self, filename):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        meta = joblib.load(f"{filename}_meta.pkl")
        self.vectorizer = meta['vectorizer']

        # –ó–∞–≥—Ä—É–∂–∞–µ–º H2O –º–æ–¥–µ–ª—å
        self.leader_model = h2o.load_model(meta['model_path'])
        self.is_trained = True

        print(f"üì• –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {meta['model_id']}")

    def __del__(self):
        """
        –ó–∞–∫—Ä—ã—Ç–∏–µ H2O –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–∞
        """
        try:
            h2o.cluster().shutdown()
        except:
            pass


# –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞
class SimpleH2OClassifier:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è H2O –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    """

    def __init__(self, time_seconds=120):
        h2o.init()

        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.aml = H2OAutoML(max_runtime_secs=time_seconds, seed=42)
        self.is_trained = False

        print(f"‚ö° SimpleH2O: {time_seconds} —Å–µ–∫")

    def train_and_test(self, train_data, test_data):
        """–û–±—É—á–∞–µ—Ç –∏ —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç –∑–∞ –æ–¥–∏–Ω —à–∞–≥"""
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        train_texts = [item['text'] for item in train_data]
        train_labels = [item['sentiment'] for item in train_data]
        test_texts = [item['text'] for item in test_data]
        test_labels = [item['sentiment'] for item in test_data]

        print(f"üìö –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(train_texts)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ H2O Frame
        self.vectorizer.fit(train_texts)

        train_features = self.vectorizer.transform(train_texts).toarray()
        test_features = self.vectorizer.transform(test_texts).toarray()

        # –°–æ–∑–¥–∞–µ–º DataFrame
        feature_names = [f"f_{i}" for i in range(train_features.shape[1])]
        train_df = pd.DataFrame(train_features, columns=feature_names)
        train_df['target'] = train_labels

        test_df = pd.DataFrame(test_features, columns=feature_names)
        test_df['target'] = test_labels

        # H2O Frames
        train_frame = h2o.H2OFrame(train_df)
        test_frame = h2o.H2OFrame(test_df)
        train_frame['target'] = train_frame['target'].asfactor()
        test_frame['target'] = test_frame['target'].asfactor()

        # AutoML
        self.aml.train(x=feature_names, y='target', training_frame=train_frame)
        self.is_trained = True

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = self.aml.leader.predict(test_frame)
        pred_array = predictions['predict'].as_data_frame().values.flatten()

        accuracy = accuracy_score(test_labels, pred_array)

        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        print(f"   –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {self.aml.leader.algo}")
        print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f}")

        return accuracy


# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def main():
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è H2O AutoML
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    train_data = read_jsonl_basic('../../util/news_sentiment_train.jsonl')
    test_data = read_jsonl_basic('../../util/news_sentiment_test.jsonl')

    print("=" * 50)
    print("üöÄ H2O.AI AUTOML –î–õ–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –¢–û–ù–ê–õ–¨–ù–û–°–¢–ò")
    print("=" * 50)
    print(f"üìÅ Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"üìÅ Test: {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ü–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è H2O
    print("\nüéØ –í–ê–†–ò–ê–ù–¢ 1: –ü–û–õ–ù–ê–Ø –í–ï–†–°–ò–Ø H2O AUTOML")
    h2o_model = H2OSentimentClassifier(max_runtime_secs=180)  # 3 –º–∏–Ω—É—Ç—ã

    try:
        h2o_model.train(train_data)
        h2o_accuracy = h2o_model.evaluate(test_data)

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö
        model_info = h2o_model.get_model_info()
        print(f"\nüìã –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ú–û–î–ï–õ–Ø–•:")
        print(f"   –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {model_info['leader_algorithm']}")
        print(f"   –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π: {model_info['total_models_trained']}")
        print(f"   –¢–æ–ø-3 –º–æ–¥–µ–ª–∏:")
        for i, model in enumerate(model_info['top_models'][:3]):
            print(f"      {i + 1}. {model['model_id']} (AUC: {model['auc']:.4f})")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ H2O: {e}")
        h2o_accuracy = 0

    # –í–∞—Ä–∏–∞–Ω—Ç 2: –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
    print("\nüéØ –í–ê–†–ò–ê–ù–¢ 2: –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø")
    simple_h2o = SimpleH2OClassifier(time_seconds=120)  # 2 –º–∏–Ω—É—Ç—ã

    try:
        simple_accuracy = simple_h2o.train_and_test(train_data, test_data)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏: {e}")
        simple_accuracy = 0

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    if h2o_accuracy > 0:
        print("\nüß™ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:")
        test_texts = [
            "–ö–æ–º–ø–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∞ —Ä–µ–∫–æ—Ä–¥–Ω—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã",
            "–°–µ—Ä—å–µ–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–æ—Å—Ç–∞–≤–∫–∞–º–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–¥—É–∫—Ü–∏–∏",
            "–°—Ç–∞–±–∏–ª—å–Ω—ã–π —Ä–æ—Å—Ç –∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã —Ä–∞–∑–≤–∏—Ç–∏—è"
        ]

        for text in test_texts:
            result = h2o_model.predict_single(text)
            print(f"üìù '{text}'")
            print(f"   ‚Üí –ö–ª–∞—Å—Å: {result['prediction']}")
            print(f"   ‚Üí –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']}")
            print(f"   ‚Üí –ú–æ–¥–µ–ª—å: {result['model_type']}")
            print()

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        h2o_model.save_model("h2o_sentiment_model")

    # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã
    h2o.cluster().shutdown()
    print("‚úÖ H2O –∫–ª–∞—Å—Ç–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")


if __name__ == "__main__":
    main()