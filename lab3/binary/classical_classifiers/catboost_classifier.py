from catboost import CatBoostClassifier, Pool
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import numpy as np
import joblib
import pandas as pd

from util.jsonl_process import read_jsonl_basic


class CatBoostSentimentClassifier:
    """
    –ë–∏–Ω–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ CatBoost
    """

    def __init__(self, iterations=1000, learning_rate=0.1, depth=6,
                 l2_leaf_reg=3, random_state=42, verbose=100,
                 positive_label=1, negative_label=0,
                 text_processing='tfidf'):
        """
        Args:
            iterations: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π
            learning_rate: —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            depth: –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤
            l2_leaf_reg: L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
            random_state: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
            verbose: –≤—ã–≤–æ–¥ –ª–æ–≥–æ–≤
            positive_label: –º–µ—Ç–∫–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
            negative_label: –º–µ—Ç–∫–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
            text_processing: 'tfidf' –∏–ª–∏ 'bow' –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
        """
        self.vectorizer = TfidfVectorizer(
            max_features=10000,
            min_df=2,
            max_df=0.9,
            ngram_range=(1, 2),
            stop_words=None
        )

        self.model = CatBoostClassifier(
            iterations=iterations,
            learning_rate=learning_rate,
            depth=depth,
            l2_leaf_reg=l2_leaf_reg,
            random_seed=random_state,
            verbose=verbose,
            loss_function='Logloss',
            eval_metric='Accuracy',
            early_stopping_rounds=50,
            use_best_model=True
        )

        self.positive_label = positive_label
        self.negative_label = negative_label
        self.text_processing = text_processing
        self.is_trained = False

    def prepare_data(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏
        """
        texts = [item['text'] for item in data]
        labels = [item['sentiment'] for item in data]
        return texts, labels

    def train(self, train_data, val_data=None):
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ CatBoost
        """
        print("üéØ –û–ë–£–ß–ï–ù–ò–ï CATBOOST...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train = self.prepare_data(train_data)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É –Ω–∞—Å —Ç–æ–ª—å–∫–æ 2 –∫–ª–∞—Å—Å–∞
        unique_labels = set(y_train)
        if len(unique_labels) != 2:
            print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(unique_labels)} –∫–ª–∞—Å—Å–æ–≤: {unique_labels}")

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
        print("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        X_train_vec = self.vectorizer.fit_transform(X_train)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–ª–æ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è CatBoost
        X_train_dense = X_train_vec.toarray()

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_dense.shape}")
        print(f"   –ö–ª–∞—Å—Å—ã: {unique_labels}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π: {self.model.get_param('iterations')}")
        print(f"   –ì–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤: {self.model.get_param('depth')}")
        print(f"   Learning rate: {self.model.get_param('learning_rate')}")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è CatBoost
        if val_data:
            X_val, y_val = self.prepare_data(val_data)
            X_val_vec = self.vectorizer.transform(X_val)
            X_val_dense = X_val_vec.toarray()

            train_pool = Pool(X_train_dense, label=y_train)
            val_pool = Pool(X_val_dense, label=y_val)

            print("ü§ñ –û–±—É—á–µ–Ω–∏–µ CatBoost —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π...")
            self.model.fit(
                train_pool,
                eval_set=val_pool,
                plot=False,
                verbose=self.model.get_param('verbose')
            )
        else:
            train_pool = Pool(X_train_dense, label=y_train)
            print("ü§ñ –û–±—É—á–µ–Ω–∏–µ CatBoost...")
            self.model.fit(train_pool)

        self.is_trained = True

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        train_pred = self.model.predict(X_train_dense)
        train_accuracy = accuracy_score(y_train, train_pred)
        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train: {train_accuracy:.3f}")

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        if val_data:
            val_pred = self.model.predict(X_val_dense)
            val_accuracy = accuracy_score(y_val, val_pred)
            print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ val: {val_accuracy:.3f}")

        # –ü–æ–∫–∞–∂–µ–º –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        self._show_important_features(top_n=20)

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        self._show_model_info()

    def predict(self, texts):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)
        X_dense = X_vec.toarray()

        predictions = self.model.predict(X_dense)
        probabilities = self.model.predict_proba(X_dense)

        return predictions, probabilities

    def predict_single(self, text):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        predictions, probabilities = self.predict([text])
        pred = predictions[0]
        prob = probabilities[0]

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        if self.model.classes_[0] == self.positive_label:
            pos_prob = prob[0]
            neg_prob = prob[1]
        else:
            pos_prob = prob[1]
            neg_prob = prob[0]

        sentiment = "POSITIVE" if pred == self.positive_label else "NEGATIVE"

        return {
            'prediction': pred,
            'sentiment': sentiment,
            'positive_prob': pos_prob,
            'negative_prob': neg_prob,
            'confidence': max(pos_prob, neg_prob)
        }

    def predict_proba(self, texts):
        """
        –¢–æ–ª—å–∫–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)
        X_dense = X_vec.toarray()

        return self.model.predict_proba(X_dense)

    def evaluate(self, test_data):
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        X_test, y_test = self.prepare_data(test_data)
        X_test_vec = self.vectorizer.transform(X_test)
        X_test_dense = X_test_vec.toarray()

        y_pred = self.model.predict(X_test_dense)
        accuracy = accuracy_score(y_test, y_pred)

        print("\nüìä –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        target_names = [f'NEGATIVE({self.negative_label})', f'POSITIVE({self.positive_label})']
        print(classification_report(y_test, y_pred, target_names=target_names))

        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        print("\nüìà –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö:")
        cm = confusion_matrix(y_test, y_pred)
        print(f"               –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ {self.negative_label}  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ {self.positive_label}")
        print(f"–†–µ–∞–ª—å–Ω–æ {self.negative_label}:     {cm[0][0]:^10}        {cm[0][1]:^10}")
        print(f"–†–µ–∞–ª—å–Ω–æ {self.positive_label}:     {cm[1][0]:^10}        {cm[1][1]:^10}")

        return accuracy

    def _show_important_features(self, top_n=20):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        """
        try:
            feature_importances = self.model.get_feature_importance()
            feature_names = self.vectorizer.get_feature_names_out()

            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç
            if len(feature_importances) != len(feature_names):
                print(
                    f"‚ö†Ô∏è  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–æ—Å—Ç–µ–π ({len(feature_importances)}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ({len(feature_names)})")
                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ n –≤–∞–∂–Ω–æ—Å—Ç–µ–π
                feature_importances = feature_importances[:len(feature_names)]

            print(f"\nüîç –¢–û–ü-{top_n} –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í (CatBoost):")

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            indices = np.argsort(feature_importances)[::-1]

            print(f"\n   –°–ê–ú–´–ï –í–ê–ñ–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò:")
            for i in range(min(top_n, len(indices))):
                idx = indices[i]
                print(f"      {feature_names[idx]}: {feature_importances[idx]:.4f}")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏
            total_importance = np.sum(feature_importances)
            top_n_importance = np.sum(feature_importances[indices[:top_n]])
            print(
                f"\n   üìä –¢–æ–ø-{top_n} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–±—ä—è—Å–Ω—è—é—Ç {top_n_importance / total_importance * 100:.1f}% –æ–±—â–µ–π –≤–∞–∂–Ω–æ—Å—Ç–∏")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")

    def _show_model_info(self):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        print(f"\nüìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û CATBOOST –ú–û–î–ï–õ–ò:")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤: {self.model.tree_count_}")

        # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
        if hasattr(self.model, 'get_evals_result'):
            try:
                evals_result = self.model.get_evals_result()
                if evals_result and 'learn' in evals_result:
                    learn_accuracy = evals_result['learn']['Accuracy'][-1]
                    print(f"   Final train accuracy: {learn_accuracy:.4f}")

                if evals_result and 'validation' in evals_result:
                    val_accuracy = evals_result['validation']['Accuracy'][-1]
                    print(f"   Final validation accuracy: {val_accuracy:.4f}")
            except:
                pass

        # Best iteration
        try:
            best_iteration = self.model.get_best_iteration()
            print(f"   Best iteration: {best_iteration}")
        except:
            pass

    def get_feature_importance_df(self, top_n=50):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame —Å –≤–∞–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        try:
            feature_importances = self.model.get_feature_importance()
            feature_names = self.vectorizer.get_feature_names_out()

            # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
            min_len = min(len(feature_importances), len(feature_names))
            feature_importances = feature_importances[:min_len]
            feature_names = feature_names[:min_len]

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            indices = np.argsort(feature_importances)[::-1]

            importance_data = []
            for i in range(min(top_n, len(indices))):
                idx = indices[i]
                importance_data.append({
                    'feature': feature_names[idx],
                    'importance': feature_importances[idx],
                    'rank': i + 1
                })

            return importance_data
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ DataFrame –≤–∞–∂–Ω–æ—Å—Ç–∏: {e}")
            return None

    def save_model(self, filename):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        joblib.dump({
            'model': self.model,
            'vectorizer': self.vectorizer,
            'positive_label': self.positive_label,
            'negative_label': self.negative_label
        }, filename)
        print(f"üíæ –ú–æ–¥–µ–ª—å CatBoost —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")

        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ native CatBoost format
        cb_filename = filename.replace('.pkl', '.cbm')
        self.model.save_model(cb_filename)
        print(f"üíæ –ú–æ–¥–µ–ª—å CatBoost —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (native): {cb_filename}")

    def load_model(self, filename):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        loaded = joblib.load(filename)
        self.model = loaded['model']
        self.vectorizer = loaded['vectorizer']
        self.positive_label = loaded.get('positive_label', 1)
        self.negative_label = loaded.get('negative_label', 0)
        self.is_trained = True
        print(f"üì• –ú–æ–¥–µ–ª—å CatBoost –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filename}")


# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ CatBoost
def compare_catboost_parameters(train_data, val_data):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ CatBoost
    """
    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –ü–ê–†–ê–ú–ï–¢–†–û–í CATBOOST")
    print("=" * 50)

    models = {}

    # 1. –†–∞–∑–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤
    for depth in [4, 6, 8]:
        print(f"\n1. CatBoost —Å depth={depth}:")
        model = CatBoostSentimentClassifier(depth=depth, iterations=500, verbose=0)
        model.train(train_data, val_data)
        models[f'CB_depth_{depth}'] = model

    # 2. –†–∞–∑–Ω—ã–π learning rate
    for lr in [0.05, 0.1, 0.2]:
        print(f"\n2. CatBoost —Å learning_rate={lr}:")
        model = CatBoostSentimentClassifier(learning_rate=lr, iterations=500, verbose=0)
        model.train(train_data, val_data)
        models[f'CB_lr_{lr}'] = model

    return models


# –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π
def analyze_catboost_features(model, top_n=30):
    """
    –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è CatBoost
    """
    importance_data = model.get_feature_importance_df(top_n=top_n)

    if importance_data:
        print(f"\nüìà –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í CATBOOST (–¢–æ–ø-{top_n}):")
        print("=" * 60)

        for i, item in enumerate(importance_data[:top_n]):
            print(f"{i + 1:2d}. {item['feature']:20s} : {item['importance']:.4f}")

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        positive_words = []
        negative_words = []
        neutral_words = []

        positive_keywords = ['—Ö–æ—Ä–æ—à', '–æ—Ç–ª–∏—á', '–ø—Ä–µ–∫—Ä–∞—Å', '–¥–æ–≤–æ–ª', '—Ä–µ–∫–æ–º–µ–Ω–¥', '–≤–µ–ª–∏–∫–æ–ª', '–∑–∞–º–µ—á–∞—Ç']
        negative_keywords = ['–ø–ª–æ—Ö', '—É–∂–∞—Å', '—Ä–∞–∑–æ—á–∞—Ä', '–Ω–µ–¥–æ–≤–æ–ª', '–ø—Ä–æ–±–ª–µ–º', '–∫–æ—à–º–∞—Ä', '–Ω–µ–∫–∞—á–µ—Å—Ç–≤']

        for item in importance_data:
            feature = item['feature']
            if any(keyword in feature for keyword in positive_keywords):
                positive_words.append(item)
            elif any(keyword in feature for keyword in negative_keywords):
                negative_words.append(item)
            else:
                neutral_words.append(item)

        print(f"\nüéØ –ü–û–õ–û–ñ–ò–¢–ï–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò:")
        for item in positive_words[:10]:
            print(f"   {item['feature']}: {item['importance']:.4f}")

        print(f"\nüéØ –û–¢–†–ò–¶–ê–¢–ï–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò:")
        for item in negative_words[:10]:
            print(f"   {item['feature']}: {item['importance']:.4f}")

        print(f"\nüéØ –ù–ï–ô–¢–†–ê–õ–¨–ù–´–ï/–°–ú–ï–®–ê–ù–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò:")
        for item in neutral_words[:10]:
            print(f"   {item['feature']}: {item['importance']:.4f}")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è CatBoost –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
    """
    train_data = read_jsonl_basic('../../util/news_sentiment_train.jsonl')
    val_data = read_jsonl_basic('../../util/news_sentiment_val.jsonl')
    test_data = read_jsonl_basic('../../util/news_sentiment_test.jsonl')

    print(f"üìä –î–∞–Ω–Ω—ã–µ: {len(train_data)} train, {len(val_data)} val")

    # –û–±—É—á–∞–µ–º CatBoost
    print("\n" + "=" * 50)
    cb_classifier = CatBoostSentimentClassifier(
        iterations=1000,
        learning_rate=0.1,
        depth=6,
        verbose=100
    )
    cb_classifier.train(train_data, val_data)

    # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    analyze_catboost_features(cb_classifier, top_n=25)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    cb_classifier.save_model("catboost_sentiment_classifier.pkl")


if __name__ == "__main__":
    main()