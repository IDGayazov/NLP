import warnings

import joblib
from scipy.stats import randint, uniform
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.model_selection import RandomizedSearchCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC

from util.jsonl_process import read_jsonl_basic

warnings.filterwarnings('ignore')


class SimpleAutoMLSentimentClassifier:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π AutoML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å RandomizedSearchCV
    """

    def __init__(self, max_training_time=300, n_iter=50,
                 positive_label=1, negative_label=0, random_state=42):
        """
        Args:
            max_training_time: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
            n_iter: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            positive_label: –º–µ—Ç–∫–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
            negative_label: –º–µ—Ç–∫–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
            random_state: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        """
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            min_df=2,
            max_df=0.8,
            ngram_range=(1, 2),
            stop_words=None
        )

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
        self.models = {
            'logistic': {
                'model': LogisticRegression(random_state=random_state),
                'params': {
                    'C': uniform(0.001, 100),
                    'penalty': ['l1', 'l2', 'elasticnet'],
                    'solver': ['liblinear', 'saga'],
                    'max_iter': [1000, 2000]
                }
            },
            'svm': {
                'model': SVC(random_state=random_state, probability=True),
                'params': {
                    'C': uniform(0.1, 10),
                    'kernel': ['linear', 'rbf', 'poly'],
                    'gamma': ['scale', 'auto'] + list(uniform(0.001, 0.1).rvs(5))
                }
            },
            'random_forest': {
                'model': RandomForestClassifier(random_state=random_state),
                'params': {
                    'n_estimators': randint(50, 300),
                    'max_depth': [None, 10, 20, 30],
                    'min_samples_split': randint(2, 20),
                    'min_samples_leaf': randint(1, 10),
                    'max_features': ['sqrt', 'log2', None]
                }
            },
            'naive_bayes': {
                'model': MultinomialNB(),
                'params': {
                    'alpha': uniform(0.001, 2.0)
                }
            },
            'gradient_boosting': {
                'model': GradientBoostingClassifier(random_state=random_state),
                'params': {
                    'n_estimators': randint(50, 200),
                    'learning_rate': uniform(0.01, 0.3),
                    'max_depth': randint(3, 10),
                    'min_samples_split': randint(2, 20)
                }
            }
        }

        self.max_training_time = max_training_time
        self.n_iter = n_iter
        self.positive_label = positive_label
        self.negative_label = negative_label
        self.random_state = random_state
        self.is_trained = False
        self.best_model = None
        self.best_model_name = None
        self.best_score = 0

        print(f"üöÄ Simple AutoML –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {max_training_time} —Å–µ–∫")
        print(f"   –ò—Ç–µ—Ä–∞—Ü–∏–π –ø–æ–∏—Å–∫–∞: {n_iter}")
        print(f"   –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {list(self.models.keys())}")

    def prepare_data(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏
        """
        texts = [item['text'] for item in data]
        labels = [item['sentiment'] for item in data]
        return texts, labels

    def train(self, train_data, val_data=None):
        """
        –û–±—É—á–µ–Ω–∏–µ AutoML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        """
        print("üéØ –ê–í–¢–û–ú–ê–¢–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–û–î–ë–û–† –ú–û–î–ï–õ–ï–ô...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train = self.prepare_data(train_data)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É –Ω–∞—Å —Ç–æ–ª—å–∫–æ 2 –∫–ª–∞—Å—Å–∞
        unique_labels = set(y_train)
        if len(unique_labels) != 2:
            print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(unique_labels)} –∫–ª–∞—Å—Å–æ–≤: {unique_labels}")

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
        print("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        X_train_vec = self.vectorizer.fit_transform(X_train)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_vec.shape}")
        print(f"   –ö–ª–∞—Å—Å—ã: {unique_labels}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(y_train)}")

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –º–æ–¥–µ–ª–µ–π
        print("\nü§ñ –ó–ê–ü–£–°–ö –°–õ–£–ß–ê–ô–ù–û–ì–û –ü–û–ò–°–ö–ê –ü–û –ú–û–î–ï–õ–Ø–ú...")

        best_models = {}

        for model_name, model_config in self.models.items():
            print(f"   üîç –ü–æ–∏—Å–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è {model_name}...")

            try:
                # –°–ª—É—á–∞–π–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
                search = RandomizedSearchCV(
                    model_config['model'],
                    model_config['params'],
                    n_iter=self.n_iter // len(self.models),  # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Ç–µ—Ä–∞—Ü–∏–∏
                    cv=3,
                    scoring='accuracy',
                    random_state=self.random_state,
                    n_jobs=-1,
                    verbose=0
                )

                search.fit(X_train_vec, y_train)

                best_models[model_name] = {
                    'model': search.best_estimator_,
                    'score': search.best_score_,
                    'params': search.best_params_
                }

                print(f"      ‚úÖ –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {search.best_score_:.3f}")

            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–ª—è {model_name}: {e}")
                continue

        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if best_models:
            self.best_model_name = max(best_models.keys(), key=lambda x: best_models[x]['score'])
            self.best_model = best_models[self.best_model_name]['model']
            self.best_score = best_models[self.best_model_name]['score']
            self.best_params = best_models[self.best_model_name]['params']

            print(f"\nüèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {self.best_model_name}")
            print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {self.best_score:.3f}")
            print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {self.best_params}")

            self.is_trained = True

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
            self._show_model_comparison(best_models)

            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
            if val_data:
                val_accuracy = self.evaluate(val_data)
                print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ val: {val_accuracy:.3f}")
        else:
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å!")

    def _show_model_comparison(self, best_models):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        print(f"\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô:")
        print("-" * 50)

        for model_name, results in sorted(best_models.items(),
                                          key=lambda x: x[1]['score'], reverse=True):
            score = results['score']
            params = results['params']
            print(f"   {model_name:<15}: {score:.3f}")

    def predict(self, texts):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)
        predictions = self.best_model.predict(X_vec)
        probabilities = self.best_model.predict_proba(X_vec)

        return predictions, probabilities

    def predict_single(self, text):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        predictions, probabilities = self.predict([text])
        pred = predictions[0]
        prob = probabilities[0]

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        if self.best_model.classes_[0] == self.positive_label:
            pos_prob = prob[0]
            neg_prob = prob[1]
        else:
            pos_prob = prob[1]
            neg_prob = prob[0]

        sentiment = "POSITIVE" if pred == self.positive_label else "NEGATIVE"

        return {
            'prediction': pred,
            'sentiment': sentiment,
            'positive_prob': pos_prob,
            'negative_prob': neg_prob,
            'confidence': max(pos_prob, neg_prob),
            'model_type': type(self.best_model).__name__,
            'model_name': self.best_model_name
        }

    def evaluate(self, test_data):
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        X_test, y_test = self.prepare_data(test_data)
        X_test_vec = self.vectorizer.transform(X_test)

        y_pred = self.best_model.predict(X_test_vec)
        accuracy = accuracy_score(y_test, y_pred)

        print("\nüìä –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        target_names = [f'NEGATIVE({self.negative_label})', f'POSITIVE({self.positive_label})']
        print(classification_report(y_test, y_pred, target_names=target_names))

        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        print("\nüìà –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö:")
        cm = confusion_matrix(y_test, y_pred)
        print(f"               –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ {self.negative_label}  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ {self.positive_label}")
        print(f"–†–µ–∞–ª—å–Ω–æ {self.negative_label}:     {cm[0][0]:^10}        {cm[0][1]:^10}")
        print(f"–†–µ–∞–ª—å–Ω–æ {self.positive_label}:     {cm[1][0]:^10}        {cm[1][1]:^10}")

        return accuracy

    def get_model_info(self):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        """
        return {
            'model_name': self.best_model_name,
            'model_type': type(self.best_model).__name__,
            'best_score': self.best_score,
            'parameters': self.best_params,
            'feature_count': len(self.vectorizer.get_feature_names_out())
        }

    def save_model(self, filename):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        joblib.dump({
            'best_model': self.best_model,
            'vectorizer': self.vectorizer,
            'best_model_name': self.best_model_name,
            'best_score': self.best_score,
            'best_params': self.best_params,
            'positive_label': self.positive_label,
            'negative_label': self.negative_label
        }, filename)
        print(f"üíæ AutoML –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")

    def load_model(self, filename):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        loaded = joblib.load(filename)
        self.best_model = loaded['best_model']
        self.vectorizer = loaded['vectorizer']
        self.best_model_name = loaded['best_model_name']
        self.best_score = loaded.get('best_score', 0)
        self.best_params = loaded.get('best_params', {})
        self.positive_label = loaded.get('positive_label', 1)
        self.negative_label = loaded.get('negative_label', 0)
        self.is_trained = True
        print(f"üì• AutoML –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filename}")


# –í–∞—Ä–∏–∞–Ω—Ç 2: –° –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TPOT (Tree-based Pipeline Optimization Tool)
try:
    from tpot import TPOTClassifier


    class TPOTSentimentClassifier:
        """
        AutoML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ TPOT
        """

        def __init__(self, generations=5, population_size=20,
                     max_time_mins=5, cv=3, random_state=42):
            self.vectorizer = TfidfVectorizer(max_features=5000)
            self.tpot = TPOTClassifier(
                generations=generations,
                population_size=population_size,
                cv=cv,
                random_state=random_state,
                verbosity=2,
                max_time_mins=max_time_mins,
                n_jobs=-1
            )
            self.is_trained = False

        def train(self, train_data, val_data=None):
            texts = [item['text'] for item in train_data]
            labels = [item['sentiment'] for item in train_data]

            X_vec = self.vectorizer.fit_transform(texts)
            self.tpot.fit(X_vec, labels)
            self.is_trained = True

        def predict(self, texts):
            X_vec = self.vectorizer.transform(texts)
            return self.tpot.predict(X_vec), self.tpot.predict_proba(X_vec)

except ImportError:
    print("TPOT –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install tpot")


# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ AutoML
def compare_automl_approaches(train_data, val_data, test_data):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö AutoML –ø–æ–¥—Ö–æ–¥–æ–≤
    """
    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï AUTOML –ü–û–î–•–û–î–û–í")
    print("=" * 50)

    approaches = {}

    # Simple AutoML
    print("\nüéØ SIMPLE AUTOML:")
    simple_automl = SimpleAutoMLSentimentClassifier(n_iter=30, max_training_time=180)
    simple_automl.train(train_data, val_data)
    test_accuracy = simple_automl.evaluate(test_data)
    approaches['simple_automl'] = {
        'classifier': simple_automl,
        'accuracy': test_accuracy
    }

    # TPOT (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
    try:
        from tpot import TPOTClassifier
        print("\nüéØ TPOT:")
        tpot_classifier = TPOTSentimentClassifier(max_time_mins=2)  # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
        tpot_classifier.train(train_data, val_data)
        tpot_accuracy = tpot_classifier.evaluate(test_data)
        approaches['tpot'] = {
            'classifier': tpot_classifier,
            'accuracy': tpot_accuracy
        }
    except ImportError:
        print("   TPOT –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")

    # –†—É—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    print("\nüéØ –†–£–ß–ù–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê (LogisticRegression):")
    from sklearn.linear_model import LogisticRegression
    manual_classifier = SimpleAutoMLSentimentClassifier(n_iter=1, max_training_time=10)
    manual_classifier.models = {
        'logistic': {
            'model': LogisticRegression(random_state=42),
            'params': {'C': [1.0], 'max_iter': [1000]}
        }
    }
    manual_classifier.train(train_data, val_data)
    manual_accuracy = manual_classifier.evaluate(test_data)
    approaches['manual'] = {
        'classifier': manual_classifier,
        'accuracy': manual_accuracy
    }

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüìä –ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï:")
    print("=" * 30)
    for name, result in sorted(approaches.items(), key=lambda x: x[1]['accuracy'], reverse=True):
        model_info = result['classifier'].get_model_info()
        print(f"   {name:<15}: {result['accuracy']:.3f} (–º–æ–¥–µ–ª—å: {model_info['model_name']})")

    return approaches

def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AutoML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    """
    train_data = read_jsonl_basic('../../util/news_sentiment_train.jsonl')
    val_data = read_jsonl_basic('../../util/news_sentiment_val.jsonl')
    test_data = read_jsonl_basic('../../util/news_sentiment_test.jsonl')

    print(f"üìä –î–∞–Ω–Ω—ã–µ: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test")

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Simple AutoML
    print("\n" + "=" * 50)
    automl_classifier = SimpleAutoMLSentimentClassifier(
        n_iter=50,
        max_training_time=300  # 5 –º–∏–Ω—É—Ç
    )

    automl_classifier.train(train_data, val_data)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    automl_classifier.save_model("simple_automl_classifier.pkl")

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤
    print("\n" + "=" * 50)
    approaches = compare_automl_approaches(train_data, val_data, test_data)


if __name__ == "__main__":
    main()