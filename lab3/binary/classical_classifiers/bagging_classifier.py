from sklearn.ensemble import BaggingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import numpy as np
import joblib

from util.jsonl_process import read_jsonl_basic


class BaggingSentimentClassifier:
    """
    –ë–∏–Ω–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ Bagging –∞–Ω—Å–∞–º–±–ª—è
    """

    def __init__(self, base_estimator='logistic', n_estimators=10,
                 max_samples=1.0, max_features=1.0, bootstrap=True,
                 bootstrap_features=False, random_state=42,
                 positive_label=1, negative_label=0):
        """
        Args:
            base_estimator: 'logistic' –∏–ª–∏ 'tree' - –±–∞–∑–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
            n_estimators: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–∑–æ–≤—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤
            max_samples: –¥–æ–ª—è/–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ samples –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
            max_features: –¥–æ–ª—è/–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ features –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
            bootstrap: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ bootstrap sampling
            bootstrap_features: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ bootstrap –¥–ª—è features
            random_state: –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
            positive_label: –º–µ—Ç–∫–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
            negative_label: –º–µ—Ç–∫–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
        """
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            min_df=2,
            max_df=0.8,
            ngram_range=(1, 2)
        )

        # –í—ã–±–æ—Ä –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        if base_estimator == 'logistic':
            base_est = LogisticRegression(
                random_state=random_state,
                max_iter=1000,
                C=1.0
            )
        elif base_estimator == 'tree':
            base_est = DecisionTreeClassifier(
                random_state=random_state,
                max_depth=None
            )
        else:
            raise ValueError("base_estimator –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 'logistic' –∏–ª–∏ 'tree'")

        self.model = BaggingClassifier(
            estimator=base_est,
            n_estimators=n_estimators,
            max_samples=max_samples,
            max_features=max_features,
            bootstrap=bootstrap,
            bootstrap_features=bootstrap_features,
            random_state=random_state,
            n_jobs=-1,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —è–¥—Ä–∞
            verbose=0
        )

        self.base_estimator = base_estimator
        self.positive_label = positive_label
        self.negative_label = negative_label
        self.is_trained = False

    def prepare_data(self, data):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏
        """
        texts = [item['text'] for item in data]
        labels = [item['sentiment'] for item in data]
        return texts, labels

    def train(self, train_data, val_data=None):
        """
        –û–±—É—á–µ–Ω–∏–µ Bagging –º–æ–¥–µ–ª–∏
        """
        print(f"üéØ –û–ë–£–ß–ï–ù–ò–ï BAGGING ({self.base_estimator.upper()})...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train = self.prepare_data(train_data)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É –Ω–∞—Å —Ç–æ–ª—å–∫–æ 2 –∫–ª–∞—Å—Å–∞
        unique_labels = set(y_train)
        if len(unique_labels) != 2:
            print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(unique_labels)} –∫–ª–∞—Å—Å–æ–≤: {unique_labels}")

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
        print("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
        X_train_vec = self.vectorizer.fit_transform(X_train)

        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_vec.shape}")
        print(f"   –ö–ª–∞—Å—Å—ã: {unique_labels}")
        print(f"   –ë–∞–∑–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º: {self.base_estimator}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π: {self.model.n_estimators}")
        print(f"   Max samples: {self.model.max_samples}")
        print(f"   Max features: {self.model.max_features}")
        print(f"   Bootstrap: {self.model.bootstrap}")
        print(f"   Bootstrap features: {self.model.bootstrap_features}")

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("ü§ñ –û–±—É—á–µ–Ω–∏–µ Bagging –∞–Ω—Å–∞–º–±–ª—è...")
        self.model.fit(X_train_vec, y_train)
        self.is_trained = True

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        train_pred = self.model.predict(X_train_vec)
        train_accuracy = accuracy_score(y_train, train_pred)
        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train: {train_accuracy:.3f}")

        # Out-of-bag –æ—Ü–µ–Ω–∫–∞ (–µ—Å–ª–∏ bootstrap=True)
        if hasattr(self.model, 'oob_score_') and self.model.oob_score:
            print(f"‚úÖ Out-of-bag score: {self.model.oob_score_:.3f}")

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        if val_data:
            val_accuracy = self.evaluate(val_data)
            print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ val: {val_accuracy:.3f}")

        # –ê–Ω–∞–ª–∏–∑ –∞–Ω—Å–∞–º–±–ª—è
        self._analyze_ensemble()

        # –ü–æ–∫–∞–∂–µ–º –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ)
        self._show_important_features(top_n=15)

    def predict(self, texts):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)

        # –î–ª—è BaggingClassifier predict_proba –¥–æ—Å—Ç—É–ø–µ–Ω
        predictions = self.model.predict(X_vec)
        probabilities = self.model.predict_proba(X_vec)

        return predictions, probabilities

    def predict_single(self, text):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        predictions, probabilities = self.predict([text])
        pred = predictions[0]
        prob = probabilities[0]

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        if self.model.classes_[0] == self.positive_label:
            pos_prob = prob[0]
            neg_prob = prob[1]
        else:
            pos_prob = prob[1]
            neg_prob = prob[0]

        sentiment = "POSITIVE" if pred == self.positive_label else "NEGATIVE"

        return {
            'prediction': pred,
            'sentiment': sentiment,
            'positive_prob': pos_prob,
            'negative_prob': neg_prob,
            'confidence': max(pos_prob, neg_prob)
        }

    def predict_with_consensus(self, texts):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–æ–Ω—Å–µ–Ω—Å—É—Å–µ –∞–Ω—Å–∞–º–±–ª—è
        """
        if not self.is_trained:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        X_vec = self.vectorizer.transform(texts)

        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Å–µ—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        all_predictions = []

        for i, estimator in enumerate(self.model.estimators_):
            try:
                # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –≤—ã–±–∏—Ä–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∏—á–∏
                if hasattr(self.model, 'estimators_features_') and self.model.estimators_features_:
                    features_idx = self.model.estimators_features_[i]
                    X_subset = X_vec[:, features_idx]
                else:
                    X_subset = X_vec

                predictions = estimator.predict(X_subset)
                all_predictions.append(predictions)
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ –º–æ–¥–µ–ª–∏ {i}: {e}")
                continue

        if not all_predictions:
            raise Exception("–ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ —Å–º–æ–≥–ª–∞ —Å–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ")

        all_predictions = np.array(all_predictions)

        # –û—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        final_predictions = self.model.predict(X_vec)
        probabilities = self.model.predict_proba(X_vec)

        results = []
        for i, text in enumerate(texts):
            # –°—á–∏—Ç–∞–µ–º –≥–æ–ª–æ—Å–∞
            votes = all_predictions[:, i]
            positive_votes = np.sum(votes == self.positive_label)
            negative_votes = np.sum(votes == self.negative_label)
            total_votes = len(votes)

            consensus_ratio = max(positive_votes, negative_votes) / total_votes

            results.append({
                'prediction': final_predictions[i],
                'probability': probabilities[i],
                'positive_votes': positive_votes,
                'negative_votes': negative_votes,
                'total_votes': total_votes,
                'consensus_ratio': consensus_ratio,
                'unanimous': consensus_ratio == 1.0
            })

        return results

    def evaluate(self, test_data):
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        X_test, y_test = self.prepare_data(test_data)
        X_test_vec = self.vectorizer.transform(X_test)

        y_pred = self.model.predict(X_test_vec)
        accuracy = accuracy_score(y_test, y_pred)

        print("\nüìä –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        target_names = [f'NEGATIVE({self.negative_label})', f'POSITIVE({self.positive_label})']
        print(classification_report(y_test, y_pred, target_names=target_names))

        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        print("\nüìà –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö:")
        cm = confusion_matrix(y_test, y_pred)
        print(f"               –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ {self.negative_label}  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ {self.positive_label}")
        print(f"–†–µ–∞–ª—å–Ω–æ {self.negative_label}:     {cm[0][0]:^10}        {cm[0][1]:^10}")
        print(f"–†–µ–∞–ª—å–Ω–æ {self.positive_label}:     {cm[1][0]:^10}        {cm[1][1]:^10}")

        return accuracy

    def _analyze_ensemble(self):
        """
        –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω—Å–∞–º–±–ª—è
        """
        print(f"\nüìä –ê–ù–ê–õ–ò–ó BAGGING –ê–ù–°–ê–ú–ë–õ–Ø:")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π: {len(self.model.estimators_)}")

        # –û—Ü–µ–Ω–∏–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∞–Ω—Å–∞–º–±–ª—è
        if hasattr(self.model, 'estimators_features_'):
            unique_features_sets = len(set(
                tuple(features) for features in self.model.estimators_features_
            ))
            print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {unique_features_sets}")

        # Out-of-bag score
        if hasattr(self.model, 'oob_score_'):
            print(f"   Out-of-bag score: {self.model.oob_score_:.3f}")

    def _show_important_features(self, top_n=15):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–¥–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏)
        """
        if self.base_estimator != 'logistic':
            print(f"\n‚ö†Ô∏è  –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ {self.base_estimator}")
            return

        try:
            # –î–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —É—Å—Ä–µ–¥–Ω—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –ø–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º
            all_coefs = []
            for estimator in self.model.estimators_:
                if hasattr(estimator, 'coef_'):
                    all_coefs.append(estimator.coef_[0])

            if not all_coefs:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–æ–¥–µ–ª–µ–π")
                return

            # –£—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
            avg_coefs = np.mean(all_coefs, axis=0)
            feature_names = self.vectorizer.get_feature_names_out()

            print(f"\nüîç –¢–û–ü-{top_n} –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í (Bagging Logistic):")

            # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            print(f"\n   –ü–û–õ–û–ñ–ò–¢–ï–õ–¨–ù–´–ï (—É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –∫–ª–∞—Å—Å {self.positive_label}):")
            pos_indices = np.argsort(avg_coefs)[-top_n:][::-1]
            for idx in pos_indices:
                print(f"      {feature_names[idx]}: {avg_coefs[idx]:.3f}")

            # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            print(f"\n   –û–¢–†–ò–¶–ê–¢–ï–õ–¨–ù–´–ï (—É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –∫–ª–∞—Å—Å {self.negative_label}):")
            neg_indices = np.argsort(avg_coefs)[:top_n]
            for idx in neg_indices:
                print(f"      {feature_names[idx]}: {avg_coefs[idx]:.3f}")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")

    def get_ensemble_diversity(self, data):
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∞–Ω—Å–∞–º–±–ª—è
        """
        X, y = self.prepare_data(data)
        X_vec = self.vectorizer.transform(X)

        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        all_predictions = []

        for i, estimator in enumerate(self.model.estimators_):
            try:
                if hasattr(self.model, 'estimators_features_') and self.model.estimators_features_:
                    features_idx = self.model.estimators_features_[i]
                    X_subset = X_vec[:, features_idx]
                else:
                    X_subset = X_vec

                predictions = estimator.predict(X_subset)
                all_predictions.append(predictions)
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ –º–æ–¥–µ–ª–∏ {i} –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è: {e}")
                continue

        if not all_predictions:
            return {'diversity_score': 0, 'average_disagreement': 0, 'n_models': 0}

        all_predictions = np.array(all_predictions)
        n_models = len(all_predictions)

        # –°—á–∏—Ç–∞–µ–º –ø–æ–ø–∞—Ä–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è
        disagreements = 0
        total_pairs = 0

        for i in range(n_models):
            for j in range(i + 1, n_models):
                disagreements += np.sum(all_predictions[i] != all_predictions[j])
                total_pairs += len(y)

        diversity_score = disagreements / total_pairs if total_pairs > 0 else 0

        return {
            'diversity_score': diversity_score,
            'average_disagreement': disagreements / (n_models * (n_models - 1) / 2) if n_models > 1 else 0,
            'n_models': n_models
        }

    def save_model(self, filename):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        joblib.dump({
            'model': self.model,
            'vectorizer': self.vectorizer,
            'base_estimator': self.base_estimator,
            'positive_label': self.positive_label,
            'negative_label': self.negative_label
        }, filename)
        print(f"üíæ Bagging –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")

    def load_model(self, filename):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        """
        loaded = joblib.load(filename)
        self.model = loaded['model']
        self.vectorizer = loaded['vectorizer']
        self.base_estimator = loaded.get('base_estimator', 'logistic')
        self.positive_label = loaded.get('positive_label', 1)
        self.negative_label = loaded.get('negative_label', 0)
        self.is_trained = True
        print(f"üì• Bagging –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filename}")


# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π Bagging
def compare_bagging_configs(train_data, val_data):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π Bagging
    """
    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ô BAGGING")
    print("=" * 50)

    models = {}

    # 1. Bagging —Å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π
    configs = [
        ('logistic', 10, 0.8, 0.8),
        ('logistic', 20, 0.8, 0.8),
        ('logistic', 10, 1.0, 0.6),
    ]

    for base_est, n_est, max_samp, max_feat in configs:
        print(f"\n1. Bagging {base_est} (n_est={n_est}, samples={max_samp}, features={max_feat}):")
        model = BaggingSentimentClassifier(
            base_estimator=base_est,
            n_estimators=n_est,
            max_samples=max_samp,
            max_features=max_feat
        )
        model.train(train_data, val_data)
        models[f'Bagging_{base_est}_{n_est}'] = model

    # 2. Bagging —Å –¥–µ—Ä–µ–≤—å—è–º–∏ —Ä–µ—à–µ–Ω–∏–π
    print(f"\n2. Bagging —Å Decision Trees:")
    model_tree = BaggingSentimentClassifier(
        base_estimator='tree',
        n_estimators=15,
        max_samples=0.7,
        max_features=0.7
    )
    model_tree.train(train_data, val_data)
    models['Bagging_tree'] = model_tree

    return models


# –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∞–Ω—Å–∞–º–±–ª—è
def analyze_ensemble_stability(model, data):
    """
    –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∞–Ω—Å–∞–º–±–ª—è
    """
    print(f"\nüìä –ê–ù–ê–õ–ò–ó –°–¢–ê–ë–ò–õ–¨–ù–û–°–¢–ò –ê–ù–°–ê–ú–ë–õ–Ø:")

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –∫–æ–Ω—Å–µ–Ω—Å—É—Å–æ–º
    results = model.predict_with_consensus([item['text'] for item in data])

    unanimous_count = sum(1 for r in results if r['unanimous'])
    high_consensus = sum(1 for r in results if r['consensus_ratio'] >= 0.8)

    print(f"   –ï–¥–∏–Ω–æ–≥–ª–∞—Å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è: {unanimous_count}/{len(results)} ({unanimous_count / len(results) * 100:.1f}%)")
    print(f"   –í—ã—Å–æ–∫–∏–π –∫–æ–Ω—Å–µ–Ω—Å—É—Å (‚â•80%): {high_consensus}/{len(results)} ({high_consensus / len(results) * 100:.1f}%)")

    # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∞–Ω—Å–∞–º–±–ª—è
    diversity = model.get_ensemble_diversity(data)
    print(f"   Score —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è: {diversity['diversity_score']:.3f}")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –Ω–µ—Å–æ–≥–ª–∞—Å–∏–µ: {diversity['average_disagreement']:.1f} –ø–∞—Ä –Ω–∞ –º–æ–¥–µ–ª—å")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Bagging –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    """
    train_data = read_jsonl_basic('../../util/news_sentiment_train.jsonl')
    val_data = read_jsonl_basic('../../util/news_sentiment_val.jsonl')
    test_data = read_jsonl_basic('../../util/news_sentiment_test.jsonl')

    print(f"üìä –î–∞–Ω–Ω—ã–µ: {len(train_data)} train, {len(val_data)} val")

    # –û–±—É—á–∞–µ–º Bagging —Å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π
    print("\n" + "=" * 50)
    bagging_classifier = BaggingSentimentClassifier(
        base_estimator='logistic',
        n_estimators=15,
        max_samples=0.8,
        max_features=0.8,
        bootstrap=True
    )
    bagging_classifier.train(train_data, val_data)

    analyze_ensemble_stability(bagging_classifier, val_data)

    bagging_classifier.save_model("bagging_sentiment_classifier.pkl")

    print("\n" + "=" * 50)
    models = compare_bagging_configs(train_data[:150], val_data[:30])


if __name__ == "__main__":
    main()