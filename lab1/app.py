import streamlit as st
import nltk
import re
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from collections import Counter
import pandas as pd
import numpy as np
from io import BytesIO
import base64
import time
import os
import html
import json
from typing import Dict, List, Any, Optional, Tuple
from bs4 import BeautifulSoup

# Subword –º–æ–¥–µ–ª–∏
from tokenizers import Tokenizer
from tokenizers.models import BPE, WordPiece, Unigram
from tokenizers.trainers import BpeTrainer, WordPieceTrainer, UnigramTrainer
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.normalizers import NFD, Lowercase, StripAccents
from tokenizers import normalizers
import sentencepiece as spm
from text_cleaner import TextCleaner
from universal_preprocessor import UniversalPreprocessor

# Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('words')

# Sample datasets with Tatar language
SAMPLE_DATASETS = {
    "english_news": {
        "name": "English News Headlines",
        "data": [
            "Breaking news: Scientists discover new species in Amazon rainforest",
            "Stock markets reach all-time high amid economic recovery",
            "Climate change conference concludes with new agreements",
            "Technology company unveils revolutionary AI system",
            "Global health organization reports progress in disease prevention"
        ]
    },
    "russian_literature": {
        "name": "Russian Literature Excerpts", 
        "data": [
            "–í —Ç–æ—Ç –¥–µ–Ω—å, –∫–æ–≥–¥–∞ —è –≤–ø–µ—Ä–≤—ã–µ —É–≤–∏–¥–µ–ª –µ—ë, —Å–æ–ª–Ω—Ü–µ —Å–≤–µ—Ç–∏–ª–æ –æ—Å–æ–±–µ–Ω–Ω–æ —è—Ä–∫–æ.",
            "–û–Ω –¥–æ–ª–≥–æ —à—ë–ª –ø–æ –ø—É—Å—Ç—ã–Ω–Ω–æ–π —É–ª–∏—Ü–µ, —Ä–∞–∑–º—ã—à–ª—è—è –æ –∂–∏–∑–Ω–∏ –∏ –µ—ë —Å–º—ã—Å–ª–µ.",
            "–¢–∏—à–∏–Ω–∞ –≤ –¥–æ–º–µ –±—ã–ª–∞ –∑–≤–µ–Ω—è—â–µ–π, –Ω–∞—Ä—É—à–∞–µ–º–æ–π –ª–∏—à—å —Ç–∏–∫–∞–Ω—å–µ–º —Å—Ç–∞—Ä—ã—Ö —á–∞—Å–æ–≤.",
            "–û–Ω–∞ –æ—Ç–∫—Ä—ã–ª–∞ –∫–Ω–∏–≥—É –∏ –ø–æ–≥—Ä—É–∑–∏–ª–∞—Å—å –≤ –º–∏—Ä, —Å–æ–∑–¥–∞–Ω–Ω—ã–π –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –∞–≤—Ç–æ—Ä–∞.",
            "–í–µ—Ç–µ—Ä –≥–Ω–∞–ª –ø–æ –Ω–µ–±—É —Ä–≤–∞–Ω—ã–µ –æ–±–ª–∞–∫–∞, –ø—Ä–µ–¥–≤–µ—â–∞—è —Å–∫–æ—Ä—ã–π –¥–æ–∂–¥—å."
        ]
    },
    "tatar_texts": {
        "name": "Tatar Language Examples",
        "data": [
            "–¢–∞—Ç–∞—Ä —Ç–µ–ª–µ - –±–æ–µ–∫ “ª”ô–º –±–∞–π —Ç–µ–ª, –∞–Ω—ã“£ —Ç–∞—Ä–∏—Ö—ã –±–∏–∫ –±–æ—Ä—ã–Ω–≥—ã.",
            "–ö–∞–∑–∞–Ω —à”ô“ª”ô—Ä–µ –ò–¥–µ–ª –±—É–µ–Ω–¥–∞ —É—Ä–Ω–∞—à–∫–∞–Ω –º–∞—Ç—É—Ä —à”ô“ª”ô—Ä.",
            "–¢–∞—Ç–∞—Ä —Ö–∞–ª–∫—ã–Ω—ã“£ –º”ô–¥”ô–Ω–∏—è—Ç–µ “ª”ô–º –≥–æ—Ä–µ—Ñ-–≥–∞–¥”ô—Ç–ª”ô—Ä–µ –±–∏–∫ –±–∞–π.",
            "”ò–¥”ô–±–∏—è—Ç –±–µ–∑–Ω–µ“£ –º–∏–ª–ª”ô—Ç–µ–±–µ–∑–Ω–µ“£ –∫“Ø“£–µ–ª–µ–Ω –∞—á–∞.",
            "–¢–µ–ª - —Ö–∞–ª—ã–∫–Ω—ã“£ —Ä—É—Ö–∏ –±–∞–π–ª—ã–≥—ã, –∞–Ω—ã —Å–∞–∫–ª–∞—Ä–≥–∞ –∫–∏—Ä”ô–∫."
        ]
    },
    "tech_articles": {
        "name": "Technology Articles",
        "data": [
            "Artificial intelligence is transforming modern healthcare with innovative solutions.",
            "Blockchain technology provides secure and transparent transaction systems.",
            "Cloud computing enables scalable and flexible infrastructure for businesses.",
            "Machine learning algorithms can predict consumer behavior with high accuracy.",
            "Cybersecurity measures are essential for protecting digital assets."
        ]
    }
}

class TextProcessor:
    def __init__(self):
        self.stopwords = {
            'en': set(nltk.corpus.stopwords.words('english')),
            'ru': set(nltk.corpus.stopwords.words('russian')),
            'tt': TextCleaner(language='tatar')._get_tatar_stopwords()
        }
    
    def tokenize(self, text, method='word', language='en'):
        """Tokenize text using different methods"""
        text = str(text)
        if method == 'word':
            if language == 'en':
                tokens = re.findall(r'\b\w+\b', text.lower())
            elif language == 'ru':
                tokens = re.findall(r'\b[–∞-—è—ë]+\b', text.lower())
            elif language == 'tt':
                # –¢–∞—Ç–∞—Ä—Å–∫–∏–π —è–∑—ã–∫: –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ + —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
                tokens = re.findall(r'\b[–∞-—è”ô”©“Ø“ó“£“ª]+\b', text.lower())
            else:
                tokens = re.findall(r'\b\w+\b', text.lower())
        elif method == 'nltk':
            tokens = nltk.word_tokenize(text.lower())
        else:
            tokens = text.split()
        
        return tokens
    
    def normalize(self, tokens, normalization='lowercase'):
        """Normalize tokens"""
        if not tokens:
            return []
            
        if normalization == 'lowercase':
            return [token.lower() for token in tokens]
        elif normalization == 'stemming':
            stemmer = nltk.stem.PorterStemmer()
            return [stemmer.stem(token) for token in tokens]
        elif normalization == 'lemmatization':
            lemmatizer = nltk.stem.WordNetLemmatizer()
            return [lemmatizer.lemmatize(token) for token in tokens]
        else:
            return tokens
    
    def remove_stopwords(self, tokens, language='en'):
        """Remove stopwords"""
        return [token for token in tokens if token not in self.stopwords.get(language, set())]

class SubwordModelComparator:
    def __init__(self, corpus: List[str]):
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –æ—á–∏—â–∞–µ–º –∫–æ—Ä–ø—É—Å
        self.corpus = [text.strip() for text in corpus if text.strip()]
        self.results = []
        
    def prepare_corpus_file(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ—Ä–ø—É—Å –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è sentencepiece"""
        if not self.corpus:
            raise ValueError("–ö–æ—Ä–ø—É—Å –ø—É—Å—Ç –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
            
        with open('temp_corpus.txt', 'w', encoding='utf-8') as f:
            for text in self.corpus:
                f.write(text + '\n')
        return 'temp_corpus.txt'

    def calculate_fragmentation(self, tokenized_texts: List[List[str]]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å–ª–æ–≤"""
        total_words = 0
        fragmented_words = 0

        for tokens in tokenized_texts:
            for token in tokens:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
                if (token.startswith('##') or '‚ñÅ' in token or 
                    (len(token) < 3 and token not in ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]'])):
                    fragmented_words += 1
                total_words += 1

        return (fragmented_words / total_words * 100) if total_words > 0 else 0

    def calculate_compression_ratio(self, original_texts: List[str], tokenized_texts: List[List[str]]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è"""
        total_original_tokens = sum(len(text.split()) for text in original_texts if text.strip())
        total_subword_tokens = sum(len(tokens) for tokens in tokenized_texts)

        return total_subword_tokens / total_original_tokens if total_original_tokens > 0 else 1

    def normalize_text(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç"""
        if not text:
            return ""
        text = text.lower().strip()
        text = re.sub(r'\s+([.,!?;:])', r'\1', text)
        text = re.sub(r'\(\s+', '(', text)
        text = re.sub(r'\s+\)', ')', text)
        text = re.sub(r'\s*-\s*', '-', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def reconstruct_text_for_model(self, tokens: List[str], model_name: str) -> str:
        """–ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏"""
        if not tokens:
            return ""

        try:
            if model_name == "Unigram_SP":
                text = ''.join(tokens).replace('‚ñÅ', ' ').strip()
            elif model_name == "WordPiece":
                if not tokens:
                    return ""
                text = tokens[0]
                for token in tokens[1:]:
                    if token.startswith('##'):
                        text += token[2:]
                    else:
                        text += ' ' + token
            elif model_name == "BPE":
                text = ' '.join(tokens).replace(' ##', '')
            elif model_name == "Unigram_HF":
                text = ' '.join(tokens).replace(' ##', '')
            else:
                text = ' '.join(tokens)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è {model_name}: {e}")
            text = ' '.join(tokens)

        return self.normalize_text(text)

    def calculate_reconstruction_accuracy(self, original_texts: List[str], reconstructed_texts: List[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"""
        correct = 0
        total = min(len(original_texts), len(reconstructed_texts))

        for i in range(total):
            orig = original_texts[i]
            rec = reconstructed_texts[i]

            if not orig.strip() or not rec.strip():
                continue

            orig_norm = self.normalize_text(orig)
            rec_norm = self.normalize_text(rec)

            if orig_norm == rec_norm:
                correct += 1

        accuracy = (correct / total * 100) if total > 0 else 0
        return accuracy

    def debug_tokenization(self, model_name: str, tokenized_texts: List[List[str]], num_examples: int = 1):
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
        with st.expander(f"üîç –ü—Ä–∏–º–µ—Ä—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ ({model_name})"):
            for i in range(min(num_examples, len(tokenized_texts))):
                if i < len(self.corpus):
                    original = self.corpus[i]
                    tokens = tokenized_texts[i]
                    reconstructed = self.reconstruct_text_for_model(tokens, model_name)

                    st.write(f"**–ü—Ä–∏–º–µ—Ä {i+1}:**")
                    st.text(f"–û—Ä–∏–≥–∏–Ω–∞–ª: {original[:100]}{'...' if len(original) > 100 else ''}")
                    st.text(f"–¢–æ–∫–µ–Ω—ã: {tokens[:15]}{'...' if len(tokens) > 15 else ''}")
                    st.text(f"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π: {reconstructed[:100]}{'...' if len(reconstructed) > 100 else ''}")
                    st.text(f"–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {self.normalize_text(original) == self.normalize_text(reconstructed)}")
                    st.write("---")

    def train_bpe(self, vocab_size: int, min_frequency: int) -> Tuple[Any, List[List[str]]]:
        """–û–±—É—á–∞–µ—Ç BPE –º–æ–¥–µ–ª—å"""
        try:
            # –°–æ–∑–¥–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
            tokenizer.pre_tokenizer = Whitespace()

            # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
            trainer = BpeTrainer(
                vocab_size=vocab_size,
                min_frequency=min_frequency,
                special_tokens=["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"],
                show_progress=False,
            )

            # –û–±—É—á–∞–µ–º –Ω–∞ –∫–æ—Ä–ø—É—Å–µ
            tokenizer.train_from_iterator(self.corpus, trainer=trainer)

            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∫–æ—Ä–ø—É—Å
            tokenized_texts = []
            for text in self.corpus:
                if text.strip():
                    encoding = tokenizer.encode(text)
                    tokens = encoding.tokens
                    tokenized_texts.append(tokens)

            return tokenizer, tokenized_texts

        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è BPE: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
            return None, [[] for _ in self.corpus]

    def train_wordpiece(self, vocab_size: int, min_frequency: int) -> Tuple[Any, List[List[str]]]:
        """–û–±—É—á–∞–µ—Ç WordPiece –º–æ–¥–µ–ª—å"""
        try:
            tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))
            tokenizer.pre_tokenizer = Whitespace()

            trainer = WordPieceTrainer(
                vocab_size=vocab_size,
                min_frequency=min_frequency,
                special_tokens=["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"],
                show_progress=False,
                continuing_subword_prefix="##"
            )

            tokenizer.train_from_iterator(self.corpus, trainer=trainer)

            tokenized_texts = []
            for text in self.corpus:
                if text.strip():
                    encoding = tokenizer.encode(text)
                    tokens = encoding.tokens
                    tokenized_texts.append(tokens)

            return tokenizer, tokenized_texts

        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è WordPiece: {e}")
            return None, [[] for _ in self.corpus]

    def train_unigram_sentencepiece(self, vocab_size: int, min_frequency: int) -> Tuple[Any, List[List[str]]]:
        """–û–±—É—á–∞–µ—Ç Unigram –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É—è sentencepiece"""
        try:
            corpus_file = self.prepare_corpus_file()
            
            # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –ø–æ–¥ —Ä–∞–∑–º–µ—Ä –∫–æ—Ä–ø—É—Å–∞
            unique_words = len(set(" ".join(self.corpus).split()))
            actual_vocab_size = min(vocab_size, unique_words * 2, 8000)
            
            if actual_vocab_size < 100:
                st.warning(f"–°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π –∫–æ—Ä–ø—É—Å –¥–ª—è SentencePiece. –¢—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª—å—à–µ —Ç–µ–∫—Å—Ç–æ–≤.")
                return None, [[] for _ in self.corpus]

            model_prefix = "unigram_temp_model"

            spm.SentencePieceTrainer.train(
                input=corpus_file,
                model_prefix=model_prefix,
                vocab_size=actual_vocab_size,
                model_type='unigram',
                character_coverage=1.0,
                pad_id=0,
                unk_id=1,
                bos_id=2,
                eos_id=3,
                pad_piece='[PAD]',
                unk_piece='[UNK]',
                split_by_whitespace=True,
                max_sentence_length=10000,
            )

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            sp = spm.SentencePieceProcessor()
            sp.load(f"{model_prefix}.model")

            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∫–æ—Ä–ø—É—Å
            tokenized_texts = []
            for text in self.corpus:
                if text.strip():
                    tokens = sp.encode_as_pieces(text)
                    tokenized_texts.append(tokens)

            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            for ext in ['.model', '.vocab']:
                if os.path.exists(f"{model_prefix}{ext}"):
                    os.remove(f"{model_prefix}{ext}")
            if os.path.exists(corpus_file):
                os.remove(corpus_file)

            return sp, tokenized_texts

        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è Unigram SentencePiece: {e}")
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            for file in ['temp_corpus.txt', 'unigram_temp_model.model', 'unigram_temp_model.vocab']:
                if os.path.exists(file):
                    try:
                        os.remove(file)
                    except:
                        pass
            return None, [[] for _ in self.corpus]

    def train_unigram_huggingface(self, vocab_size: int, min_frequency: int) -> Tuple[Any, List[List[str]]]:
        """–û–±—É—á–∞–µ—Ç Unigram –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ Hugging Face"""
        try:
            tokenizer = Tokenizer(Unigram())
            tokenizer.pre_tokenizer = Whitespace()

            trainer = UnigramTrainer(
                vocab_size=vocab_size,
                special_tokens=["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"],
                unk_token="[UNK]",
                max_piece_length=16,
            )

            tokenizer.train_from_iterator(self.corpus, trainer=trainer)

            tokenized_texts = []
            for text in self.corpus:
                if text.strip():
                    encoding = tokenizer.encode(text)
                    tokens = encoding.tokens
                    tokenized_texts.append(tokens)

            return tokenizer, tokenized_texts

        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è Unigram HF: {e}")
            return None, [[] for _ in self.corpus]

    def evaluate_model(self, model_name: str, tokenized_texts: List[List[str]],
                      processing_time: float, vocab_size: int) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏"""
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã
        valid_original = [text for text in self.corpus if text.strip()]
        valid_tokenized = [tokens for tokens in tokenized_texts if tokens]

        if not valid_tokenized or not valid_original:
            return {
                'model': model_name,
                'vocab_size': vocab_size,
                'actual_vocab_size': 0,
                'fragmentation_rate': 0,
                'compression_ratio': 1,
                'reconstruction_accuracy': 0,
                'processing_time_sec': round(processing_time, 2),
                'avg_token_length': 0,
                'status': 'failed'
            }

        try:
            # –ü—Ä–æ—Ü–µ–Ω—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            fragmentation = self.calculate_fragmentation(valid_tokenized)

            # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è
            compression_ratio = self.calculate_compression_ratio(valid_original, valid_tokenized)

            # –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
            reconstructed_texts = []
            for tokens in valid_tokenized:
                reconstructed = self.reconstruct_text_for_model(tokens, model_name)
                reconstructed_texts.append(reconstructed)

            reconstruction_accuracy = self.calculate_reconstruction_accuracy(valid_original, reconstructed_texts)

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
            all_tokens = [token for tokens in valid_tokenized for token in tokens]
            avg_token_length = np.mean([len(token) for token in all_tokens]) if all_tokens else 0
            actual_vocab_size = len(set(all_tokens))

            return {
                'model': model_name,
                'vocab_size': vocab_size,
                'actual_vocab_size': actual_vocab_size,
                'fragmentation_rate': round(fragmentation, 2),
                'compression_ratio': round(compression_ratio, 3),
                'reconstruction_accuracy': round(reconstruction_accuracy, 2),
                'processing_time_sec': round(processing_time, 2),
                'avg_token_length': round(avg_token_length, 2),
                'status': 'success'
            }
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return {
                'model': model_name,
                'vocab_size': vocab_size,
                'actual_vocab_size': 0,
                'fragmentation_rate': 0,
                'compression_ratio': 1,
                'reconstruction_accuracy': 0,
                'processing_time_sec': round(processing_time, 2),
                'avg_token_length': 0,
                'status': 'error'
            }

    def run_comparison(self, vocab_sizes: List[int] = None, min_frequency: int = 2, show_debug: bool = False) -> pd.DataFrame:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–µ–π"""
        if not self.corpus:
            st.error("–ö–æ—Ä–ø—É—Å –ø—É—Å—Ç. –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
            return pd.DataFrame()

        if vocab_sizes is None:
            vocab_sizes = [1000, 2000]

        st.write(f"üîç **–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä–ø—É—Å–∞:** {len(self.corpus)} —Ç–µ–∫—Å—Ç–æ–≤, {len(set(' '.join(self.corpus).split()))} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤")

        models_to_train = [
            ("BPE", self.train_bpe),
            ("WordPiece", self.train_wordpiece), 
            ("Unigram_HF", self.train_unigram_huggingface),
            ("Unigram_SP", self.train_unigram_sentencepiece)
        ]

        progress_bar = st.progress(0)
        total_steps = len(vocab_sizes) * len(models_to_train)
        current_step = 0

        for vocab_size in vocab_sizes:
            st.write(f"### üìä –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size}")

            for model_name, train_func in models_to_train:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º SentencePiece –¥–ª—è –±–æ–ª—å—à–∏—Ö —Å–ª–æ–≤–∞—Ä–µ–π
                if model_name == "Unigram_SP" and vocab_size > 8000:
                    current_step += 1
                    progress_bar.progress(current_step / total_steps)
                    continue

                status_text = st.empty()
                status_text.text(f"–û–±—É—á–∞–µ—Ç—Å—è {model_name}...")

                try:
                    start_time = time.time()
                    model, tokens = train_func(vocab_size, min_frequency)
                    processing_time = time.time() - start_time

                    if model is not None and tokens:
                        metrics = self.evaluate_model(model_name, tokens, processing_time, vocab_size)
                        self.results.append(metrics)
                        
                        if show_debug and metrics.get('status') == 'success':
                            self.debug_tokenization(model_name, tokens)
                            
                        status_text.success(f"{model_name} ‚úì")
                    else:
                        status_text.warning(f"{model_name} –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å")

                except Exception as e:
                    status_text.error(f"–û—à–∏–±–∫–∞ {model_name}: {str(e)}")

                current_step += 1
                progress_bar.progress(current_step / total_steps)

        progress_bar.empty()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        successful_results = [r for r in self.results if r.get('status') in ['success', None]]
        return pd.DataFrame(successful_results)

def main():
    st.set_page_config(
        page_title="Advanced Text Analysis Tool",
        page_icon="üìä",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("üìä Advanced Text Analysis Tool")
    st.markdown("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ç–µ–∫—Å—Ç —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–π—Ç–µ subword –º–æ–¥–µ–ª–∏")
    
    # Initialize processors
    processor = TextProcessor()
    text_cleaner = None
    universal_preprocessor = UniversalPreprocessor()
    
    # Sidebar for parameters
    with st.sidebar:
        st.header("Parameters")
        
        # Data source selection
        data_source = st.radio(
            "Data Source",
            ["Sample Dataset", "Custom Text", "Upload File"]
        )
        
        text_data = ""
        corpus = []
        original_text_data = ""  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
        
        if data_source == "Sample Dataset":
            dataset_choice = st.selectbox(
                "Choose dataset",
                list(SAMPLE_DATASETS.keys()),
                format_func=lambda x: SAMPLE_DATASETS[x]['name']
            )
            if dataset_choice:
                corpus = SAMPLE_DATASETS[dataset_choice]['data']
                text_data = " ".join(corpus)
                original_text_data = text_data
                with st.expander("View Sample Text"):
                    st.text(text_data)
        
        elif data_source == "Custom Text":
            text_data = st.text_area("Enter your text", height=200,
                                   placeholder="Paste your text here...")
            original_text_data = text_data
            if text_data.strip():
                corpus = [text_data]
        
        else:  # Upload File
            uploaded_file = st.file_uploader("Upload text file", type=['txt'])
            if uploaded_file is not None:
                text_data = uploaded_file.getvalue().decode("utf-8")
                original_text_data = text_data
                corpus = [text_data]
                with st.expander("View Uploaded Text"):
                    st.text(text_data[:1000] + "..." if len(text_data) > 1000 else text_data)
        
        # Text preprocessing pipeline
        st.subheader("üîÑ Text Preprocessing Pipeline")
        
        # Step 1: Universal Preprocessing
        st.markdown("**1. Universal Preprocessing**")
        enable_universal_preprocessing = st.checkbox("Enable Universal Preprocessing", value=False)
        
        if enable_universal_preprocessing:
            col1, col2 = st.columns(2)
            with col1:
                normalize_punctuation = st.checkbox("Normalize punctuation", value=True)
                normalize_whitespace = st.checkbox("Normalize whitespace", value=True)
                replace_numbers = st.checkbox("Replace numbers", value=True)
                replace_urls = st.checkbox("Replace URLs", value=True)
            with col2:
                replace_emails = st.checkbox("Replace emails", value=True)
                expand_abbreviations = st.checkbox("Expand abbreviations", value=True)
                expand_special_abbr = st.checkbox("Expand special abbreviations", value=True)
                preserve_sentences = st.checkbox("Preserve sentence endings", value=True)
        
        # Step 2: Text Cleaning
        st.markdown("**2. Text Cleaning**")
        enable_cleaning = st.checkbox("Enable Advanced Text Cleaning", value=False)
        
        if enable_cleaning:
            cleaning_language = st.selectbox("Cleaning Language", ["russian", "tatar", "english"])
            clean_html = st.checkbox("Remove HTML tags", value=True)
            remove_special_chars = st.checkbox("Remove special characters", value=True)
            cleaning_lowercase = st.checkbox("Convert to lowercase", value=True)
            cleaning_remove_stopwords = st.checkbox("Remove stopwords", value=True)
        
        # Step 3: Analysis Parameters
        st.subheader("üîç Analysis Parameters")
        language = st.selectbox("Analysis Language", ["en", "ru", "tt"])
        tokenization = st.selectbox(
            "Tokenization Method",
            ["word", "nltk", "split"]
        )
        normalization = st.selectbox(
            "Normalization",
            ["lowercase", "stemming", "lemmatization", "none"]
        )
        remove_stopwords = st.checkbox("Remove Stop Words (Analysis)")
        
        # Step 4: Advanced Analysis
        st.subheader("üî¨ Advanced Analysis")
        enable_subword_analysis = st.checkbox("Enable Subword Model Comparison")
        if enable_subword_analysis:
            st.markdown("**üß† Subword Models:**")
            col1, col2 = st.columns(2)
            with col1:
                enable_bpe = st.checkbox("BPE", value=True)
                enable_wordpiece = st.checkbox("WordPiece", value=True)
            with col2:
                enable_unigram_hf = st.checkbox("Unigram HF", value=True)
                enable_unigram_sp = st.checkbox("Unigram SP", value=True)
            
            vocab_sizes = st.multiselect(
                "Vocabulary Sizes",
                [500, 1000, 2000, 5000, 10000],
                default=[1000, 2000]
            )
            min_frequency = st.slider("Minimum Frequency", 1, 10, 2)
            show_debug_info = st.checkbox("Show Debug Information", value=False)
        
        analyze_button = st.button("Analyze Text", type="primary", use_container_width=True)
    
    # Main content area
    if analyze_button and text_data.strip():
        # Apply preprocessing pipeline
        processed_text_data = text_data
        preprocessing_steps = []
        preprocessing_info = ""
        
        # Step 1: Universal Preprocessing
        if enable_universal_preprocessing:
            with st.spinner("Applying universal preprocessing..."):
                universal_preprocessing_config = {
                    'normalize_punctuation': normalize_punctuation,
                    'normalize_whitespace': normalize_whitespace,
                    'replace_numbers': replace_numbers,
                    'replace_urls': replace_urls,
                    'replace_emails': replace_emails,
                    'expand_abbreviations': expand_abbreviations,
                    'expand_special_abbreviations': expand_special_abbr,
                    'preserve_sentence_endings': preserve_sentences
                }
                
                processed_text_data = universal_preprocessor.preprocess_text(
                    processed_text_data, **universal_preprocessing_config
                )
                preprocessing_steps.append("Universal Preprocessing")
        
        # Step 2: Text Cleaning
        if enable_cleaning:
            with st.spinner("Cleaning text..."):
                text_cleaner = TextCleaner(
                    lowercase=cleaning_lowercase,
                    remove_stopwords=cleaning_remove_stopwords,
                    language=cleaning_language
                )
                
                processed_text_data = text_cleaner.clean_text(
                    processed_text_data,
                    clean_html=clean_html,
                    remove_special_chars=remove_special_chars,
                    normalize_whitespace=True
                )
                preprocessing_steps.append("Text Cleaning")
        
        # Calculate preprocessing statistics
        original_length = len(text_data.split())
        processed_length = len(processed_text_data.split())
        removed_percentage = ((original_length - processed_length) / original_length * 100) if original_length > 0 else 0
        
        preprocessing_info = f"""
        **Preprocessing Pipeline Results:**
        - **Steps applied:** {', '.join(preprocessing_steps) if preprocessing_steps else 'None'}
        - **Original words:** {original_length}
        - **After preprocessing:** {processed_length}
        - **Removed:** {original_length - processed_length} words ({removed_percentage:.1f}%)
        """
        
        # Basic text analysis
        with st.spinner("Performing basic text analysis..."):
            tokens = processor.tokenize(processed_text_data, tokenization, language)
            tokens = processor.normalize(tokens, normalization)
            if remove_stopwords:
                tokens = processor.remove_stopwords(tokens, language)
            
            analysis = generate_analysis(tokens, processed_text_data, language)
            display_basic_results(analysis, original_text_data, processed_text_data, tokens, language, 
                               preprocessing_steps, preprocessing_info)
        
        # Subword model comparison
        if enable_subword_analysis and corpus:
            st.header("üî¨ Subword Model Comparison")
            
            # Use processed corpus as list of texts, not single string
            if isinstance(processed_text_data, str):
                analysis_corpus = [processed_text_data]
            else:
                analysis_corpus = processed_text_data
            
            # –î–æ–±–∞–≤—å—Ç–µ –ø—Ä–æ–≤–µ—Ä–∫—É —Ä–∞–∑–º–µ—Ä–∞ –∫–æ—Ä–ø—É—Å–∞
            if len(analysis_corpus) < 2:
                st.warning("""
                **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –î–ª—è –ª—É—á—à–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è subword –º–æ–¥–µ–ª–µ–π –¥–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ —Ç–µ–∫—Å—Ç–æ–≤.
                Subword –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É—é—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
                """)
            
            with st.expander("‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö"):
                st.markdown("""
                **–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:**
                - **üß© BPE (Byte Pair Encoding)** - –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —Å—É–±—Å–ª–æ–≤–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
                - **üî§ WordPiece** - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ BERT, –ø–æ—Ö–æ–∂ –Ω–∞ BPE –Ω–æ —Å –¥—Ä—É–≥–∏–º –∫—Ä–∏—Ç–µ—Ä–∏–µ–º –≤—ã–±–æ—Ä–∞  
                - **üìä Unigram HF** - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ Hugging Face
                - **üéØ Unigram SP** - —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Unigram —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Unicode —á–µ—Ä–µ–∑ SentencePiece
                
                **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é:**
                - –î–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤ (< 1000 —Å–ª–æ–≤) –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è 500-1000
                - –î–ª—è —Å—Ä–µ–¥–Ω–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤ (1000-5000 —Å–ª–æ–≤) –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ 1000-2000
                - –î–ª—è –±–æ–ª—å—à–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤ (> 5000 —Å–ª–æ–≤) –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 3000-5000
                """)
            
            comparator = SubwordModelComparator(analysis_corpus)
            
            with st.spinner("Training and comparing subword models..."):
                # –§–∏–ª—å—Ç—Ä—É–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
                selected_models = []
                if enable_bpe:
                    selected_models.append(("BPE", comparator.train_bpe))
                if enable_wordpiece:
                    selected_models.append(("WordPiece", comparator.train_wordpiece))
                if enable_unigram_hf:
                    selected_models.append(("Unigram_HF", comparator.train_unigram_huggingface))
                if enable_unigram_sp:
                    selected_models.append(("Unigram_SP", comparator.train_unigram_sentencepiece))
                
                if not selected_models:
                    st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.")
                else:
                    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–æ–¥ run_comparison –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
                    results_df = run_custom_comparison(
                        comparator, selected_models, vocab_sizes, min_frequency, show_debug_info
                    )
                    
                    if not results_df.empty:
                        display_subword_results(results_df)
                    else:
                        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")
    
    elif analyze_button and not text_data.strip():
        st.warning("Please provide some text to analyze!")
    
    else:
        show_welcome_message()

def run_custom_comparison(comparator, selected_models, vocab_sizes, min_frequency, show_debug):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    if not vocab_sizes:
        vocab_sizes = [1000, 2000]

    progress_bar = st.progress(0)
    total_steps = len(vocab_sizes) * len(selected_models)
    current_step = 0

    for vocab_size in vocab_sizes:
        st.write(f"### üìä –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size}")

        for model_name, train_func in selected_models:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º SentencePiece –¥–ª—è –±–æ–ª—å—à–∏—Ö —Å–ª–æ–≤–∞—Ä–µ–π
            if model_name == "Unigram_SP" and vocab_size > 8000:
                current_step += 1
                progress_bar.progress(current_step / total_steps)
                continue

            status_text = st.empty()
            status_text.text(f"–û–±—É—á–∞–µ—Ç—Å—è {model_name}...")

            try:
                start_time = time.time()
                model, tokens = train_func(vocab_size, min_frequency)
                processing_time = time.time() - start_time

                if model is not None and tokens:
                    metrics = comparator.evaluate_model(model_name, tokens, processing_time, vocab_size)
                    comparator.results.append(metrics)
                    
                    if show_debug and metrics.get('status') == 'success':
                        comparator.debug_tokenization(model_name, tokens)
                        
                    status_text.success(f"{model_name} ‚úì")
                else:
                    status_text.warning(f"{model_name} –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å")

            except Exception as e:
                status_text.error(f"–û—à–∏–±–∫–∞ {model_name}: {str(e)}")

            current_step += 1
            progress_bar.progress(current_step / total_steps)

    progress_bar.empty()
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    successful_results = [r for r in comparator.results if r.get('status') in ['success', None]]
    return pd.DataFrame(successful_results)

def generate_analysis(tokens, original_text, language):
    """Generate comprehensive text analysis"""
    total_tokens = len(tokens)
    unique_tokens = len(set(tokens))
    avg_token_length = sum(len(token) for token in tokens) / total_tokens if total_tokens > 0 else 0
    
    token_freq = Counter(tokens)
    
    # OOV analysis
    if language == 'en':
        try:
            common_vocab = set(nltk.corpus.words.words()[:5000])
        except:
            common_vocab = set(['the', 'and', 'is', 'in', 'to', 'of', 'a', 'for', 'on', 'with'])
    elif language == 'ru':
        common_vocab = set(['–≤', '–Ω–∞', '–∏', '—Å', '–ø–æ', '–∫', '—É', '–æ', '–Ω–µ', '—á—Ç–æ'])
    elif language == 'tt':
        # –ë–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å —Ç–∞—Ç–∞—Ä—Å–∫–∏—Ö —Å–ª–æ–≤
        common_vocab = set(['“ª”ô–º', '–≤”ô', '–±–µ–ª”ô–Ω', '”©—á–µ–Ω', '”ô–ª–µ', '–∏–Ω–¥–µ', '–±–∏–∫', '“Ø–∫', '–∫“Ø–ø', '–∞–∑',
                           '–±–∞—Ä', '—é–∫', '—Ç–µ–ª', '—Ö–∞–ª—ã–∫', '–º”ô–¥”ô–Ω–∏—è—Ç', '—Ç–∞—Ä–∏—Ö', '—à”ô“ª”ô—Ä', '–ö–∞–∑–∞–Ω'])
    else:
        common_vocab = set()
    
    oov_tokens = [token for token in tokens if token not in common_vocab]
    oov_ratio = len(oov_tokens) / total_tokens if total_tokens > 0 else 0
    
    sentences = nltk.sent_tokenize(original_text)
    avg_sentence_length = sum(len(nltk.word_tokenize(sent)) for sent in sentences) / len(sentences) if sentences else 0
    
    return {
        'statistics': {
            'total_tokens': total_tokens,
            'unique_tokens': unique_tokens,
            'avg_token_length': round(avg_token_length, 2),
            'oov_ratio': round(oov_ratio, 4),
            'vocabulary_richness': round(unique_tokens / total_tokens, 4) if total_tokens > 0 else 0,
            'sentence_count': len(sentences),
            'avg_sentence_length': round(avg_sentence_length, 2)
        },
        'token_freq': token_freq,
        'tokens': tokens,
        'sentences': sentences
    }

def display_basic_results(analysis, original_text, processed_text, tokens, language, 
                         preprocessing_steps, preprocessing_info):
    """Display basic text analysis results"""
    st.header("üìà Text Analysis Results")
    
    # Show preprocessing results
    if preprocessing_steps:
        with st.expander("üîÑ Preprocessing Pipeline Summary", expanded=True):
            st.markdown(preprocessing_info)
            
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("Original Text Sample")
                st.text_area("", original_text[:500] + "..." if len(original_text) > 500 else original_text, 
                           height=150, key="original_preview")
            with col2:
                st.subheader("Processed Text Sample")
                st.text_area("", processed_text[:500] + "..." if len(processed_text) > 500 else processed_text, 
                           height=150, key="processed_preview")
    
    # Language info
    language_names = {'en': 'English', 'ru': 'Russian', 'tt': 'Tatar'}
    st.info(f"**Analyzed language:** {language_names.get(language, language)}")
    
    # Statistics
    col1, col2, col3, col4, col5 = st.columns(5)
    stats = analysis['statistics']
    
    with col1:
        st.metric("Total Tokens", stats['total_tokens'])
    with col2:
        st.metric("Unique Tokens", stats['unique_tokens'])
    with col3:
        st.metric("Avg Token Length", stats['avg_token_length'])
    with col4:
        st.metric("OOV Ratio", f"{stats['oov_ratio'] * 100:.2f}%")
    with col5:
        st.metric("Vocabulary Richness", f"{stats['vocabulary_richness'] * 100:.2f}%")
    
    # Visualizations
    tab1, tab2, tab3 = st.tabs(["üìè Token Length", "üìà Frequency", "üìã Data"])
    
    with tab1:
        display_length_analysis(tokens)
    
    with tab2:
        display_frequency_analysis(analysis['token_freq'], language)
    
    with tab3:
        display_sample_data(original_text, processed_text, tokens, analysis['sentences'], bool(preprocessing_steps))

def display_subword_results(results_df):
    """Display subword model comparison results"""
    st.header("üéØ Subword Model Comparison Results")
    
    # Results table
    st.subheader("Detailed Results")
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ü–≤–µ—Ç–æ–≤–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    styled_df = results_df.style.format({
        'fragmentation_rate': '{:.2f}%',
        'compression_ratio': '{:.3f}',
        'reconstruction_accuracy': '{:.2f}%',
        'processing_time_sec': '{:.2f}s',
        'avg_token_length': '{:.2f}'
    }).background_gradient(subset=['fragmentation_rate'], cmap='Reds_r')\
      .background_gradient(subset=['reconstruction_accuracy'], cmap='Greens')\
      .background_gradient(subset=['compression_ratio'], cmap='Blues')
    
    st.dataframe(styled_df, use_container_width=True)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    if len(results_df) > 0:
        col1, col2 = st.columns(2)
        
        with col1:
            # Fragmentation rate comparison
            fig_frag = px.bar(results_df, x='model', y='fragmentation_rate', 
                             color='vocab_size', barmode='group',
                             title='üìä Fragmentation Rate by Model',
                             labels={'fragmentation_rate': 'Fragmentation Rate (%)', 'model': 'Model'})
            fig_frag.update_layout(template="plotly_white")
            st.plotly_chart(fig_frag, use_container_width=True)
        
        with col2:
            # Compression ratio comparison
            fig_comp = px.bar(results_df, x='model', y='compression_ratio',
                             color='vocab_size', barmode='group',
                             title='üìà Compression Ratio by Model',
                             labels={'compression_ratio': 'Compression Ratio', 'model': 'Model'})
            fig_comp.update_layout(template="plotly_white")
            st.plotly_chart(fig_comp, use_container_width=True)
        
        col3, col4 = st.columns(2)
        
        with col3:
            # Reconstruction accuracy
            fig_acc = px.bar(results_df, x='model', y='reconstruction_accuracy',
                            color='vocab_size', barmode='group',
                            title='üéØ Reconstruction Accuracy by Model',
                            labels={'reconstruction_accuracy': 'Accuracy (%)', 'model': 'Model'})
            fig_acc.update_layout(template="plotly_white")
            st.plotly_chart(fig_acc, use_container_width=True)
        
        with col4:
            # Processing time
            fig_time = px.bar(results_df, x='model', y='processing_time_sec',
                             color='vocab_size', barmode='group',
                             title='‚è±Ô∏è Processing Time by Model',
                             labels={'processing_time_sec': 'Time (seconds)', 'model': 'Model'})
            fig_time.update_layout(template="plotly_white")
            st.plotly_chart(fig_time, use_container_width=True)
        
        # Best models analysis
        st.subheader("üèÜ Best Performing Models")
        
        successful_models = results_df[results_df['actual_vocab_size'] > 50]
        
        if not successful_models.empty:
            best_fragmentation = successful_models.loc[successful_models['fragmentation_rate'].idxmin()]
            best_compression = successful_models.loc[successful_models['compression_ratio'].idxmin()]
            best_reconstruction = successful_models.loc[successful_models['reconstruction_accuracy'].idxmax()]
            best_speed = successful_models.loc[successful_models['processing_time_sec'].idxmin()]
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Best Fragmentation", 
                         f"{best_fragmentation['model']}", 
                         f"{best_fragmentation['fragmentation_rate']}%")
            with col2:
                st.metric("Best Compression", 
                         f"{best_compression['model']}",
                         f"{best_compression['compression_ratio']}")
            with col3:
                st.metric("Best Reconstruction", 
                         f"{best_reconstruction['model']}",
                         f"{best_reconstruction['reconstruction_accuracy']}%")
            with col4:
                st.metric("Fastest", 
                         f"{best_speed['model']}",
                         f"{best_speed['processing_time_sec']}s")

def display_length_analysis(tokens):
    """Display token length analysis"""
    token_lengths = [len(token) for token in tokens]
    if token_lengths:
        fig_length = px.histogram(
            x=token_lengths,
            title="Distribution of Token Lengths",
            labels={'x': 'Token Length', 'y': 'Frequency'},
            nbins=min(20, len(set(token_lengths)))
        )
        fig_length.update_layout(template="plotly_white")
        st.plotly_chart(fig_length, use_container_width=True)

def display_frequency_analysis(token_freq, language):
    """Display token frequency analysis"""
    top_tokens = token_freq.most_common(20)
    if top_tokens:
        tokens_list, counts_list = zip(*top_tokens)
        df_freq = pd.DataFrame({
            'Token': tokens_list,
            'Frequency': counts_list
        })
        
        fig_freq = px.bar(
            df_freq,
            x='Token',
            y='Frequency',
            title=f"Top 20 Most Frequent Tokens ({language})"
        )
        fig_freq.update_layout(xaxis_tickangle=-45, template="plotly_white")
        st.plotly_chart(fig_freq, use_container_width=True)

def display_sample_data(original_text, processed_text, tokens, sentences, preprocessing_enabled=False):
    """Display sample data"""
    col1, col2 = st.columns(2)
    
    with col1:
        if preprocessing_enabled:
            st.subheader("Original Text Sample")
            st.text_area("", original_text[:800] + "..." if len(original_text) > 800 else original_text, 
                       height=200, key="original_text")
        else:
            st.subheader("Text Sample")
            st.text_area("", original_text[:800] + "..." if len(original_text) > 800 else original_text, 
                       height=200, key="original_text")
    
    with col2:
        st.subheader("Processed Tokens")
        st.text_area("", ", ".join(tokens[:100]), height=200, key="tokens_preview")
        
        st.subheader("Token Statistics")
        token_lengths = [len(token) for token in tokens]
        if token_lengths:
            stats_df = pd.DataFrame({
                'Statistic': ['Min Length', 'Max Length', 'Median Length', 'Std Dev'],
                'Value': [
                    min(token_lengths),
                    max(token_lengths),
                    np.median(token_lengths),
                    np.std(token_lengths)
                ]
            })
            st.dataframe(stats_df, use_container_width=True)

def show_welcome_message():
    """Display welcome message"""
    st.markdown("""
    ## üöÄ Welcome to Advanced Text Analysis Tool!
    
    This tool combines traditional NLP analysis with advanced preprocessing and subword model comparison.
    
    ### üìù To get started:
    1. **Select your data source** in the sidebar
    2. **Configure preprocessing pipeline**:
       - **Universal Preprocessing**: Normalize punctuation, replace entities, expand abbreviations
       - **Text Cleaning**: Remove HTML, special characters, stopwords
    3. **Configure analysis parameters**:
       - Language (English, Russian, or Tatar)
       - Tokenization method and normalization
    4. **Enable advanced analysis** for subword model comparison
    5. **Click 'Analyze Text'** to see comprehensive results!
    
    ### üõ†Ô∏è Preprocessing Features:
    - **Universal Preprocessing**: Standardize text format, replace numbers/URLs/emails with tokens
    - **Text Cleaning**: Remove HTML tags, special characters, and stopwords
    - **Multi-language Support**: English, Russian, and Tatar text processing
    - **Flexible Pipeline**: Configure each preprocessing step independently
    
    ### üî¨ Advanced Features:
    - **Subword Model Training**: Train BPE, WordPiece, Unigram models
    - **Comparative Analysis**: Compare performance across different vocabulary sizes
    - **Quality Metrics**: Fragmentation rate, compression ratio, reconstruction accuracy
    
    ### üéØ Use Cases:
    - Preprocessing text for machine learning pipelines
    - Choosing optimal tokenization strategies
    - Understanding subword model trade-offs
    - Educational purposes in computational linguistics
    - Research and development of NLP systems
    """)

if __name__ == "__main__":
    main()