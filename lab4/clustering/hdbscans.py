import time

import matplotlib.pyplot as plt
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import (silhouette_score, calinski_harabasz_score,
                             davies_bouldin_score, adjusted_rand_score,
                             normalized_mutual_info_score, v_measure_score)

from util.decribe import get_labels, get_texts


def compare_hdbscan_parameters(texts, true_labels=None, min_cluster_size_range=None, min_samples_range=None):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ HDBSCAN —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ min_cluster_size –∏ min_samples
    """
    vectorizer = TfidfVectorizer(max_features=500)
    X = vectorizer.fit_transform(texts)
    X_dense = X.toarray()

    has_true_labels = true_labels is not None

    if min_cluster_size_range is None:
        min_cluster_size_range = [2, 3, 5, 10]
    if min_samples_range is None:
        min_samples_range = [1, 2, 3]

    if has_true_labels:
        print("üî¨ HDBSCAN –° –í–ù–£–¢–†–ï–ù–ù–ò–ú–ò –ò –í–ù–ï–®–ù–ò–ú–ò –ú–ï–¢–†–ò–ö–ê–ú–ò:")
        print("min_clust\tmin_sam\tClusters\tNoise\tSilhouette\tCalinski\tDavies-B\tARI\t\tNMI\t\tV-measure")
        print("-" * 105)
    else:
        print("üî¨ HDBSCAN –° –í–ù–£–¢–†–ï–ù–ù–ò–ú–ò –ú–ï–¢–†–ò–ö–ê–ú–ò:")
        print("min_clust\tmin_sam\tClusters\tNoise\tSilhouette\tCalinski\tDavies-B")
        print("-" * 75)

    results = []

    for min_cluster_size in min_cluster_size_range:
        for min_samples in min_samples_range:
            # HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=min_cluster_size,
                min_samples=min_samples,
                metric='cosine',  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤
                cluster_selection_epsilon=0.0
            )
            labels = clusterer.fit_predict(X)

            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —à—É–º–æ–≤—ã–µ —Ç–æ—á–∫–∏ (-1) –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫
            non_noise_mask = labels != -1
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            n_noise = np.sum(labels == -1)

            # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–∞–∫ –º–∏–Ω–∏–º—É–º 2 –∫–ª–∞—Å—Ç–µ—Ä–∞ –∏ –Ω–µ –≤—Å–µ —Ç–æ—á–∫–∏ - —à—É–º
            if n_clusters >= 2 and np.sum(non_noise_mask) >= 2:
                # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (–±–µ–∑ —à—É–º–∞)
                silhouette = silhouette_score(X_dense[non_noise_mask], labels[non_noise_mask])
                calinski = calinski_harabasz_score(X_dense[non_noise_mask], labels[non_noise_mask])
                davies = davies_bouldin_score(X_dense[non_noise_mask], labels[non_noise_mask])
            else:
                silhouette = calinski = davies = -1

            if has_true_labels:
                # –í–Ω–µ—à–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (–≤–∫–ª—é—á–∞—è —à—É–º –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä)
                ari = adjusted_rand_score(true_labels, labels)
                nmi = normalized_mutual_info_score(true_labels, labels)
                v_measure = v_measure_score(true_labels, labels)

                print(f"{min_cluster_size}\t\t{min_samples}\t{n_clusters}\t\t{n_noise}\t{silhouette:.3f}\t\t"
                      f"{calinski:.3f}\t\t{davies:.3f}\t\t{ari:.3f}\t\t{nmi:.3f}\t\t{v_measure:.3f}")
            else:
                print(f"{min_cluster_size}\t\t{min_samples}\t{n_clusters}\t\t{n_noise}\t{silhouette:.3f}\t\t"
                      f"{calinski:.3f}\t\t{davies:.3f}")

            results.append({
                'min_cluster_size': min_cluster_size,
                'min_samples': min_samples,
                'n_clusters': n_clusters,
                'n_noise': n_noise,
                'silhouette': silhouette,
                'calinski': calinski,
                'davies': davies,
                'ari': ari if has_true_labels else -1,
                'nmi': nmi if has_true_labels else -1,
                'v_measure': v_measure if has_true_labels else -1,
                'labels': labels,
                'clusterer': clusterer
            })

    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    valid_results = [r for r in results if r['n_clusters'] >= 2]

    if valid_results:
        best_by_silhouette = max(valid_results, key=lambda x: x['silhouette'])
        if has_true_labels:
            best_by_ari = max(valid_results, key=lambda x: x['ari'])

        print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        print(f"   –ü–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –º–µ—Ç—Ä–∏–∫–∞–º: min_cluster_size={best_by_silhouette['min_cluster_size']}, "
              f"min_samples={best_by_silhouette['min_samples']} "
              f"(Silhouette: {best_by_silhouette['silhouette']:.3f}, "
              f"–ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {best_by_silhouette['n_clusters']})")
        if has_true_labels:
            print(f"   –ü–æ –≤–Ω–µ—à–Ω–∏–º –º–µ—Ç—Ä–∏–∫–∞–º: min_cluster_size={best_by_ari['min_cluster_size']}, "
                  f"min_samples={best_by_ari['min_samples']} "
                  f"(ARI: {best_by_ari['ari']:.3f}, "
                  f"–ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {best_by_ari['n_clusters']})")
    else:
        print(f"\n‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Å–æ–∑–¥–∞—é—â–∏—Ö —Ö–æ—Ç—è –±—ã 2 –∫–ª–∞—Å—Ç–µ—Ä–∞")
        best_by_silhouette = best_by_ari = None

    # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫–∏
    if has_true_labels and valid_results:
        _plot_hdbscan_metrics(results, texts, true_labels, best_by_silhouette, best_by_ari)
    elif valid_results:
        _plot_hdbscan_internal_metrics(results, best_by_silhouette)

    if has_true_labels and valid_results:
        return best_by_silhouette, best_by_ari
    elif valid_results:
        return best_by_silhouette
    else:
        return None, None if has_true_labels else None


def _plot_hdbscan_metrics(results, texts, true_labels, best_silhouette, best_ari):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è HDBSCAN —Å–æ –≤—Å–µ–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10, 8))

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
    min_cluster_sizes = sorted(set(r['min_cluster_size'] for r in results))
    min_samples_values = sorted(set(r['min_samples'] for r in results))

    # –ì—Ä–∞—Ñ–∏–∫ 1: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ —à—É–º–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö min_cluster_size
    cluster_data_by_size = {}
    for min_cluster_size in min_cluster_sizes:
        size_data = [r for r in results if r['min_cluster_size'] == min_cluster_size and r['min_samples'] == 1]
        if size_data:
            cluster_data_by_size[min_cluster_size] = size_data[0]

    if cluster_data_by_size:
        sizes = list(cluster_data_by_size.keys())
        clusters = [cluster_data_by_size[size]['n_clusters'] for size in sizes]
        noise = [cluster_data_by_size[size]['n_noise'] for size in sizes]

        ax1.plot(sizes, clusters, 'bo-', linewidth=2, markersize=6, label='–ö–ª–∞—Å—Ç–µ—Ä—ã')
        ax1.plot(sizes, noise, 'ro-', linewidth=2, markersize=6, label='–®—É–º')

    ax1.set_xlabel('min_cluster_size')
    ax1.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
    ax1.set_title('HDBSCAN: –ö–õ–ê–°–¢–ï–†–´ –ò –®–£–ú\n(min_samples=1)')
    ax1.grid(True, alpha=0.3)
    ax1.legend()

    # –ì—Ä–∞—Ñ–∏–∫ 2: –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö min_samples
    for min_cluster_size in [2, 5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–ª—è –¥–≤—É—Ö –∑–Ω–∞—á–µ–Ω–∏–π min_cluster_size
        samples_data = [r for r in results if r['min_cluster_size'] == min_cluster_size and r['silhouette'] > -1]
        if samples_data:
            min_samples_vals = [r['min_samples'] for r in samples_data]
            silhouette_vals = [r['silhouette'] for r in samples_data]
            ax2.plot(min_samples_vals, silhouette_vals, 'o-', linewidth=2, markersize=6,
                     label=f'Silhouette (min_cluster_size={min_cluster_size})')

    ax2.set_xlabel('min_samples')
    ax2.set_ylabel('Silhouette Score')
    ax2.set_title('HDBSCAN: SILHOUETTE SCORE\n(‚Üë –ª—É—á—à–µ)')
    ax2.grid(True, alpha=0.3)
    ax2.legend()

    # –ì—Ä–∞—Ñ–∏–∫ 3: –í–Ω–µ—à–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    if best_ari:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º ARI –¥–ª—è —Ä–∞–∑–Ω—ã—Ö min_cluster_size –ø—Ä–∏ min_samples=1
        ari_data = [r for r in results if r['min_samples'] == 1 and r['ari'] > -1]
        if ari_data:
            sizes = [r['min_cluster_size'] for r in ari_data]
            ari_vals = [r['ari'] for r in ari_data]
            ax3.plot(sizes, ari_vals, 'go-', linewidth=2, markersize=6, label='ARI')

    ax3.set_xlabel('min_cluster_size')
    ax3.set_ylabel('ARI Score')
    ax3.set_title('HDBSCAN: ADJUSTED RAND INDEX\n(min_samples=1, ‚Üë –ª—É—á—à–µ)')
    ax3.grid(True, alpha=0.3)
    ax3.legend()

    # –ì—Ä–∞—Ñ–∏–∫ 4: –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    times = []
    vectorizer = TfidfVectorizer(max_features=500)
    X = vectorizer.fit_transform(texts)

    test_sizes = [2, 5, 10, 15, 20]
    for min_cluster_size in test_sizes:
        clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=1, metric='cosine')
        start_time = time.time()
        clusterer.fit(X)
        times.append(time.time() - start_time)

    ax4.plot(test_sizes, times, color='purple', linestyle='-', marker='o', linewidth=2, markersize=6)
    ax4.set_xlabel('min_cluster_size')
    ax4.set_ylabel('–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (—Å–µ–∫—É–Ω–¥—ã)')
    ax4.set_title('HDBSCAN: –í–†–ï–ú–Ø –í–´–ü–û–õ–ù–ï–ù–ò–Ø\n(min_samples=1)')
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()


def _plot_hdbscan_internal_metrics(results, best_params):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

    min_cluster_sizes = sorted(set(r['min_cluster_size'] for r in results))

    # –ì—Ä–∞—Ñ–∏–∫ 1: Silhouette Score –¥–ª—è —Ä–∞–∑–Ω—ã—Ö min_cluster_size
    silhouette_data = []
    for min_cluster_size in min_cluster_sizes:
        size_data = [r for r in results if
                     r['min_cluster_size'] == min_cluster_size and r['min_samples'] == 1 and r['silhouette'] > -1]
        if size_data:
            silhouette_data.append((min_cluster_size, size_data[0]['silhouette']))

    if silhouette_data:
        sizes, silhouettes = zip(*silhouette_data)
        ax1.plot(sizes, silhouettes, 'bo-', linewidth=2, markersize=6)

        if best_params:
            ax1.axvline(x=best_params['min_cluster_size'], color='red', linestyle='--', alpha=0.7,
                        label=f'–õ—É—á—à–µ–µ: {best_params["min_cluster_size"]}')

    ax1.set_xlabel('min_cluster_size')
    ax1.set_ylabel('Silhouette Score')
    ax1.set_title('HDBSCAN: SILHOUETTE SCORE\n(min_samples=1, ‚Üë –ª—É—á—à–µ)')
    ax1.grid(True, alpha=0.3)
    if best_params:
        ax1.legend()

    # –ì—Ä–∞—Ñ–∏–∫ 2: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    cluster_data = []
    for min_cluster_size in min_cluster_sizes:
        size_data = [r for r in results if r['min_cluster_size'] == min_cluster_size and r['min_samples'] == 1]
        if size_data:
            cluster_data.append((min_cluster_size, size_data[0]['n_clusters']))

    if cluster_data:
        sizes, clusters = zip(*cluster_data)
        ax2.plot(sizes, clusters, 'go-', linewidth=2, markersize=6, label='–ö–ª–∞—Å—Ç–µ—Ä—ã')

    ax2.set_xlabel('min_cluster_size')
    ax2.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')
    ax2.set_title('HDBSCAN: –ö–û–õ–ò–ß–ï–°–¢–í–û –ö–õ–ê–°–¢–ï–†–û–í\n(min_samples=1)')
    ax2.grid(True, alpha=0.3)
    ax2.legend()

    plt.tight_layout()
    plt.show()


# –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ HDBSCAN
def simple_hdbscan_cluster(texts, min_cluster_size=5, min_samples=1):
    """
    –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —Å HDBSCAN
    """
    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
    vectorizer = TfidfVectorizer(max_features=500)
    X = vectorizer.fit_transform(texts)
    X_dense = X.toarray()

    # HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=min_cluster_size,
        min_samples=min_samples,
        metric='cosine',  # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤
        cluster_selection_epsilon=0.0
    )
    labels = clusterer.fit_predict(X)

    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    unique_labels = set(labels)
    n_clusters = len(unique_labels) - (1 if -1 in labels else 0)
    n_noise = np.sum(labels == -1)

    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ (–±–µ–∑ —à—É–º–∞)
    non_noise_mask = labels != -1

    if n_clusters >= 2 and np.sum(non_noise_mask) >= 2:
        metrics = {
            'silhouette': silhouette_score(X_dense[non_noise_mask], labels[non_noise_mask]),
            'calinski_harabasz': calinski_harabasz_score(X_dense[non_noise_mask], labels[non_noise_mask]),
            'davies_bouldin': davies_bouldin_score(X_dense[non_noise_mask], labels[non_noise_mask]),
            'n_clusters': n_clusters,
            'n_noise': n_noise,
            'cluster_persistence': clusterer.cluster_persistence_ if hasattr(clusterer,
                                                                             'cluster_persistence_') else None
        }
    else:
        metrics = {
            'silhouette': -1,
            'calinski_harabasz': -1,
            'davies_bouldin': -1,
            'n_clusters': n_clusters,
            'n_noise': n_noise,
            'cluster_persistence': None
        }

    # –ü—Ä–æ—Å—Ç–æ–π –≤—ã–≤–æ–¥
    print(f"üìä HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤:")
    print(f"‚öôÔ∏è  –ü–ê–†–ê–ú–ï–¢–†–´: min_cluster_size={min_cluster_size}, min_samples={min_samples}")
    print(f"üéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}")
    print(f"   –®—É–º–æ–≤—ã—Ö —Ç–æ—á–µ–∫: {n_noise} ({n_noise / len(texts) * 100:.1f}%)")

    if metrics['silhouette'] > -1:
        print(f"   Silhouette Score: {metrics['silhouette']:.3f}")
        print(f"   Calinski-Harabasz: {metrics['calinski_harabasz']:.3f}")
        print(f"   Davies-Bouldin: {metrics['davies_bouldin']:.3f}")

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è Silhouette Score
        silhouette_val = metrics['silhouette']
        if silhouette_val > 0.7:
            interpretation = "–û—Ç–ª–∏—á–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ"
        elif silhouette_val > 0.5:
            interpretation = "–†–∞–∑—É–º–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ"
        elif silhouette_val > 0.25:
            interpretation = "–°–ª–∞–±–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ"
        else:
            interpretation = "–ù–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è"
        print(f"   –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: {interpretation}")
    else:
        print("   ‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫")

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
    print(f"\nüîç –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ö–õ–ê–°–¢–ï–†–ê–•:")
    for label in sorted(unique_labels):
        if label == -1:
            print(f"üî∏ –®—É–º: {n_noise} —Ç–µ–∫—Å—Ç–æ–≤")
            continue

        cluster_texts = [texts[j] for j, lbl in enumerate(labels) if lbl == label]
        print(f"üî∏ –ö–ª–∞—Å—Ç–µ—Ä {label}: {len(cluster_texts)} —Ç–µ–∫—Å—Ç–æ–≤")
        if len(cluster_texts) > 0:
            for text in cluster_texts[:2]:
                print(f"   - {text[:60]}..." if len(text) > 60 else f"   - {text}")
            if len(cluster_texts) > 2:
                print(f"   ... –∏ –µ—â–µ {len(cluster_texts) - 2} —Ç–µ–∫—Å—Ç–æ–≤")
        print()

    return labels, metrics, clusterer


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–µ—Ä–µ–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ HDBSCAN
def plot_cluster_tree(clusterer, texts):
    """
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ä–µ–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ HDBSCAN
    """
    if hasattr(clusterer, 'condensed_tree_'):
        plt.figure(figsize=(10, 6))
        clusterer.condensed_tree_.plot(select_clusters=True,
                                       selection_palette=['red', 'blue', 'green', 'orange', 'purple'])
        plt.title('HDBSCAN: –î–ï–†–ï–í–û –ö–õ–ê–°–¢–ï–†–û–í')
        plt.tight_layout()
        plt.show()

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
        if hasattr(clusterer, 'cluster_persistence_'):
            print("\nüìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –£–°–¢–û–ô–ß–ò–í–û–°–¢–ò –ö–õ–ê–°–¢–ï–†–û–í:")
            for i, persistence in enumerate(clusterer.cluster_persistence_):
                print(f"   –ö–ª–∞—Å—Ç–µ—Ä {i}: —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å = {persistence:.3f}")


if __name__ == "__main__":
    texts = get_texts()
    true_labels = get_labels()

    print("üöÄ HDBSCAN –î–õ–Ø –¢–ï–ö–°–¢–û–í")
    print("=" * 50)

    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ hdbscan –µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    try:
        import hdbscan
    except ImportError:
        print("‚ùå HDBSCAN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install hdbscan")
        exit()

    # –®–∞–≥ 1: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –≤–Ω–µ—à–Ω–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    print("üéØ –®–ê–ì 1: –ü–û–õ–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –° –ú–ï–¢–†–ò–ö–ê–ú–ò")
    best_by_int, best_by_ext = compare_hdbscan_parameters(
        texts, true_labels,
        min_cluster_size_range=[2, 3, 5, 10, 15],
        min_samples_range=[1, 2, 3]
    )

    # –®–∞–≥ 2: –ë—ã—Å—Ç—Ä–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    print("\nüéØ –®–ê–ì 2: –ë–´–°–¢–†–ê–Ø –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø")
    if best_by_int:
        labels, metrics, clusterer = simple_hdbscan_cluster(
            texts,
            min_cluster_size=best_by_int['min_cluster_size'],
            min_samples=best_by_int['min_samples']
        )
    else:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ª—É—á—à–∏—Ö
        labels, metrics, clusterer = simple_hdbscan_cluster(
            texts,
            min_cluster_size=5,
            min_samples=1
        )

    # –®–∞–≥ 3: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ä–µ–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    print("\nüéØ –®–ê–ì 3: –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –î–ï–†–ï–í–ê –ö–õ–ê–°–¢–ï–†–û–í")
    plot_cluster_tree(clusterer, texts)