from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics import (silhouette_score, calinski_harabasz_score,
                             davies_bouldin_score, adjusted_rand_score,
                             normalized_mutual_info_score, v_measure_score,
                             homogeneity_score, completeness_score)
from gensim import corpora, models
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel
import matplotlib.pyplot as plt
import numpy as np
import time
import warnings

warnings.filterwarnings('ignore')

from util.decribe import get_labels, get_texts


def preprocess_for_lda(texts):
    """
    –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è LDA
    """
    # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—É—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É)
    tokenized_texts = [text.lower().split() for text in texts]
    return tokenized_texts


def compare_lda_models(texts, true_labels=None, num_topics_range=None, passes_range=None):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ LDA –º–æ–¥–µ–ª–µ–π —Å —Ä–∞–∑–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–º –∏ passes
    """
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è LDA
    tokenized_texts = preprocess_for_lda(texts)

    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –∏ –∫–æ—Ä–ø—É—Å–∞
    dictionary = corpora.Dictionary(tokenized_texts)
    dictionary.filter_extremes(no_below=2, no_above=0.8)  # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–¥–∫–∏—Ö –∏ —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤
    corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_texts]

    has_true_labels = true_labels is not None

    if num_topics_range is None:
        num_topics_range = [2, 3, 5, 8, 10]
    if passes_range is None:
        passes_range = [5, 10]

    if has_true_labels:
        print("üî¨ LDA MODEL –° –ú–ï–¢–†–ò–ö–ê–ú–ò:")
        print("Topics\tPasses\tCoherence\tPerplexity\tSilhouette\tARI\t\tNMI\t\tV-measure")
        print("-" * 95)
    else:
        print("üî¨ LDA MODEL –° –ú–ï–¢–†–ò–ö–ê–ú–ò:")
        print("Topics\tPasses\tCoherence\tPerplexity\tSilhouette")
        print("-" * 65)

    results = []

    for num_topics in num_topics_range:
        for passes in passes_range:
            try:
                # –û–±—É—á–µ–Ω–∏–µ LDA –º–æ–¥–µ–ª–∏
                lda_model = LdaModel(
                    corpus=corpus,
                    id2word=dictionary,
                    num_topics=num_topics,
                    passes=passes,
                    random_state=42,
                    alpha='auto',
                    eta='auto'
                )

                # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ LDA
                # –ö–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–º
                coherence_model = CoherenceModel(
                    model=lda_model,
                    texts=tokenized_texts,
                    dictionary=dictionary,
                    coherence='c_v'
                )
                coherence = coherence_model.get_coherence()

                # –ü–µ—Ä–ø–ª–µ–∫—Å–∏—è
                perplexity = lda_model.log_perplexity(corpus)

                # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —Ç–µ–º –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                topic_distributions = []
                for doc in corpus:
                    topic_dist = lda_model.get_document_topics(doc, minimum_probability=0)
                    topic_distributions.append([prob for _, prob in topic_dist])

                topic_distributions = np.array(topic_distributions)

                # –ñ–µ—Å—Ç–∫–æ–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Ç–µ–º (—Ç–µ–º–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é)
                hard_labels = np.argmax(topic_distributions, axis=1)

                # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —Ç–µ–º
                # –î–ª—è silhouette –Ω—É–∂–Ω—ã –ø–ª–æ—Ç–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º
                silhouette = silhouette_score(topic_distributions, hard_labels)

                if has_true_labels:
                    # –í–Ω–µ—à–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
                    ari = adjusted_rand_score(true_labels, hard_labels)
                    nmi = normalized_mutual_info_score(true_labels, hard_labels)
                    v_measure = v_measure_score(true_labels, hard_labels)

                    print(f"{num_topics}\t{passes}\t{coherence:.3f}\t\t{perplexity:.3f}\t\t{silhouette:.3f}\t\t"
                          f"{ari:.3f}\t\t{nmi:.3f}\t\t{v_measure:.3f}")
                else:
                    print(f"{num_topics}\t{passes}\t{coherence:.3f}\t\t{perplexity:.3f}\t\t{silhouette:.3f}")

                results.append({
                    'num_topics': num_topics,
                    'passes': passes,
                    'coherence': coherence,
                    'perplexity': perplexity,
                    'silhouette': silhouette,
                    'ari': ari if has_true_labels else -1,
                    'nmi': nmi if has_true_labels else -1,
                    'v_measure': v_measure if has_true_labels else -1,
                    'hard_labels': hard_labels,
                    'topic_distributions': topic_distributions,
                    'lda_model': lda_model,
                    'dictionary': dictionary,
                    'corpus': corpus
                })

            except Exception as e:
                print(f"{num_topics}\t{passes}\tERROR: {str(e)[:30]}...")

    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    if results:
        best_by_coherence = max(results, key=lambda x: x['coherence'])
        best_by_silhouette = max(results, key=lambda x: x['silhouette'])

        if has_true_labels:
            best_by_ari = max(results, key=lambda x: x['ari'])

        print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        print(f"   –ü–æ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–º: {best_by_coherence['num_topics']} —Ç–µ–º, "
              f"{best_by_coherence['passes']} passes "
              f"(Coherence: {best_by_coherence['coherence']:.3f})")
        print(f"   –ü–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –º–µ—Ç—Ä–∏–∫–∞–º: {best_by_silhouette['num_topics']} —Ç–µ–º, "
              f"{best_by_silhouette['passes']} passes "
              f"(Silhouette: {best_by_silhouette['silhouette']:.3f})")
        if has_true_labels:
            print(f"   –ü–æ –≤–Ω–µ—à–Ω–∏–º –º–µ—Ç—Ä–∏–∫–∞–º: {best_by_ari['num_topics']} —Ç–µ–º, "
                  f"{best_by_ari['passes']} passes "
                  f"(ARI: {best_by_ari['ari']:.3f})")
    else:
        print(f"\n‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å LDA –º–æ–¥–µ–ª–∏")
        best_by_coherence = best_by_silhouette = best_by_ari = None

    # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫–∏
    if has_true_labels and results:
        _plot_lda_metrics(results, texts, true_labels, best_by_coherence, best_by_ari, best_by_silhouette)
    elif results:
        _plot_lda_internal_metrics(results, best_by_coherence, best_by_silhouette)

    if has_true_labels and results:
        return best_by_coherence, best_by_ari, best_by_silhouette
    elif results:
        return best_by_coherence, best_by_silhouette
    else:
        return None, None, None


def _plot_lda_metrics(results, texts, true_labels, best_coherence, best_ari, best_silhouette):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è LDA —Å–æ –≤—Å–µ–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
    passes_values = sorted(set(r['passes'] for r in results))
    topics_values = sorted(set(r['num_topics'] for r in results))

    # –ì—Ä–∞—Ñ–∏–∫ 1: Coherence –¥–ª—è —Ä–∞–∑–Ω—ã—Ö passes
    for passes in passes_values:
        pass_data = [r for r in results if r['passes'] == passes]
        if pass_data:
            topics_vals = [r['num_topics'] for r in pass_data]
            coherence_vals = [r['coherence'] for r in pass_data]
            ax1.plot(topics_vals, coherence_vals, 'o-', linewidth=2, markersize=6,
                     label=f'passes={passes}')

    if best_coherence:
        ax1.axvline(x=best_coherence['num_topics'], color='red', linestyle='--', alpha=0.7,
                    label=f'–õ—É—á—à–µ–µ: {best_coherence["num_topics"]} —Ç–µ–º')

    ax1.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º')
    ax1.set_ylabel('Coherence Score')
    ax1.set_title('LDA: COHERENCE SCORE\n(‚Üë –ª—É—á—à–µ)')
    ax1.grid(True, alpha=0.3)
    ax1.legend()

    # –ì—Ä–∞—Ñ–∏–∫ 2: Perplexity
    for passes in passes_values:
        pass_data = [r for r in results if r['passes'] == passes]
        if pass_data:
            topics_vals = [r['num_topics'] for r in pass_data]
            perplexity_vals = [r['perplexity'] for r in pass_data]
            ax2.plot(topics_vals, perplexity_vals, 'o-', linewidth=2, markersize=6,
                     label=f'passes={passes}')

    ax2.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º')
    ax2.set_ylabel('Perplexity')
    ax2.set_title('LDA: PERPLEXITY\n(‚Üì –ª—É—á—à–µ)')
    ax2.grid(True, alpha=0.3)
    ax2.legend()

    # –ì—Ä–∞—Ñ–∏–∫ 3: –í–Ω–µ—à–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (ARI)
    if best_ari:
        for passes in passes_values:
            pass_data = [r for r in results if r['passes'] == passes and r['ari'] > -1]
            if pass_data:
                topics_vals = [r['num_topics'] for r in pass_data]
                ari_vals = [r['ari'] for r in pass_data]
                ax3.plot(topics_vals, ari_vals, 'o-', linewidth=2, markersize=6,
                         label=f'passes={passes}')

    ax3.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º')
    ax3.set_ylabel('ARI Score')
    ax3.set_title('LDA: ADJUSTED RAND INDEX\n(‚Üë –ª—É—á—à–µ)')
    ax3.grid(True, alpha=0.3)
    ax3.legend()

    # –ì—Ä–∞—Ñ–∏–∫ 4: –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    times = {}
    tokenized_texts = preprocess_for_lda(texts)
    dictionary = corpora.Dictionary(tokenized_texts)
    dictionary.filter_extremes(no_below=2, no_above=0.8)
    corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_texts]

    for passes in passes_values:
        pass_times = []
        for num_topics in topics_values:
            try:
                start_time = time.time()
                lda_model = LdaModel(
                    corpus=corpus,
                    id2word=dictionary,
                    num_topics=num_topics,
                    passes=passes,
                    random_state=42
                )
                pass_times.append(time.time() - start_time)
            except:
                pass_times.append(np.nan)

        times[passes] = pass_times

    for passes, time_vals in times.items():
        ax4.plot(topics_values, time_vals, 'o-', linewidth=2, markersize=6, label=f'passes={passes}')

    ax4.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º')
    ax4.set_ylabel('–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (—Å–µ–∫—É–Ω–¥—ã)')
    ax4.set_title('LDA: –í–†–ï–ú–Ø –í–´–ü–û–õ–ù–ï–ù–ò–Ø')
    ax4.grid(True, alpha=0.3)
    ax4.legend()

    plt.tight_layout()
    plt.show()


def _plot_lda_internal_metrics(results, best_coherence, best_silhouette):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫ LDA
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

    passes_values = sorted(set(r['passes'] for r in results))
    topics_values = sorted(set(r['num_topics'] for r in results))

    # –ì—Ä–∞—Ñ–∏–∫ 1: Coherence Score
    for passes in passes_values:
        pass_data = [r for r in results if r['passes'] == passes]
        if pass_data:
            topics_vals = [r['num_topics'] for r in pass_data]
            coherence_vals = [r['coherence'] for r in pass_data]
            ax1.plot(topics_vals, coherence_vals, 'o-', linewidth=2, markersize=6,
                     label=f'passes={passes}')

    if best_coherence:
        ax1.axvline(x=best_coherence['num_topics'], color='red', linestyle='--', alpha=0.7,
                    label=f'–õ—É—á—à–µ–µ: {best_coherence["num_topics"]} —Ç–µ–º')

    ax1.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º')
    ax1.set_ylabel('Coherence Score')
    ax1.set_title('LDA: COHERENCE SCORE\n(‚Üë –ª—É—á—à–µ)')
    ax1.grid(True, alpha=0.3)
    ax1.legend()

    # –ì—Ä–∞—Ñ–∏–∫ 2: Silhouette Score
    for passes in passes_values:
        pass_data = [r for r in results if r['passes'] == passes]
        if pass_data:
            topics_vals = [r['num_topics'] for r in pass_data]
            silhouette_vals = [r['silhouette'] for r in pass_data]
            ax2.plot(topics_vals, silhouette_vals, 'o-', linewidth=2, markersize=6,
                     label=f'passes={passes}')

    if best_silhouette:
        ax2.axvline(x=best_silhouette['num_topics'], color='blue', linestyle='--', alpha=0.7,
                    label=f'–õ—É—á—à–µ–µ: {best_silhouette["num_topics"]} —Ç–µ–º')

    ax2.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º')
    ax2.set_ylabel('Silhouette Score')
    ax2.set_title('LDA: SILHOUETTE SCORE\n(‚Üë –ª—É—á—à–µ)')
    ax2.grid(True, alpha=0.3)
    ax2.legend()

    plt.tight_layout()
    plt.show()


# –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è LDA
def simple_lda_modeling(texts, num_topics=5, passes=10):
    """
    –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å LDA
    """
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
    tokenized_texts = preprocess_for_lda(texts)

    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –∏ –∫–æ—Ä–ø—É—Å–∞
    dictionary = corpora.Dictionary(tokenized_texts)
    dictionary.filter_extremes(no_below=2, no_above=0.8)
    corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_texts]

    print(f"üìä LDA —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤:")
    print(f"‚öôÔ∏è  –ü–ê–†–ê–ú–ï–¢–†–´: {num_topics} —Ç–µ–º, {passes} passes")
    print(f"üìù –°–õ–û–í–ê–†–¨: {len(dictionary)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤")

    # –û–±—É—á–µ–Ω–∏–µ LDA –º–æ–¥–µ–ª–∏
    lda_model = LdaModel(
        corpus=corpus,
        id2word=dictionary,
        num_topics=num_topics,
        passes=passes,
        random_state=42,
        alpha='auto',
        eta='auto'
    )

    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    coherence_model = CoherenceModel(
        model=lda_model,
        texts=tokenized_texts,
        dictionary=dictionary,
        coherence='c_v'
    )
    coherence = coherence_model.get_coherence()
    perplexity = lda_model.log_perplexity(corpus)

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    topic_distributions = []
    for doc in corpus:
        topic_dist = lda_model.get_document_topics(doc, minimum_probability=0)
        topic_distributions.append([prob for _, prob in topic_dist])

    topic_distributions = np.array(topic_distributions)
    hard_labels = np.argmax(topic_distributions, axis=1)

    # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    silhouette = silhouette_score(topic_distributions, hard_labels)

    metrics = {
        'coherence': coherence,
        'perplexity': perplexity,
        'silhouette': silhouette,
        'num_topics': num_topics,
        'passes': passes
    }

    print(f"üéØ –ú–ï–¢–†–ò–ö–ò –ú–û–î–ï–õ–ò:")
    print(f"   Coherence Score: {coherence:.3f}")
    print(f"   Perplexity: {perplexity:.3f}")
    print(f"   Silhouette Score: {silhouette:.3f}")

    # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è Coherence Score
    if coherence > 0.6:
        interpretation = "–û—Ç–ª–∏—á–Ω–∞—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–º"
    elif coherence > 0.5:
        interpretation = "–•–æ—Ä–æ—à–∞—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–º"
    elif coherence > 0.4:
        interpretation = "–£–º–µ—Ä–µ–Ω–Ω–∞—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–º"
    else:
        interpretation = "–ù–∏–∑–∫–∞—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–º"
    print(f"   –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: {interpretation}")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–º
    print(f"\nüîç –¢–ï–ú–´ –ò –ò–• –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê:")
    topics = lda_model.print_topics(num_words=8)
    for idx, topic in topics:
        print(f"üî∏ –¢–µ–º–∞ {idx}: {topic}")

    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º
    print(f"\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –î–û–ö–£–ú–ï–ù–¢–û–í –ü–û –¢–ï–ú–ê–ú:")
    unique_labels, counts = np.unique(hard_labels, return_counts=True)
    for topic_id, count in zip(unique_labels, counts):
        percentage = (count / len(texts)) * 100
        print(f"   –¢–µ–º–∞ {topic_id}: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({percentage:.1f}%)")

    # –ü—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã
    print(f"\nüìÑ –ü–†–ò–ú–ï–†–´ –î–û–ö–£–ú–ï–ù–¢–û–í –î–õ–Ø –ö–ê–ñ–î–û–ô –¢–ï–ú–´:")
    for topic_id in range(num_topics):
        topic_docs = [texts[i] for i, label in enumerate(hard_labels) if label == topic_id]
        print(f"\nüî∏ –¢–µ–º–∞ {topic_id} ({len(topic_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤):")
        if topic_docs:
            for doc in topic_docs[:2]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 2 –¥–æ–∫—É–º–µ–Ω—Ç–∞
                print(f"   - {doc[:80]}..." if len(doc) > 80 else f"   - {doc}")
            if len(topic_docs) > 2:
                print(f"   ... –∏ –µ—â–µ {len(topic_docs) - 2} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    # –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è
    max_probs = np.max(topic_distributions, axis=1)
    confidence_stats = {
        'high_confidence': np.sum(max_probs > 0.8) / len(max_probs) * 100,
        'medium_confidence': np.sum((max_probs > 0.6) & (max_probs <= 0.8)) / len(max_probs) * 100,
        'low_confidence': np.sum(max_probs <= 0.6) / len(max_probs) * 100
    }

    print(f"\nüéØ –£–í–ï–†–ï–ù–ù–û–°–¢–¨ –¢–ï–ú–ê–¢–ò–ß–ï–°–ö–û–ì–û –ù–ê–ó–ù–ê–ß–ï–ù–ò–Ø:")
    print(f"   –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (>0.8): {confidence_stats['high_confidence']:.1f}% –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print(f"   –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (0.6-0.8): {confidence_stats['medium_confidence']:.1f}% –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print(f"   –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (‚â§0.6): {confidence_stats['low_confidence']:.1f}% –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    return hard_labels, topic_distributions, metrics, lda_model, dictionary


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–µ–º –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
def predict_lda_topics(new_texts, lda_model, dictionary):
    """
    –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–µ–º –¥–ª—è –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    """
    tokenized_texts = preprocess_for_lda(new_texts)
    corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_texts]

    topic_distributions = []
    for doc in corpus:
        topic_dist = lda_model.get_document_topics(doc, minimum_probability=0)
        topic_distributions.append([prob for _, prob in topic_dist])

    hard_labels = np.argmax(topic_distributions, axis=1)

    return hard_labels, np.array(topic_distributions)


if __name__ == "__main__":
    texts = get_texts()
    true_labels = get_labels()

    print("üöÄ LDA THEMATIC MODELING –î–õ–Ø –¢–ï–ö–°–¢–û–í")
    print("=" * 60)

    # –í–∞—Ä–∏–∞–Ω—Ç 1: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    print("üéØ –í–ê–†–ò–ê–ù–¢ 1: –ü–û–õ–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –° –ú–ï–¢–†–ò–ö–ê–ú–ò")
    best_by_coh, best_by_ext, best_by_sil = compare_lda_models(
        texts, true_labels,
        num_topics_range=[2, 3, 5, 8, 10],
        passes_range=[5, 10]
    )

    # –í–∞—Ä–∏–∞–Ω—Ç 2: –ë—ã—Å—Ç—Ä–æ–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
    print("\nüéØ –í–ê–†–ò–ê–ù–¢ 2: –ë–´–°–¢–†–û–ï –¢–ï–ú–ê–¢–ò–ß–ï–°–ö–û–ï –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–ï")
    if best_by_coh:
        hard_labels, topic_dists, metrics, lda_model, dictionary = simple_lda_modeling(
            texts,
            num_topics=best_by_coh['num_topics'],
            passes=best_by_coh['passes']
        )
    else:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ª—É—á—à–∏—Ö
        hard_labels, topic_dists, metrics, lda_model, dictionary = simple_lda_modeling(
            texts,
            num_topics=5,
            passes=10
        )