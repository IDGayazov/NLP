import time

import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import (silhouette_score, calinski_harabasz_score,
                             davies_bouldin_score, adjusted_rand_score,
                             normalized_mutual_info_score, v_measure_score)
from sklearn.neighbors import NearestNeighbors

from util.decribe import get_labels, get_texts


def compare_dbscan_parameters(texts, true_labels=None, max_eps=0.5, eps_step=0.1, min_samples_range=None):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ DBSCAN —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ eps –∏ min_samples
    """
    vectorizer = TfidfVectorizer(max_features=500)
    X = vectorizer.fit_transform(texts)
    X_dense = X.toarray()

    has_true_labels = true_labels is not None

    if min_samples_range is None:
        min_samples_range = [2, 3, 5]

    if has_true_labels:
        print("üî¨ DBSCAN –° –í–ù–£–¢–†–ï–ù–ù–ò–ú–ò –ò –í–ù–ï–®–ù–ò–ú–ò –ú–ï–¢–†–ò–ö–ê–ú–ò:")
        print("eps\tmin_sam\tClusters\tNoise\tSilhouette\tCalinski\tDavies-B\tARI\t\tNMI\t\tV-measure")
        print("-" * 100)
    else:
        print("üî¨ DBSCAN –° –í–ù–£–¢–†–ï–ù–ù–ò–ú–ò –ú–ï–¢–†–ò–ö–ê–ú–ò:")
        print("eps\tmin_sam\tClusters\tNoise\tSilhouette\tCalinski\tDavies-B")
        print("-" * 70)

    results = []

    for eps in np.arange(0.1, max_eps + eps_step, eps_step):
        for min_samples in min_samples_range:
            # DBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
            dbscan = DBSCAN(
                eps=eps,
                min_samples=min_samples,
                metric='cosine'  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤
            )
            labels = dbscan.fit_predict(X)

            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —à—É–º–æ–≤—ã–µ —Ç–æ—á–∫–∏ (-1) –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫
            non_noise_mask = labels != -1
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            n_noise = np.sum(labels == -1)

            # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–∞–∫ –º–∏–Ω–∏–º—É–º 2 –∫–ª–∞—Å—Ç–µ—Ä–∞ –∏ –Ω–µ –≤—Å–µ —Ç–æ—á–∫–∏ - —à—É–º
            if n_clusters >= 2 and np.sum(non_noise_mask) >= 2:
                # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (–±–µ–∑ —à—É–º–∞)
                silhouette = silhouette_score(X_dense[non_noise_mask], labels[non_noise_mask])
                calinski = calinski_harabasz_score(X_dense[non_noise_mask], labels[non_noise_mask])
                davies = davies_bouldin_score(X_dense[non_noise_mask], labels[non_noise_mask])
            else:
                silhouette = calinski = davies = -1

            if has_true_labels:
                # –í–Ω–µ—à–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (–≤–∫–ª—é—á–∞—è —à—É–º –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä)
                ari = adjusted_rand_score(true_labels, labels)
                nmi = normalized_mutual_info_score(true_labels, labels)
                v_measure = v_measure_score(true_labels, labels)

                print(f"{eps:.1f}\t{min_samples}\t{n_clusters}\t\t{n_noise}\t{silhouette:.3f}\t\t"
                      f"{calinski:.3f}\t\t{davies:.3f}\t\t{ari:.3f}\t\t{nmi:.3f}\t\t{v_measure:.3f}")
            else:
                print(f"{eps:.1f}\t{min_samples}\t{n_clusters}\t\t{n_noise}\t{silhouette:.3f}\t\t"
                      f"{calinski:.3f}\t\t{davies:.3f}")

            results.append({
                'eps': eps,
                'min_samples': min_samples,
                'n_clusters': n_clusters,
                'n_noise': n_noise,
                'silhouette': silhouette,
                'calinski': calinski,
                'davies': davies,
                'ari': ari if has_true_labels else -1,
                'nmi': nmi if has_true_labels else -1,
                'v_measure': v_measure if has_true_labels else -1,
                'labels': labels
            })

    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    valid_results = [r for r in results if r['n_clusters'] >= 2]

    if valid_results:
        best_by_silhouette = max(valid_results, key=lambda x: x['silhouette'])
        if has_true_labels:
            best_by_ari = max(valid_results, key=lambda x: x['ari'])

        print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        print(f"   –ü–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –º–µ—Ç—Ä–∏–∫–∞–º: eps={best_by_silhouette['eps']:.1f}, "
              f"min_samples={best_by_silhouette['min_samples']} "
              f"(Silhouette: {best_by_silhouette['silhouette']:.3f}, "
              f"–ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {best_by_silhouette['n_clusters']})")
        if has_true_labels:
            print(f"   –ü–æ –≤–Ω–µ—à–Ω–∏–º –º–µ—Ç—Ä–∏–∫–∞–º: eps={best_by_ari['eps']:.1f}, "
                  f"min_samples={best_by_ari['min_samples']} "
                  f"(ARI: {best_by_ari['ari']:.3f}, "
                  f"–ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {best_by_ari['n_clusters']})")
    else:
        print(f"\n‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Å–æ–∑–¥–∞—é—â–∏—Ö —Ö–æ—Ç—è –±—ã 2 –∫–ª–∞—Å—Ç–µ—Ä–∞")
        best_by_silhouette = best_by_ari = None

    # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫–∏
    if has_true_labels and valid_results:
        _plot_dbscan_metrics(results, texts, true_labels, best_by_silhouette, best_by_ari)
    elif valid_results:
        _plot_dbscan_internal_metrics(results, best_by_silhouette)

    if has_true_labels and valid_results:
        return best_by_silhouette, best_by_ari
    elif valid_results:
        return best_by_silhouette
    else:
        return None, None if has_true_labels else None


def find_optimal_eps(texts, k=5):
    """
    –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ eps —Å –ø–æ–º–æ—â—å—é k-—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
    """
    vectorizer = TfidfVectorizer(max_features=500)
    X = vectorizer.fit_transform(texts)

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
    neighbors = NearestNeighbors(n_neighbors=k, metric='cosine')
    neighbors_fit = neighbors.fit(X)
    distances, indices = neighbors_fit.kneighbors(X)

    distances = np.sort(distances[:, k - 1], axis=0)

    plt.figure(figsize=(10, 6))
    plt.plot(distances)
    plt.xlabel('–¢–æ—á–∫–∏')
    plt.ylabel(f'{k}-–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ')
    plt.title('–ú–ï–¢–û–î –ö–û–õ–ï–ù–ê –î–õ–Ø –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø EPS\n(–∏—â–µ–º —Ç–æ—á–∫—É –∏–∑–≥–∏–±–∞)')
    plt.grid(True, alpha=0.3)

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ—á–∫–∏ –∏–∑–≥–∏–±–∞
    gradients = np.gradient(distances)
    elbow_point = np.argmax(gradients) + 1

    plt.axvline(x=elbow_point, color='red', linestyle='--',
                label=f'–¢–æ—á–∫–∞ –∏–∑–≥–∏–±–∞ (eps ‚âà {distances[elbow_point]:.3f})')
    plt.legend()
    plt.show()

    recommended_eps = distances[elbow_point]
    print(f"üéØ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ô EPS: {recommended_eps:.3f}")

    return recommended_eps


def _plot_dbscan_metrics(results, texts, true_labels, best_silhouette, best_ari):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è DBSCAN —Å–æ –≤—Å–µ–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
    eps_values = sorted(set(r['eps'] for r in results))
    min_samples_values = sorted(set(r['min_samples'] for r in results))

    # –ì—Ä–∞—Ñ–∏–∫ 1: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ —à—É–º–∞
    for min_samples in min_samples_values:
        cluster_data = [r for r in results if r['min_samples'] == min_samples]
        eps_vals = [r['eps'] for r in cluster_data]
        cluster_vals = [r['n_clusters'] for r in cluster_data]
        noise_vals = [r['n_noise'] for r in cluster_data]

        ax1.plot(eps_vals, cluster_vals, 'o-', linewidth=2,
                 label=f'–ö–ª–∞—Å—Ç–µ—Ä—ã (min_samples={min_samples})')
        ax1.plot(eps_vals, noise_vals, 'o--', linewidth=2,
                 label=f'–®—É–º (min_samples={min_samples})')

    if best_silhouette:
        ax1.axvline(x=best_silhouette['eps'], color='blue', linestyle='--', alpha=0.7,
                    label=f'–õ—É—á—à–µ–µ eps={best_silhouette["eps"]:.1f} (–≤–Ω—É—Ç—Ä.)')
    if best_ari:
        ax1.axvline(x=best_ari['eps'], color='red', linestyle='--', alpha=0.7,
                    label=f'–õ—É—á—à–µ–µ eps={best_ari["eps"]:.1f} (–≤–Ω–µ—à.)')

    ax1.set_xlabel('Eps')
    ax1.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
    ax1.set_title('DBSCAN: –ö–õ–ê–°–¢–ï–†–´ –ò –®–£–ú\n(‚Üë –∫–ª–∞—Å—Ç–µ—Ä—ã, ‚Üì —à—É–º)')
    ax1.grid(True, alpha=0.3)
    ax1.legend()

    # –ì—Ä–∞—Ñ–∏–∫ 2: –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    for min_samples in min_samples_values:
        cluster_data = [r for r in results if r['min_samples'] == min_samples and r['silhouette'] > -1]
        if cluster_data:
            eps_vals = [r['eps'] for r in cluster_data]
            silhouette_vals = [r['silhouette'] for r in cluster_data]
            ax2.plot(eps_vals, silhouette_vals, 'o-', linewidth=2,
                     label=f'Silhouette (min_samples={min_samples})')

    ax2.set_xlabel('Eps')
    ax2.set_ylabel('Silhouette Score')
    ax2.set_title('DBSCAN: SILHOUETTE SCORE\n(‚Üë –ª—É—á—à–µ)')
    ax2.grid(True, alpha=0.3)
    ax2.legend()

    # –ì—Ä–∞—Ñ–∏–∫ 3: –í–Ω–µ—à–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    for min_samples in min_samples_values:
        cluster_data = [r for r in results if r['min_samples'] == min_samples and r['ari'] > -1]
        if cluster_data:
            eps_vals = [r['eps'] for r in cluster_data]
            ari_vals = [r['ari'] for r in cluster_data]
            ax3.plot(eps_vals, ari_vals, 'o-', linewidth=2,
                     label=f'ARI (min_samples={min_samples})')

    ax3.set_xlabel('Eps')
    ax3.set_ylabel('ARI Score')
    ax3.set_title('DBSCAN: ADJUSTED RAND INDEX\n(‚Üë –ª—É—á—à–µ)')
    ax3.grid(True, alpha=0.3)
    ax3.legend()

    # –ì—Ä–∞—Ñ–∏–∫ 4: –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    times = []
    vectorizer = TfidfVectorizer(max_features=500)
    X = vectorizer.fit_transform(texts)

    test_eps = [0.1, 0.2, 0.3, 0.4, 0.5]
    for eps in test_eps:
        dbscan = DBSCAN(eps=eps, min_samples=3, metric='cosine')
        start_time = time.time()
        dbscan.fit(X)
        times.append(time.time() - start_time)

    ax4.plot(test_eps, times, color='purple', linestyle='-', marker='o', linewidth=2)
    ax4.set_xlabel('Eps')
    ax4.set_ylabel('–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (—Å–µ–∫—É–Ω–¥—ã)')
    ax4.set_title('DBSCAN: –í–†–ï–ú–Ø –í–´–ü–û–õ–ù–ï–ù–ò–Ø')
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()


def _plot_dbscan_internal_metrics(results, best_params):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    min_samples_values = sorted(set(r['min_samples'] for r in results))

    # –ì—Ä–∞—Ñ–∏–∫ 1: Silhouette Score
    for min_samples in min_samples_values:
        cluster_data = [r for r in results if r['min_samples'] == min_samples and r['silhouette'] > -1]
        if cluster_data:
            eps_vals = [r['eps'] for r in cluster_data]
            silhouette_vals = [r['silhouette'] for r in cluster_data]
            ax1.plot(eps_vals, silhouette_vals, 'o-', linewidth=2,
                     label=f'min_samples={min_samples}')

    if best_params:
        ax1.axvline(x=best_params['eps'], color='red', linestyle='--', alpha=0.7,
                    label=f'–õ—É—á—à–µ–µ eps={best_params["eps"]:.1f}')

    ax1.set_xlabel('Eps')
    ax1.set_ylabel('Silhouette Score')
    ax1.set_title('DBSCAN: SILHOUETTE SCORE\n(‚Üë –ª—É—á—à–µ)')
    ax1.grid(True, alpha=0.3)
    ax1.legend()

    # –ì—Ä–∞—Ñ–∏–∫ 2: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    for min_samples in min_samples_values:
        cluster_data = [r for r in results if r['min_samples'] == min_samples]
        eps_vals = [r['eps'] for r in cluster_data]
        cluster_vals = [r['n_clusters'] for r in cluster_data]
        ax2.plot(eps_vals, cluster_vals, 'o-', linewidth=2,
                 label=f'min_samples={min_samples}')

    ax2.set_xlabel('Eps')
    ax2.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')
    ax2.set_title('DBSCAN: –ö–û–õ–ò–ß–ï–°–¢–í–û –ö–õ–ê–°–¢–ï–†–û–í')
    ax2.grid(True, alpha=0.3)
    ax2.legend()

    plt.tight_layout()
    plt.show()


# –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ DBSCAN
def simple_dbscan_cluster(texts, eps=0.3, min_samples=3):
    """
    –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —Å DBSCAN
    """
    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
    vectorizer = TfidfVectorizer(max_features=500)
    X = vectorizer.fit_transform(texts)
    X_dense = X.toarray()

    # DBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    dbscan = DBSCAN(
        eps=eps,
        min_samples=min_samples,
        metric='cosine'  # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤
    )
    labels = dbscan.fit_predict(X)

    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    unique_labels = set(labels)
    n_clusters = len(unique_labels) - (1 if -1 in labels else 0)
    n_noise = np.sum(labels == -1)

    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ (–±–µ–∑ —à—É–º–∞)
    non_noise_mask = labels != -1

    if n_clusters >= 2 and np.sum(non_noise_mask) >= 2:
        metrics = {
            'silhouette': silhouette_score(X_dense[non_noise_mask], labels[non_noise_mask]),
            'calinski_harabasz': calinski_harabasz_score(X_dense[non_noise_mask], labels[non_noise_mask]),
            'davies_bouldin': davies_bouldin_score(X_dense[non_noise_mask], labels[non_noise_mask]),
            'n_clusters': n_clusters,
            'n_noise': n_noise
        }
    else:
        metrics = {
            'silhouette': -1,
            'calinski_harabasz': -1,
            'davies_bouldin': -1,
            'n_clusters': n_clusters,
            'n_noise': n_noise
        }

    # –ü—Ä–æ—Å—Ç–æ–π –≤—ã–≤–æ–¥
    print(f"üìä DBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤:")
    print(f"‚öôÔ∏è  –ü–ê–†–ê–ú–ï–¢–†–´: eps={eps}, min_samples={min_samples}")
    print(f"üéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}")
    print(f"   –®—É–º–æ–≤—ã—Ö —Ç–æ—á–µ–∫: {n_noise} ({n_noise / len(texts) * 100:.1f}%)")

    if metrics['silhouette'] > -1:
        print(f"   Silhouette Score: {metrics['silhouette']:.3f}")
        print(f"   Calinski-Harabasz: {metrics['calinski_harabasz']:.3f}")
        print(f"   Davies-Bouldin: {metrics['davies_bouldin']:.3f}")

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è Silhouette Score
        silhouette_val = metrics['silhouette']
        if silhouette_val > 0.7:
            interpretation = "–û—Ç–ª–∏—á–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ"
        elif silhouette_val > 0.5:
            interpretation = "–†–∞–∑—É–º–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ"
        elif silhouette_val > 0.25:
            interpretation = "–°–ª–∞–±–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ"
        else:
            interpretation = "–ù–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è"
        print(f"   –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: {interpretation}")
    else:
        print("   ‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫")

    print(f"\nüîç –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ö–õ–ê–°–¢–ï–†–ê–•:")
    for label in sorted(unique_labels):
        if label == -1:
            print(f"üî∏ –®—É–º: {n_noise} —Ç–µ–∫—Å—Ç–æ–≤")
            continue

        cluster_texts = [texts[j] for j, lbl in enumerate(labels) if lbl == label]
        print(f"üî∏ –ö–ª–∞—Å—Ç–µ—Ä {label}: {len(cluster_texts)} —Ç–µ–∫—Å—Ç–æ–≤")
        if len(cluster_texts) > 0:
            for text in cluster_texts[:2]:
                print(f"   - {text[:60]}..." if len(text) > 60 else f"   - {text}")
            if len(cluster_texts) > 2:
                print(f"   ... –∏ –µ—â–µ {len(cluster_texts) - 2} —Ç–µ–∫—Å—Ç–æ–≤")
        print()

    return labels, metrics


if __name__ == "__main__":
    texts = get_texts()
    true_labels = get_labels()

    print("üöÄ DBSCAN –î–õ–Ø –¢–ï–ö–°–¢–û–í")
    print("=" * 50)

    # –®–∞–≥ 1: –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ eps
    print("üéØ –®–ê–ì 1: –ü–û–ò–°–ö –û–ü–¢–ò–ú–ê–õ–¨–ù–û–ì–û EPS")
    recommended_eps = find_optimal_eps(texts, k=5)

    # –®–∞–≥ 2: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –≤–Ω–µ—à–Ω–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    print("\nüéØ –®–ê–ì 2: –ü–û–õ–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –° –ú–ï–¢–†–ò–ö–ê–ú–ò")
    best_by_int, best_by_ext = compare_dbscan_parameters(
        texts, true_labels, max_eps=0.5, eps_step=0.1, min_samples_range=[2, 3, 5]
    )

    # –®–∞–≥ 3: –ë—ã—Å—Ç—Ä–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    print("\nüéØ –®–ê–ì 3: –ë–´–°–¢–†–ê–Ø –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø")
    if best_by_int:
        labels, metrics = simple_dbscan_cluster(
            texts,
            eps=best_by_int['eps'],
            min_samples=best_by_int['min_samples']
        )
    else:
        labels, metrics = simple_dbscan_cluster(
            texts,
            eps=recommended_eps,
            min_samples=3
        )