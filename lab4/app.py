import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from collections import Counter
import plotly.express as px
import plotly.graph_objects as go
from wordcloud import WordCloud
import time
import warnings

from util.decribe import get_texts_app

warnings.filterwarnings('ignore')

try:
    import umap

    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False

try:
    import hdbscan

    HDBSCAN_AVAILABLE = True
except ImportError:
    HDBSCAN_AVAILABLE = False


class TextClusterApp:
    def __init__(self):
        self.texts = []
        self.vectorizer = None
        self.X = None
        self.feature_names = []

    def load_sample_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
        sample_texts = get_texts_app()
        return sample_texts

    def setup_vectorizer(self, method='tfidf', **kwargs):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞"""
        if method == 'tfidf':
            self.vectorizer = TfidfVectorizer(**kwargs)
        elif method == 'count':
            self.vectorizer = CountVectorizer(**kwargs)
        return self.vectorizer

    def vectorize_texts(self, texts, method='tfidf', max_features=2000, ngram_range=(1, 2)):
        """–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤"""
        vectorizer_params = {
            'max_features': max_features,
            'ngram_range': ngram_range,
            'stop_words': None,
            'min_df': 2,
            'max_df': 0.8
        }

        self.setup_vectorizer(method, **vectorizer_params)
        self.X = self.vectorizer.fit_transform(texts)
        self.feature_names = self.vectorizer.get_feature_names_out()
        return self.X

    def perform_clustering(self, algorithm, X, **params):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        start_time = time.time()

        if algorithm == 'KMeans':
            model = KMeans(
                n_clusters=params.get('n_clusters', 5),
                random_state=params.get('random_state', 42)
            )
            labels = model.fit_predict(X)

        elif algorithm == 'MiniBatchKMeans':
            model = MiniBatchKMeans(
                n_clusters=params.get('n_clusters', 5),
                random_state=params.get('random_state', 42),
                batch_size=512
            )
            labels = model.fit_predict(X)

        elif algorithm == 'DBSCAN':
            model = DBSCAN(
                eps=params.get('eps', 0.5),
                min_samples=params.get('min_samples', 5)
            )
            labels = model.fit_predict(X)

        elif algorithm == 'HDBSCAN' and HDBSCAN_AVAILABLE:
            model = hdbscan.HDBSCAN(
                min_cluster_size=params.get('min_cluster_size', 10),
                min_samples=params.get('min_samples', 5)
            )
            labels = model.fit_predict(X.toarray() if hasattr(X, 'toarray') else X)

        elif algorithm == 'GaussianMixture':
            model = GaussianMixture(
                n_components=params.get('n_clusters', 5),
                random_state=params.get('random_state', 42)
            )
            if hasattr(X, 'toarray'):
                X_dense = X.toarray()
            else:
                X_dense = X
            labels = model.fit_predict(X_dense)

        elif algorithm == 'SpectralClustering':
            spectral_params = {
                'n_clusters': params.get('n_clusters', 5),
                'random_state': params.get('random_state', 42),
                'affinity': params.get('affinity', 'rbf')
            }
            if params.get('affinity') == 'nearest_neighbors':
                spectral_params['n_neighbors'] = params.get('n_neighbors', 10)

            model = SpectralClustering(**spectral_params)

            # –î–ª—è SpectralClustering –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–ª–æ—Ç–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö affinity
            if params.get('affinity') == 'cosine' and hasattr(X, 'toarray'):
                X_used = X
            else:
                X_used = X.toarray() if hasattr(X, 'toarray') else X
            labels = model.fit_predict(X_used)

        elif algorithm == 'AgglomerativeClustering':
            model = AgglomerativeClustering(
                n_clusters=params.get('n_clusters', 5),
                linkage=params.get('linkage', 'ward')
            )
            # –î–ª—è ward linkage –Ω—É–∂–Ω–∞ –ø–ª–æ—Ç–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞
            if params.get('linkage') == 'ward':
                X_used = X.toarray() if hasattr(X, 'toarray') else X
            else:
                X_used = X
            labels = model.fit_predict(X_used)

        else:
            raise ValueError(f"–ê–ª–≥–æ—Ä–∏—Ç–º {algorithm} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")

        execution_time = time.time() - start_time
        return labels, model, execution_time

    def calculate_metrics(self, X, labels):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        metrics = {}

        if hasattr(X, 'toarray'):
            X_dense = X.toarray()
        else:
            X_dense = X

        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —à—É–º (-1) –ø—Ä–∏ –ø–æ–¥—Å—á–µ—Ç–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        valid_labels = labels[labels != -1]
        n_clusters = len(np.unique(valid_labels)) if len(valid_labels) > 0 else 0

        if n_clusters > 1 and len(valid_labels) > 1:
            try:
                metrics['silhouette'] = silhouette_score(X_dense[labels != -1], valid_labels)
            except:
                metrics['silhouette'] = -1

            try:
                metrics['calinski_harabasz'] = calinski_harabasz_score(X_dense[labels != -1], valid_labels)
            except:
                metrics['calinski_harabasz'] = -1

            try:
                metrics['davies_bouldin'] = davies_bouldin_score(X_dense[labels != -1], valid_labels)
            except:
                metrics['davies_bouldin'] = float('inf')
        else:
            metrics['silhouette'] = -1
            metrics['calinski_harabasz'] = -1
            metrics['davies_bouldin'] = float('inf')

        metrics['n_clusters'] = n_clusters
        metrics['n_noise'] = np.sum(labels == -1)
        metrics['n_points'] = len(labels)

        return metrics

    def get_cluster_keywords(self, labels, n_words=10):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
        if self.X is None or len(self.feature_names) == 0:
            return {}

        unique_labels = np.unique(labels)
        cluster_keywords = {}

        for cluster_id in unique_labels:
            if cluster_id == -1:  # –®—É–º
                continue

            cluster_indices = np.where(labels == cluster_id)[0]

            if len(cluster_indices) == 0:
                cluster_keywords[cluster_id] = []
                continue

            # –°—Ä–µ–¥–Ω–∏–µ TF-IDF –≤–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞
            if hasattr(self.X, 'toarray'):
                cluster_tfidf = self.X[cluster_indices].mean(axis=0).A1
            else:
                cluster_tfidf = self.X[cluster_indices].mean(axis=0)

            # –¢–æ–ø-N —Å–ª–æ–≤
            top_indices = np.argsort(cluster_tfidf)[::-1][:n_words]
            top_words = [(self.feature_names[i], cluster_tfidf[i])
                         for i in top_indices if cluster_tfidf[i] > 0]

            cluster_keywords[cluster_id] = top_words

        return cluster_keywords

    def create_wordclouds(self, labels, n_words=20):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±–ª–∞–∫–æ–≤ —Å–ª–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
        cluster_keywords = self.get_cluster_keywords(labels, n_words)
        figs = []

        for cluster_id, words_weights in cluster_keywords.items():
            if not words_weights:
                continue

            word_freq = {word: weight for word, weight in words_weights}
            wordcloud = WordCloud(
                width=400,
                height=300,
                background_color='white',
                colormap='viridis'
            ).generate_from_frequencies(word_freq)

            fig, ax = plt.subplots(figsize=(10, 6))
            ax.imshow(wordcloud, interpolation='bilinear')
            ax.set_title(f'–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}', fontsize=16, fontweight='bold')
            ax.axis('off')
            figs.append((cluster_id, fig))

        return figs

    def reduce_dimensionality(self, X, method='umap', n_components=2):
        """–£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏"""
        if hasattr(X, 'toarray'):
            X_dense = X.toarray()
        else:
            X_dense = X

        if method == 'umap' and UMAP_AVAILABLE:
            reducer = umap.UMAP(
                n_components=n_components,
                random_state=42,
                n_neighbors=15,
                min_dist=0.1,
                metric='cosine'
            )
            embedding = reducer.fit_transform(X_dense)

        elif method == 'tsne':
            reducer = TSNE(
                n_components=n_components,
                random_state=42,
                perplexity=min(30, len(X_dense) - 1)
            )
            embedding = reducer.fit_transform(X_dense)

        else:  # PCA –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_dense)
            reducer = PCA(n_components=n_components, random_state=42)
            embedding = reducer.fit_transform(X_scaled)

        return embedding


def main():
    st.set_page_config(
        page_title="Text Clustering Analysis",
        page_icon="üìä",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.title("üîç –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤")
    st.markdown("---")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    if 'app' not in st.session_state:
        st.session_state.app = TextClusterApp()

    app = st.session_state.app

    # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–∫
    with st.sidebar:
        st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        st.subheader("üìÅ –î–∞–Ω–Ω—ã–µ")
        data_source = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö:", ["–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö", "–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª"])

        if data_source == "–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö":
            app.texts = app.load_sample_data()
            st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(app.texts)} –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤")
        else:
            uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV –∏–ª–∏ TXT —Ñ–∞–π–ª", type=['csv', 'txt'])
            if uploaded_file:
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                    text_column = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É —Å —Ç–µ–∫—Å—Ç–æ–º", df.columns)
                    app.texts = df[text_column].dropna().tolist()
                else:
                    content = uploaded_file.read().decode('utf-8')
                    app.texts = [line.strip() for line in content.split('\n') if line.strip()]
                st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(app.texts)} —Ç–µ–∫—Å—Ç–æ–≤")

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        st.subheader("üî§ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è")
        vectorization_method = st.selectbox(
            "–ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏:",
            ['tfidf', 'count']
        )

        max_features = st.slider("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:", 100, 5000, 2000)
        ngram_range = st.selectbox(
            "N-gram –¥–∏–∞–ø–∞–∑–æ–Ω:",
            [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3)],
            format_func=lambda x: f"{x[0]}-{x[1]}"
        )

        # –í—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        st.subheader("üéØ –ê–ª–≥–æ—Ä–∏—Ç–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
        algorithm = st.selectbox(
            "–ê–ª–≥–æ—Ä–∏—Ç–º:",
            ['KMeans', 'MiniBatchKMeans', 'DBSCAN', 'HDBSCAN',
             'GaussianMixture', 'SpectralClustering', 'AgglomerativeClustering']
        )

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
        st.subheader("üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞")

        if algorithm in ['KMeans', 'MiniBatchKMeans', 'SpectralClustering', 'AgglomerativeClustering']:
            n_clusters = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:", 2, 20, 5)

        if algorithm == 'GaussianMixture':
            n_components = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç:", 2, 20, 5)

        if algorithm == 'DBSCAN':
            eps = st.slider("EPS:", 0.1, 2.0, 0.5, 0.1)
            min_samples = st.slider("Min Samples:", 2, 20, 5)

        if algorithm == 'HDBSCAN' and HDBSCAN_AVAILABLE:
            min_cluster_size = st.slider("Min Cluster Size:", 2, 50, 10)
            min_samples = st.slider("Min Samples:", 1, 20, 5)

        if algorithm == 'SpectralClustering':
            affinity = st.selectbox("Affinity:", ['rbf', 'nearest_neighbors', 'cosine'])
            if affinity == 'nearest_neighbors':
                n_neighbors = st.slider("N Neighbors:", 5, 50, 10)

        if algorithm == 'AgglomerativeClustering':
            linkage = st.selectbox("Linkage:", ['ward', 'complete', 'average', 'single'])
            if linkage == 'ward':
                st.info("Ward linkage —Ç—Ä–µ–±—É–µ—Ç –µ–≤–∫–ª–∏–¥–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏")

        # –ú–µ—Ç–æ–¥ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        st.subheader("üìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è")
        viz_method = st.selectbox(
            "–ú–µ—Ç–æ–¥ —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏:",
            ['umap', 'tsne', 'pca']
        )

    # –û—Å–Ω–æ–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
    if not app.texts:
        st.warning("‚ö†Ô∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∏–º–µ—Ä—ã")
        return

    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
    with st.spinner("–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤..."):
        X = app.vectorize_texts(
            app.texts,
            method=vectorization_method,
            max_features=max_features,
            ngram_range=ngram_range
        )

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
    algorithm_params = {}

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º–∞
    if algorithm in ['KMeans', 'MiniBatchKMeans', 'SpectralClustering', 'AgglomerativeClustering']:
        algorithm_params['n_clusters'] = n_clusters
        if algorithm != 'AgglomerativeClustering':  # AgglomerativeClustering –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç random_state
            algorithm_params['random_state'] = 42

    elif algorithm == 'GaussianMixture':
        algorithm_params['n_components'] = n_components
        algorithm_params['random_state'] = 42

    elif algorithm == 'DBSCAN':
        algorithm_params['eps'] = eps
        algorithm_params['min_samples'] = min_samples

    elif algorithm == 'HDBSCAN' and HDBSCAN_AVAILABLE:
        algorithm_params['min_cluster_size'] = min_cluster_size
        algorithm_params['min_samples'] = min_samples

    if algorithm == 'SpectralClustering':
        algorithm_params['affinity'] = affinity
        if affinity == 'nearest_neighbors':
            algorithm_params['n_neighbors'] = n_neighbors

    if algorithm == 'AgglomerativeClustering':
        algorithm_params['linkage'] = linkage

    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    col1, col2 = st.columns([2, 1])

    with col1:
        st.header("üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")

        if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é", type="primary"):
            with st.spinner("–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è..."):
                try:
                    labels, model, exec_time = app.perform_clustering(algorithm, X, **algorithm_params)

                    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
                    metrics = app.calculate_metrics(X, labels)

                    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    st.success(f"–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {exec_time:.2f} —Å–µ–∫—É–Ω–¥")

                    # –ú–µ—Ç—Ä–∏–∫–∏
                    col_metric1, col_metric2, col_metric3, col_metric4 = st.columns(4)

                    with col_metric1:
                        st.metric("–ö–ª–∞—Å—Ç–µ—Ä–æ–≤", metrics['n_clusters'])
                    with col_metric2:
                        st.metric("Silhouette Score", f"{metrics['silhouette']:.3f}")
                    with col_metric3:
                        st.metric("Calinski-Harabasz", f"{metrics['calinski_harabasz']:.1f}")
                    with col_metric4:
                        st.metric("–®—É–º–æ–≤—ã–µ —Ç–æ—á–∫–∏", metrics['n_noise'])

                    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
                    st.subheader("üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")

                    with st.spinner("–°—Ç—Ä–æ–∏–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é..."):
                        embedding = app.reduce_dimensionality(X, method=viz_method)

                        # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                        viz_df = pd.DataFrame({
                            'x': embedding[:, 0],
                            'y': embedding[:, 1],
                            'cluster': labels,
                            'text': app.texts
                        })

                        # Plotly scatter plot
                        fig = px.scatter(
                            viz_df,
                            x='x',
                            y='y',
                            color='cluster',
                            hover_data=['text'],
                            title=f'{algorithm} Clustering - {viz_method.upper()} –ø—Ä–æ–µ–∫—Ü–∏—è',
                            color_continuous_scale='viridis'
                        )

                        fig.update_traces(
                            marker=dict(size=8, opacity=0.7),
                            selector=dict(mode='markers')
                        )

                        st.plotly_chart(fig, use_container_width=True)

                    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                    st.subheader("üî§ –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")

                    cluster_keywords = app.get_cluster_keywords(labels, n_words=10)

                    for cluster_id, keywords in cluster_keywords.items():
                        with st.expander(
                                f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id} ({len([x for x in labels if x == cluster_id])} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)"):
                            if keywords:
                                keywords_df = pd.DataFrame(keywords, columns=['–°–ª–æ–≤–æ', '–í–µ—Å'])
                                st.dataframe(
                                    keywords_df.style.format({'–í–µ—Å': '{:.4f}'}),
                                    use_container_width=True
                                )
                            else:
                                st.info("–ù–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")

                    # –û–±–ª–∞–∫–∞ —Å–ª–æ–≤
                    st.subheader("‚òÅÔ∏è –û–±–ª–∞–∫–∞ —Å–ª–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
                    wordcloud_figs = app.create_wordclouds(labels)

                    if wordcloud_figs:
                        cols = st.columns(2)
                        for idx, (cluster_id, fig) in enumerate(wordcloud_figs):
                            with cols[idx % 2]:
                                st.pyplot(fig)

                    # –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                    st.subheader("üìã –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")

                    # –í–∫–ª—é—á–∞–µ–º —à—É–º (-1) –≤ —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                    all_clusters = sorted(np.unique(labels))
                    selected_cluster = st.selectbox(
                        "–í—ã–±–µ—Ä–∏—Ç–µ –∫–ª–∞—Å—Ç–µ—Ä –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞:",
                        all_clusters,
                        format_func=lambda x: f"–ö–ª–∞—Å—Ç–µ—Ä {x}" if x != -1 else "–®—É–º (-1)"
                    )

                    cluster_texts = [text for i, text in enumerate(app.texts) if labels[i] == selected_cluster]

                    if cluster_texts:
                        st.write(f"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ {selected_cluster}: {len(cluster_texts)}")

                        for i, text in enumerate(cluster_texts[:10]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                            with st.expander(f"–î–æ–∫—É–º–µ–Ω—Ç {i + 1}"):
                                st.write(text)

                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    st.subheader("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

                    results_df = pd.DataFrame({
                        'text': app.texts,
                        'cluster': labels
                    })

                    csv = results_df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        label="–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã CSV",
                        data=csv,
                        file_name=f"clustering_results_{algorithm}.csv",
                        mime="text/csv"
                    )

                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {str(e)}")
                    st.error("–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –æ—à–∏–±–∫–∏:")
                    st.code(str(e))

    with col2:
        st.header("‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")

        st.subheader("–û –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö:")

        algorithm_info = {
            'KMeans': "‚Ä¢ –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è\n‚Ä¢ –¢—Ä–µ–±—É–µ—Ç —É–∫–∞–∑–∞–Ω–∏—è k\n‚Ä¢ –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –≤—ã–±—Ä–æ—Å–∞–º",
            'MiniBatchKMeans': "‚Ä¢ –ë—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è KMeans\n‚Ä¢ –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n‚Ä¢ –ü—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ",
            'DBSCAN': "‚Ä¢ –ü–ª–æ—Ç–Ω–æ—Å—Ç–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è\n‚Ä¢ –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç —à—É–º\n‚Ä¢ –ù–µ —Ç—Ä–µ–±—É–µ—Ç —É–∫–∞–∑–∞–Ω–∏—è k",
            'HDBSCAN': "‚Ä¢ –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è DBSCAN\n‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n‚Ä¢ –£—Å—Ç–æ–π—á–∏–≤ –∫ —à—É–º—É",
            'GaussianMixture': "‚Ä¢ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å\n‚Ä¢ –ú—è–≥–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è\n‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç n_components –≤–º–µ—Å—Ç–æ n_clusters",
            'SpectralClustering': "‚Ä¢ –ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–µ–∫—Ç—Ä–∞ –≥—Ä–∞—Ñ–æ–≤\n‚Ä¢ –†–∞–±–æ—Ç–∞–µ—Ç —Å –Ω–µ–≤—ã–ø—É–∫–ª—ã–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏\n‚Ä¢ –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ —Å–ª–æ–∂–Ω—ã–π",
            'AgglomerativeClustering': "‚Ä¢ –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è\n‚Ä¢ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º\n‚Ä¢ –†–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–≤—è–∑–∏"
        }

        st.info(algorithm_info.get(algorithm, "–í—ã–±–µ—Ä–∏—Ç–µ –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"))

        st.subheader("–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:")
        st.markdown("""
        - **Silhouette**: [-1, 1] - —á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ
        - **Calinski-Harabasz**: [0, ‚àû] - —á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ  
        - **Davies-Bouldin**: [0, ‚àû] - —á–µ–º –Ω–∏–∂–µ, —Ç–µ–º –ª—É—á—à–µ
        """)

        if not UMAP_AVAILABLE:
            st.warning("""
            ‚ö†Ô∏è UMAP –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. 
            –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: `pip install umap-learn`
            """)

        if not HDBSCAN_AVAILABLE and algorithm == 'HDBSCAN':
            st.error("""
            ‚ùå HDBSCAN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.
            –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: `pip install hdbscan`
            """)


if __name__ == "__main__":
    main()