from collections import Counter

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np

try:
    import umap

    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False
    print("‚ö†Ô∏è  UMAP –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install umap-learn")

from util.decribe import get_labels, get_texts


class SimpleClusterInterpreter:
    """
    –ü—Ä–æ—Å—Ç–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è TF-IDF
    """

    def __init__(self, texts, vectorizer=None):
        self.texts = texts
        self.vectorizer = vectorizer if vectorizer else TfidfVectorizer(max_features=500)
        self.X = None
        self.feature_names = None

    def fit_vectorizer(self):
        """–û–±—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞"""
        self.X = self.vectorizer.fit_transform(self.texts)
        self.feature_names = self.vectorizer.get_feature_names_out()
        return self.X

    def get_cluster_keywords(self, labels, n_words=10):
        """
        –¢–æ–ø-N –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø–æ TF-IDF
        """
        if self.X is None:
            self.fit_vectorizer()

        unique_labels = np.unique(labels)
        cluster_keywords = {}

        for cluster_id in unique_labels:
            # –ò–Ω–¥–µ–∫—Å—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
            cluster_indices = np.where(labels == cluster_id)[0]

            if len(cluster_indices) == 0:
                cluster_keywords[cluster_id] = []
                continue

            # –°—Ä–µ–¥–Ω–∏–µ TF-IDF –≤–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞
            cluster_tfidf = self.X[cluster_indices].mean(axis=0)
            cluster_tfidf = np.array(cluster_tfidf).flatten()

            # –¢–æ–ø-N —Å–ª–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ –≤–µ—Å–∞–º–∏
            top_indices = np.argsort(cluster_tfidf)[::-1][:n_words]
            top_words = [(self.feature_names[i], cluster_tfidf[i])
                         for i in top_indices if cluster_tfidf[i] > 0]

            cluster_keywords[cluster_id] = top_words

        return cluster_keywords

    def get_most_frequent_words(self, labels, n_words=10):
        """
        –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
        """
        unique_labels = np.unique(labels)
        cluster_freq_words = {}

        for cluster_id in unique_labels:
            cluster_indices = np.where(labels == cluster_id)[0]
            cluster_texts = [self.texts[i] for i in cluster_indices]

            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞
            all_words = []
            for text in cluster_texts:
                words = text.lower().split()
                words = [word for word in words if len(word) > 2]  # —É–±–∏—Ä–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                all_words.extend(words)

            # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
            word_counts = Counter(all_words)
            cluster_freq_words[cluster_id] = word_counts.most_common(n_words)

        return cluster_freq_words

    def print_cluster_info(self, labels, n_words=8):
        """
        –ö—Ä–∞—Å–∏–≤–∞—è –ø–µ—á–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
        """
        unique_labels, counts = np.unique(labels, return_counts=True)

        print("üìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ö–õ–ê–°–¢–ï–†–ê–•:")
        print("=" * 60)

        # –¢–æ–ø —Å–ª–æ–≤–∞ –ø–æ TF-IDF
        tfidf_keywords = self.get_cluster_keywords(labels, n_words)

        # –ß–∞—Å—Ç–æ—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
        freq_words = self.get_most_frequent_words(labels, n_words)

        for cluster_id in unique_labels:
            count = counts[unique_labels == cluster_id][0]
            percentage = (count / len(labels)) * 100

            print(f"\nüî∏ –ö–õ–ê–°–¢–ï–† {cluster_id} ({count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {percentage:.1f}%):")

            # TF-IDF –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if cluster_id in tfidf_keywords:
                tfidf_words = [word for word, weight in tfidf_keywords[cluster_id][:5]]
                print(f"   üìà –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (TF-IDF): {', '.join(tfidf_words)}")

            # –ß–∞—Å—Ç–æ—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
            if cluster_id in freq_words:
                freq_words_list = [word for word, count in freq_words[cluster_id][:5]]
                print(f"   üìä –ß–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞: {', '.join(freq_words_list)}")

            # –ü—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            cluster_indices = np.where(labels == cluster_id)[0]
            if len(cluster_indices) > 0:
                sample_text = self.texts[cluster_indices[0]]
                preview = sample_text[:100] + "..." if len(sample_text) > 100 else sample_text
                print(f"   üìÑ –ü—Ä–∏–º–µ—Ä: {preview}")

    def plot_keywords_barchart(self, labels, n_words=6):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        """
        cluster_keywords = self.get_cluster_keywords(labels, n_words)
        n_clusters = len(cluster_keywords)

        fig, axes = plt.subplots(1, n_clusters, figsize=(4 * n_clusters, 5))
        if n_clusters == 1:
            axes = [axes]

        for idx, (cluster_id, words_weights) in enumerate(cluster_keywords.items()):
            if not words_weights:
                continue

            words, weights = zip(*words_weights)

            axes[idx].barh(range(len(words)), weights, color=f'C{idx}', alpha=0.7)
            axes[idx].set_yticks(range(len(words)))
            axes[idx].set_yticklabels(words, fontsize=9)
            axes[idx].set_title(f'–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}')
            axes[idx].set_xlabel('TF-IDF –≤–µ—Å')

        plt.suptitle('–ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê –ö–õ–ê–°–¢–ï–†–û–í', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.show()

    def visualize_umap(self, labels, title="Gaussian Mixture - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (UMAP)"):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ 2D —Å –ø–æ–º–æ—â—å—é UMAP
        """
        if not UMAP_AVAILABLE:
            print("‚ùå UMAP –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: pip install umap-learn")
            return None

        if self.X is None:
            self.fit_vectorizer()

        print("üîÑ –°—Ç—Ä–æ–∏–º UMAP –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é...")

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–ª–æ—Ç–Ω—ã–π –º–∞—Å—Å–∏–≤ –¥–ª—è UMAP
        X_dense = self.X.toarray()

        # –°–æ–∑–¥–∞–µ–º UMAP —Ä–µ–¥—É–∫—Ç–æ—Ä
        reducer = umap.UMAP(
            n_components=2,
            random_state=42,
            n_neighbors=15,
            min_dist=0.1,
            metric='cosine'
        )

        # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        embedding_2d = reducer.fit_transform(X_dense)

        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
        plt.figure(figsize=(12, 8))

        # –†–∞–∑–Ω—ã–µ —Ü–≤–µ—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        unique_labels = np.unique(labels)
        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))

        for i, cluster_id in enumerate(unique_labels):
            mask = labels == cluster_id
            plt.scatter(embedding_2d[mask, 0], embedding_2d[mask, 1],
                        c=[colors[i]], alpha=0.7, s=30, label=f'–ö–ª–∞—Å—Ç–µ—Ä {cluster_id} ({np.sum(mask)} —Ç–æ—á–µ–∫)')

        plt.title(title, fontsize=14, fontweight='bold')
        plt.xlabel('UMAP dimension 1')
        plt.ylabel('UMAP dimension 2')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

        return embedding_2d


def simple_gmm_cluster(texts, n_components=3, covariance_type='full'):
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è GaussianMixture —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π
    """
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä–∞
    interpreter = SimpleClusterInterpreter(texts)
    X = interpreter.fit_vectorizer()
    X_dense = X.toarray()

    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è GaussianMixture
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_dense)

    print(f"üîÑ –ó–∞–ø—É—Å–∫ GaussianMixture —Å {n_components} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏...")

    # GaussianMixture –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    gmm = GaussianMixture(
        n_components=n_components,
        covariance_type=covariance_type,
        random_state=42,
        max_iter=100
    )

    # –ú—è–≥–∫–æ–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏
    soft_labels = gmm.fit_predict(X_scaled)
    # –ñ–µ—Å—Ç–∫–æ–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –º–µ—Ç—Ä–∏–∫
    hard_labels = gmm.predict(X_scaled)
    # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ –∫ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
    probabilities = gmm.predict_proba(X_scaled)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å
    converged = gmm.converged_
    n_iter = gmm.n_iter_

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\nüìä GAUSSIAN MIXTURE (–ú–Ø–ì–ö–ê–Ø –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø)")
    print("=" * 60)
    print(f"‚öôÔ∏è  –ü–ê–†–ê–ú–ï–¢–†–´: n_components={n_components}, covariance_type={covariance_type}")
    print(f"üéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(texts)}")
    print(f"   ‚Ä¢ –°—Ö–æ–¥–∏–º–æ—Å—Ç—å: {'‚úÖ –£—Å–ø–µ—à–Ω–æ' if converged else '‚ùå –ù–µ —Å–æ—à–ª–∞—Å—å'}")
    print(f"   ‚Ä¢ –ò—Ç–µ—Ä–∞—Ü–∏–π: {n_iter}")

    unique_labels, counts = np.unique(hard_labels, return_counts=True)
    for cluster_id in unique_labels:
        count = counts[unique_labels == cluster_id][0]
        percentage = (count / len(texts)) * 100
        print(f"   ‚Ä¢ –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({percentage:.1f}%)")

    # –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    max_probs = np.max(probabilities, axis=1)
    confidence_stats = {
        'high_confidence': np.sum(max_probs > 0.9) / len(max_probs) * 100,
        'medium_confidence': np.sum((max_probs > 0.7) & (max_probs <= 0.9)) / len(max_probs) * 100,
        'low_confidence': np.sum(max_probs <= 0.7) / len(max_probs) * 100
    }

    print(f"\nüéØ –£–í–ï–†–ï–ù–ù–û–°–¢–¨ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò:")
    print(f"   ‚Ä¢ –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (>0.9): {confidence_stats['high_confidence']:.1f}% —Ç–æ—á–µ–∫")
    print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (0.7-0.9): {confidence_stats['medium_confidence']:.1f}% —Ç–æ—á–µ–∫")
    print(f"   ‚Ä¢ –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (‚â§0.7): {confidence_stats['low_confidence']:.1f}% —Ç–æ—á–µ–∫")

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–∏–ø–∞—Ö –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü
    print(f"\nüìã –¢–ò–ü–´ –ö–û–í–ê–†–ò–ê–¶–ò–û–ù–ù–´–• –ú–ê–¢–†–ò–¶:")
    cov_info = {
        'full': "–ü–æ–ª–Ω–∞—è –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞",
        'tied': "–û–¥–Ω–∞ –æ–±—â–∞—è –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –≤—Å–µ—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤",
        'diag': "–î–∏–∞–≥–æ–Ω–∞–ª—å–Ω–∞—è –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞",
        'spherical': "–°—Ñ–µ—Ä–∏—á–µ—Å–∫–∞—è –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ (–æ–¥–∏–Ω–∞–∫–æ–≤–∞—è –ø–æ –≤—Å–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º)"
    }
    print(f"   {covariance_type}: {cov_info.get(covariance_type, '')}")

    # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–ø–æ –∂–µ—Å—Ç–∫–æ–º—É –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—é)
    print(f"\n{'=' * 50}")
    interpreter.print_cluster_info(hard_labels)

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    print(f"\n{'=' * 50}")
    print("üìà –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í")
    interpreter.plot_keywords_barchart(hard_labels)

    # UMAP –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    print(f"\n{'=' * 50}")
    print("üé® –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ö–õ–ê–°–¢–ï–†–û–í")
    interpreter.visualize_umap(hard_labels,
                               f"Gaussian Mixture (k={n_components}, covariance={covariance_type})")

    # –ü—Ä–∏–º–µ—Ä—ã –º—è–≥–∫–æ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è
    print(f"\n{'=' * 50}")
    print("üîÆ –ü–†–ò–ú–ï–†–´ –ú–Ø–ì–ö–û–ì–û –ù–ê–ó–ù–ê–ß–ï–ù–ò–Ø")
    print("–ü–µ—Ä–≤—ã–µ 3 –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –∏—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ –∫ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:")
    print("–î–æ–∫.\t" + "\t".join([f"–ö–ª.{i}" for i in range(n_components)]))
    for i in range(min(3, len(probabilities))):
        prob_str = "\t".join([f"{p:.3f}" for p in probabilities[i]])
        assigned_cluster = hard_labels[i]
        print(f"{i}\t{prob_str} ‚Üí –ö–ª–∞—Å—Ç–µ—Ä {assigned_cluster}")

    return hard_labels, soft_labels, probabilities, gmm


def quick_gmm_analysis(texts, n_components_values=[2, 3, 4], covariance_types=['full', 'tied', 'diag']):
    """
    –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ GaussianMixture —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    """
    interpreter = SimpleClusterInterpreter(texts)
    X = interpreter.fit_vectorizer()
    X_dense = X.toarray()
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_dense)

    print("üöÄ –ë–´–°–¢–†–´–ô –ê–ù–ê–õ–ò–ó GAUSSIAN MIXTURE")
    print("=" * 70)
    print("k\tCovariance\tConverged\tSilhouette\tBIC")
    print("-" * 70)

    results = []

    for n_components in n_components_values:
        for covariance_type in covariance_types:
            try:
                # –ó–∞–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è
                import time
                start_time = time.time()

                gmm = GaussianMixture(
                    n_components=n_components,
                    covariance_type=covariance_type,
                    random_state=42,
                    max_iter=50
                )
                hard_labels = gmm.fit_predict(X_scaled)

                execution_time = time.time() - start_time

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å
                converged = gmm.converged_

                # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                from sklearn.metrics import silhouette_score
                silhouette = silhouette_score(X_dense, hard_labels)
                bic = gmm.bic(X_scaled)

                status = "‚úÖ" if converged else "‚ùå"
                print(f"{n_components}\t{covariance_type}\t\t{status}\t\t{silhouette:.3f}\t\t{bic:.0f}")

                results.append({
                    'n_components': n_components,
                    'covariance_type': covariance_type,
                    'silhouette': silhouette,
                    'bic': bic,
                    'converged': converged,
                    'time': execution_time,
                    'hard_labels': hard_labels,
                    'gmm': gmm
                })

            except Exception as e:
                print(f"{n_components}\t{covariance_type}\t\tERROR\t\t-\t\t-")

    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é (—Ç–æ–ª—å–∫–æ —Å—Ö–æ–¥–∏–≤—à–∏–µ—Å—è –º–æ–¥–µ–ª–∏)
    converged_results = [r for r in results if r['converged']]
    if converged_results:
        best_by_silhouette = max(converged_results, key=lambda x: x['silhouette'])
        best_by_bic = min(converged_results, key=lambda x: x['bic'])

        print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ï –ü–ê–†–ê–ú–ï–¢–†–´:")
        print(f"   –ü–æ Silhouette: k={best_by_silhouette['n_components']}, "
              f"covariance={best_by_silhouette['covariance_type']} "
              f"(Silhouette: {best_by_silhouette['silhouette']:.3f})")
        print(f"   –ü–æ BIC: k={best_by_bic['n_components']}, "
              f"covariance={best_by_bic['covariance_type']} "
              f"(BIC: {best_by_bic['bic']:.0f})")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à—É—é –ø–æ Silhouette –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
        print(f"\n{'=' * 50}")
        print("üîç –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –õ–£–ß–®–ò–• –ö–õ–ê–°–¢–ï–†–û–í")
        interpreter.print_cluster_info(best_by_silhouette['hard_labels'])

        return best_by_silhouette['hard_labels'], best_by_silhouette['gmm']

    return None, None


def compare_covariance_types(texts, n_components=3):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü
    """
    interpreter = SimpleClusterInterpreter(texts)
    X = interpreter.fit_vectorizer()
    X_dense = X.toarray()
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_dense)

    covariance_types = ['full', 'tied', 'diag', 'spherical']
    cov_info = {
        'full': "–ü–æ–ª–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞",
        'tied': "–û–±—â–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –≤—Å–µ—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤",
        'diag': "–î–∏–∞–≥–æ–Ω–∞–ª—å–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞",
        'spherical': "–°—Ñ–µ—Ä–∏—á–µ—Å–∫–∞—è –º–∞—Ç—Ä–∏—Ü–∞"
    }

    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –¢–ò–ü–û–í –ö–û–í–ê–†–ò–ê–¶–ò–û–ù–ù–´–• –ú–ê–¢–†–ò–¶")
    print("=" * 70)
    print("Covariance\t–û–ø–∏—Å–∞–Ω–∏–µ\t\t\tConverged\tSilhouette")
    print("-" * 80)

    results = {}

    for cov_type in covariance_types:
        try:
            gmm = GaussianMixture(
                n_components=n_components,
                covariance_type=cov_type,
                random_state=42,
                max_iter=50
            )
            hard_labels = gmm.fit_predict(X_scaled)

            converged = gmm.converged_
            from sklearn.metrics import silhouette_score
            silhouette = silhouette_score(X_dense, hard_labels)

            description = cov_info.get(cov_type, "")
            status = "‚úÖ" if converged else "‚ùå"
            print(f"{cov_type}\t\t{description[:25]}\t{status}\t\t{silhouette:.3f}")

            results[cov_type] = {
                'hard_labels': hard_labels,
                'silhouette': silhouette,
                'converged': converged,
                'gmm': gmm
            }

        except Exception as e:
            print(f"{cov_type}\t\t{cov_info.get(cov_type, '')[:25]}\tERROR\t\t-")

    return results


if __name__ == "__main__":
    texts = get_texts()
    true_labels = get_labels()

    print("üöÄ GAUSSIAN MIXTURE –î–õ–Ø –¢–ï–ö–°–¢–û–í –° –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–ï–ô")
    print("=" * 70)

    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ü—Ä–æ—Å—Ç–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π
    print("üéØ –í–ê–†–ò–ê–ù–¢ 1: –ü–†–û–°–¢–ê–Ø –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø –° –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–ï–ô")
    hard_labels1, soft_labels1, probabilities1, gmm1 = simple_gmm_cluster(
        texts, n_components=3, covariance_type='full'
    )