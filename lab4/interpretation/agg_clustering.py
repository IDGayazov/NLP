from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt
import numpy as np
from collections import Counter

try:
    import umap

    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False
    print("‚ö†Ô∏è  UMAP –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install umap-learn")

from util.decribe import get_labels, get_texts


class SimpleClusterInterpreter:
    """
    –ü—Ä–æ—Å—Ç–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è TF-IDF
    """

    def __init__(self, texts, vectorizer=None):
        self.texts = texts
        self.vectorizer = vectorizer if vectorizer else TfidfVectorizer(max_features=500)
        self.X = None
        self.feature_names = None

    def fit_vectorizer(self):
        """–û–±—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞"""
        self.X = self.vectorizer.fit_transform(self.texts)
        self.feature_names = self.vectorizer.get_feature_names_out()
        return self.X

    def get_cluster_keywords(self, labels, n_words=10):
        """
        –¢–æ–ø-N –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø–æ TF-IDF
        """
        if self.X is None:
            self.fit_vectorizer()

        unique_labels = np.unique(labels)
        cluster_keywords = {}

        for cluster_id in unique_labels:
            # –ò–Ω–¥–µ–∫—Å—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
            cluster_indices = np.where(labels == cluster_id)[0]

            if len(cluster_indices) == 0:
                cluster_keywords[cluster_id] = []
                continue

            # –°—Ä–µ–¥–Ω–∏–µ TF-IDF –≤–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞
            cluster_tfidf = self.X[cluster_indices].mean(axis=0)
            cluster_tfidf = np.array(cluster_tfidf).flatten()

            # –¢–æ–ø-N —Å–ª–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ –≤–µ—Å–∞–º–∏
            top_indices = np.argsort(cluster_tfidf)[::-1][:n_words]
            top_words = [(self.feature_names[i], cluster_tfidf[i])
                         for i in top_indices if cluster_tfidf[i] > 0]

            cluster_keywords[cluster_id] = top_words

        return cluster_keywords

    def get_most_frequent_words(self, labels, n_words=10):
        """
        –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
        """
        unique_labels = np.unique(labels)
        cluster_freq_words = {}

        for cluster_id in unique_labels:
            cluster_indices = np.where(labels == cluster_id)[0]
            cluster_texts = [self.texts[i] for i in cluster_indices]

            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞
            all_words = []
            for text in cluster_texts:
                words = text.lower().split()
                words = [word for word in words if len(word) > 2]  # —É–±–∏—Ä–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                all_words.extend(words)

            # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
            word_counts = Counter(all_words)
            cluster_freq_words[cluster_id] = word_counts.most_common(n_words)

        return cluster_freq_words

    def print_cluster_info(self, labels, n_words=8):
        """
        –ö—Ä–∞—Å–∏–≤–∞—è –ø–µ—á–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
        """
        unique_labels, counts = np.unique(labels, return_counts=True)

        print("üìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ö–õ–ê–°–¢–ï–†–ê–•:")
        print("=" * 60)

        # –¢–æ–ø —Å–ª–æ–≤–∞ –ø–æ TF-IDF
        tfidf_keywords = self.get_cluster_keywords(labels, n_words)

        # –ß–∞—Å—Ç–æ—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
        freq_words = self.get_most_frequent_words(labels, n_words)

        for cluster_id in unique_labels:
            count = counts[unique_labels == cluster_id][0]
            percentage = (count / len(labels)) * 100

            print(f"\nüî∏ –ö–õ–ê–°–¢–ï–† {cluster_id} ({count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {percentage:.1f}%):")

            # TF-IDF –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if cluster_id in tfidf_keywords:
                tfidf_words = [word for word, weight in tfidf_keywords[cluster_id][:5]]
                print(f"   üìà –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (TF-IDF): {', '.join(tfidf_words)}")

            # –ß–∞—Å—Ç–æ—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
            if cluster_id in freq_words:
                freq_words_list = [word for word, count in freq_words[cluster_id][:5]]
                print(f"   üìä –ß–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞: {', '.join(freq_words_list)}")

            # –ü—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            cluster_indices = np.where(labels == cluster_id)[0]
            if len(cluster_indices) > 0:
                sample_text = self.texts[cluster_indices[0]]
                preview = sample_text[:100] + "..." if len(sample_text) > 100 else sample_text
                print(f"   üìÑ –ü—Ä–∏–º–µ—Ä: {preview}")

    def plot_keywords_barchart(self, labels, n_words=6):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        """
        cluster_keywords = self.get_cluster_keywords(labels, n_words)
        n_clusters = len(cluster_keywords)

        if n_clusters == 0:
            print("‚ùå –ù–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
            return

        fig, axes = plt.subplots(1, n_clusters, figsize=(4 * n_clusters, 5))
        if n_clusters == 1:
            axes = [axes]

        for idx, (cluster_id, words_weights) in enumerate(cluster_keywords.items()):
            if not words_weights:
                continue

            words, weights = zip(*words_weights)

            axes[idx].barh(range(len(words)), weights, color=f'C{idx}', alpha=0.7)
            axes[idx].set_yticks(range(len(words)))
            axes[idx].set_yticklabels(words, fontsize=9)
            axes[idx].set_title(f'–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}')
            axes[idx].set_xlabel('TF-IDF –≤–µ—Å')

        plt.suptitle('–ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê –ö–õ–ê–°–¢–ï–†–û–í', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.show()

    def visualize_umap(self, labels, title="Agglomerative Clustering - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (UMAP)"):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ 2D —Å –ø–æ–º–æ—â—å—é UMAP
        """
        if not UMAP_AVAILABLE:
            print("‚ùå UMAP –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: pip install umap-learn")
            return None

        if self.X is None:
            self.fit_vectorizer()

        print("üîÑ –°—Ç—Ä–æ–∏–º UMAP –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é...")

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–ª–æ—Ç–Ω—ã–π –º–∞—Å—Å–∏–≤ –¥–ª—è UMAP
        X_dense = self.X.toarray()

        # –°–æ–∑–¥–∞–µ–º UMAP —Ä–µ–¥—É–∫—Ç–æ—Ä
        reducer = umap.UMAP(
            n_components=2,
            random_state=42,
            n_neighbors=15,
            min_dist=0.1,
            metric='cosine'
        )

        # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        embedding_2d = reducer.fit_transform(X_dense)

        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
        plt.figure(figsize=(12, 8))

        # –†–∞–∑–Ω—ã–µ —Ü–≤–µ—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        unique_labels = np.unique(labels)
        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))

        for i, cluster_id in enumerate(unique_labels):
            mask = labels == cluster_id
            plt.scatter(embedding_2d[mask, 0], embedding_2d[mask, 1],
                        c=[colors[i]], alpha=0.7, s=30, label=f'–ö–ª–∞—Å—Ç–µ—Ä {cluster_id} ({np.sum(mask)} —Ç–æ—á–µ–∫)')

        plt.title(title, fontsize=14, fontweight='bold')
        plt.xlabel('UMAP dimension 1')
        plt.ylabel('UMAP dimension 2')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

        return embedding_2d


def simple_agglomerative_cluster(texts, n_clusters=3, linkage='average'):
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è AgglomerativeClustering —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π
    """
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä–∞
    interpreter = SimpleClusterInterpreter(texts)
    X = interpreter.fit_vectorizer()
    X_dense = X.toarray()  # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–ª–æ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç

    # –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Å–≤—è–∑–∏
    if linkage == 'ward':
        metric = 'euclidean'
        X_used = X_dense
        print(f"‚öôÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ–≤–∫–ª–∏–¥–æ–≤–∞ –º–µ—Ç—Ä–∏–∫–∞ (—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –¥–ª—è 'ward')")
    else:
        metric = 'cosine'
        X_used = X_dense  # –î–ª—è cosine —Ç–æ–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–ª–æ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        print(f"‚öôÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ—Å–∏–Ω—É—Å–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞")

    # AgglomerativeClustering –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    agglo = AgglomerativeClustering(
        n_clusters=n_clusters,
        linkage=linkage,
        metric=metric
    )

    print(f"üîÑ –ó–∞–ø—É—Å–∫ AgglomerativeClustering —Å {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏...")
    labels = agglo.fit_predict(X_used)

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\nüìä AGGLOMERATIVE CLUSTERING")
    print("=" * 50)
    print(f"‚öôÔ∏è  –ü–ê–†–ê–ú–ï–¢–†–´: n_clusters={n_clusters}, linkage={linkage}, metric={metric}")
    print(f"üéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(texts)}")

    unique_labels, counts = np.unique(labels, return_counts=True)
    for cluster_id in unique_labels:
        count = counts[unique_labels == cluster_id][0]
        percentage = (count / len(texts)) * 100
        print(f"   ‚Ä¢ –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({percentage:.1f}%)")

    # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    print(f"\n{'=' * 50}")
    interpreter.print_cluster_info(labels)

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    print(f"\n{'=' * 50}")
    print("üìà –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í")
    interpreter.plot_keywords_barchart(labels)

    # UMAP –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    print(f"\n{'=' * 50}")
    print("üé® –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ö–õ–ê–°–¢–ï–†–û–í")
    interpreter.visualize_umap(labels, f"Agglomerative Clustering (k={n_clusters}, linkage={linkage})")

    return labels


def quick_agglomerative_analysis(texts, n_clusters_values=[2, 3, 4, 5], linkage_types=['ward', 'average', 'complete']):
    """
    –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ AgglomerativeClustering —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    """
    interpreter = SimpleClusterInterpreter(texts)
    X = interpreter.fit_vectorizer()
    X_dense = X.toarray()  # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–ª–æ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç

    print("üöÄ –ë–´–°–¢–†–´–ô –ê–ù–ê–õ–ò–ó AGGLOMERATIVE CLUSTERING")
    print("=" * 70)
    print("k\tLinkage\t\tSilhouette\t–í—Ä–µ–º—è (—Å–µ–∫)")
    print("-" * 60)

    results = []

    for n_clusters in n_clusters_values:
        for linkage in linkage_types:
            try:
                # –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫—É
                if linkage == 'ward':
                    metric = 'euclidean'
                else:
                    metric = 'cosine'

                # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–ª–æ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                X_used = X_dense

                # –ó–∞–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è
                import time
                start_time = time.time()

                agglo = AgglomerativeClustering(
                    n_clusters=n_clusters,
                    linkage=linkage,
                    metric=metric
                )
                labels = agglo.fit_predict(X_used)

                execution_time = time.time() - start_time

                # –í—ã—á–∏—Å–ª—è–µ–º silhouette score
                from sklearn.metrics import silhouette_score
                silhouette = silhouette_score(X_dense, labels)

                print(f"{n_clusters}\t{linkage}\t\t{silhouette:.3f}\t\t{execution_time:.2f}")

                results.append({
                    'n_clusters': n_clusters,
                    'linkage': linkage,
                    'silhouette': silhouette,
                    'time': execution_time,
                    'labels': labels
                })

            except Exception as e:
                print(f"{n_clusters}\t{linkage}\t\tERROR\t\t-")

    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    if results:
        best_result = max(results, key=lambda x: x['silhouette'])
        print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ï –ü–ê–†–ê–ú–ï–¢–†–´:")
        print(f"   n_clusters={best_result['n_clusters']}, linkage={best_result['linkage']}")
        print(f"   Silhouette Score: {best_result['silhouette']:.3f}")
        print(f"   –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {best_result['time']:.2f} —Å–µ–∫")

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        print(f"\n{'=' * 50}")
        print("üîç –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –õ–£–ß–®–ò–• –ö–õ–ê–°–¢–ï–†–û–í")
        interpreter.print_cluster_info(best_result['labels'])

        return best_result['labels']

    return None


def compare_linkage_types(texts, n_clusters=3):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–≤—è–∑–µ–π –¥–ª—è AgglomerativeClustering
    """
    interpreter = SimpleClusterInterpreter(texts)
    X = interpreter.fit_vectorizer()
    X_dense = X.toarray()  # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–ª–æ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç

    linkage_types = ['ward', 'average', 'complete', 'single']
    linkage_info = {
        'ward': "–ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç –¥–∏—Å–ø–µ—Ä—Å–∏—é –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤",
        'average': "–°—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ç–æ—á–∫–∞–º–∏",
        'complete': "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–ø–æ–ª–Ω–∞—è —Å–≤—è–∑—å)",
        'single': "–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–æ–¥–∏–Ω–æ—á–Ω–∞—è —Å–≤—è–∑—å)"
    }

    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –¢–ò–ü–û–í –°–í–Ø–ó–ï–ô (LINKAGE)")
    print("=" * 60)
    print("Linkage\t\t–û–ø–∏—Å–∞–Ω–∏–µ\t\t\tSilhouette")
    print("-" * 70)

    results = {}

    for linkage in linkage_types:
        try:
            # –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫—É
            if linkage == 'ward':
                metric = 'euclidean'
            else:
                metric = 'cosine'

            # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–ª–æ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            X_used = X_dense

            agglo = AgglomerativeClustering(
                n_clusters=n_clusters,
                linkage=linkage,
                metric=metric
            )
            labels = agglo.fit_predict(X_used)

            from sklearn.metrics import silhouette_score
            silhouette = silhouette_score(X_dense, labels)

            description = linkage_info.get(linkage, "")
            print(f"{linkage}\t\t{description[:25]}\t{silhouette:.3f}")

            results[linkage] = {
                'labels': labels,
                'silhouette': silhouette,
                'description': description
            }

        except Exception as e:
            print(f"{linkage}\t\t{linkage_info.get(linkage, '')[:25]}\tERROR")

    return results


if __name__ == "__main__":
    texts = get_texts()
    true_labels = get_labels()

    print("üöÄ AGGLOMERATIVE CLUSTERING –î–õ–Ø –¢–ï–ö–°–¢–û–í –° –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–ï–ô")
    print("=" * 70)

    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ü—Ä–æ—Å—Ç–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π
    print("üéØ –í–ê–†–ò–ê–ù–¢ 1: –ü–†–û–°–¢–ê–Ø –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø –° –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–ï–ô")
    labels1 = simple_agglomerative_cluster(texts, n_clusters=3, linkage='average')

    # –í–∞—Ä–∏–∞–Ω—Ç 2: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —Å–≤—è–∑–µ–π
    print(f"\n{'=' * 70}")
    print("üéØ –í–ê–†–ò–ê–ù–¢ 2: –°–†–ê–í–ù–ï–ù–ò–ï –¢–ò–ü–û–í –°–í–Ø–ó–ï–ô")
    linkage_results = compare_linkage_types(texts, n_clusters=3)

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ª—É—á—à–µ–≥–æ —Ç–∏–ø–∞ —Å–≤—è–∑–∏
    if linkage_results:
        best_linkage = max(linkage_results.keys(),
                           key=lambda x: linkage_results[x]['silhouette']
                           if 'silhouette' in linkage_results[x] else -1)

        print(f"\nüé® –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –î–õ–Ø –õ–£–ß–®–ï–ì–û LINKAGE: {best_linkage}")
        interpreter = SimpleClusterInterpreter(texts)
        interpreter.fit_vectorizer()
        interpreter.visualize_umap(
            linkage_results[best_linkage]['labels'],
            f"Agglomerative Clustering (linkage={best_linkage})"
        )

    # –í–∞—Ä–∏–∞–Ω—Ç 3: –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —Å —Ä–∞–∑–Ω—ã–º–∏ k
    print(f"\n{'=' * 70}")
    print("üéØ –í–ê–†–ò–ê–ù–¢ 3: –ë–´–°–¢–†–´–ô –ê–ù–ê–õ–ò–ó –° –†–ê–ó–ù–´–ú–ò K")
    labels3 = quick_agglomerative_analysis(
        texts,
        n_clusters_values=[2, 3, 4, 5],
        linkage_types=['ward', 'average', 'complete']
    )