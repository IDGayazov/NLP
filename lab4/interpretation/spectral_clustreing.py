from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import SpectralClustering
import matplotlib.pyplot as plt
import numpy as np
from collections import Counter

try:
    import umap

    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False
    print("‚ö†Ô∏è  UMAP –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install umap-learn")

from util.decribe import get_labels, get_texts


class SimpleClusterInterpreter:
    """
    –ü—Ä–æ—Å—Ç–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è TF-IDF
    """

    def __init__(self, texts, vectorizer=None):
        self.texts = texts
        self.vectorizer = vectorizer if vectorizer else TfidfVectorizer(max_features=500)
        self.X = None
        self.feature_names = None

    def fit_vectorizer(self):
        """–û–±—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞"""
        self.X = self.vectorizer.fit_transform(self.texts)
        self.feature_names = self.vectorizer.get_feature_names_out()
        return self.X

    def get_cluster_keywords(self, labels, n_words=10):
        """
        –¢–æ–ø-N –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø–æ TF-IDF
        """
        if self.X is None:
            self.fit_vectorizer()

        unique_labels = np.unique(labels)
        cluster_keywords = {}

        for cluster_id in unique_labels:
            # –ò–Ω–¥–µ–∫—Å—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
            cluster_indices = np.where(labels == cluster_id)[0]

            if len(cluster_indices) == 0:
                cluster_keywords[cluster_id] = []
                continue

            # –°—Ä–µ–¥–Ω–∏–µ TF-IDF –≤–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞
            cluster_tfidf = self.X[cluster_indices].mean(axis=0)
            cluster_tfidf = np.array(cluster_tfidf).flatten()

            # –¢–æ–ø-N —Å–ª–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ –≤–µ—Å–∞–º–∏
            top_indices = np.argsort(cluster_tfidf)[::-1][:n_words]
            top_words = [(self.feature_names[i], cluster_tfidf[i])
                         for i in top_indices if cluster_tfidf[i] > 0]

            cluster_keywords[cluster_id] = top_words

        return cluster_keywords

    def get_most_frequent_words(self, labels, n_words=10):
        """
        –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
        """
        unique_labels = np.unique(labels)
        cluster_freq_words = {}

        for cluster_id in unique_labels:
            cluster_indices = np.where(labels == cluster_id)[0]
            cluster_texts = [self.texts[i] for i in cluster_indices]

            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞
            all_words = []
            for text in cluster_texts:
                words = text.lower().split()
                words = [word for word in words if len(word) > 2]  # —É–±–∏—Ä–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                all_words.extend(words)

            # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
            word_counts = Counter(all_words)
            cluster_freq_words[cluster_id] = word_counts.most_common(n_words)

        return cluster_freq_words

    def print_cluster_info(self, labels, n_words=8):
        """
        –ö—Ä–∞—Å–∏–≤–∞—è –ø–µ—á–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
        """
        unique_labels, counts = np.unique(labels, return_counts=True)

        print("üìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ö–õ–ê–°–¢–ï–†–ê–•:")
        print("=" * 60)

        # –¢–æ–ø —Å–ª–æ–≤–∞ –ø–æ TF-IDF
        tfidf_keywords = self.get_cluster_keywords(labels, n_words)

        # –ß–∞—Å—Ç–æ—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
        freq_words = self.get_most_frequent_words(labels, n_words)

        for cluster_id in unique_labels:
            count = counts[unique_labels == cluster_id][0]
            percentage = (count / len(labels)) * 100

            print(f"\nüî∏ –ö–õ–ê–°–¢–ï–† {cluster_id} ({count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {percentage:.1f}%):")

            # TF-IDF –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if cluster_id in tfidf_keywords:
                tfidf_words = [word for word, weight in tfidf_keywords[cluster_id][:5]]
                print(f"   üìà –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (TF-IDF): {', '.join(tfidf_words)}")

            # –ß–∞—Å—Ç–æ—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
            if cluster_id in freq_words:
                freq_words_list = [word for word, count in freq_words[cluster_id][:5]]
                print(f"   üìä –ß–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞: {', '.join(freq_words_list)}")

            # –ü—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            cluster_indices = np.where(labels == cluster_id)[0]
            if len(cluster_indices) > 0:
                sample_text = self.texts[cluster_indices[0]]
                preview = sample_text[:100] + "..." if len(sample_text) > 100 else sample_text
                print(f"   üìÑ –ü—Ä–∏–º–µ—Ä: {preview}")

    def plot_keywords_barchart(self, labels, n_words=6):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        """
        cluster_keywords = self.get_cluster_keywords(labels, n_words)
        n_clusters = len(cluster_keywords)

        if n_clusters == 0:
            print("‚ùå –ù–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
            return

        fig, axes = plt.subplots(1, n_clusters, figsize=(4 * n_clusters, 5))
        if n_clusters == 1:
            axes = [axes]

        for idx, (cluster_id, words_weights) in enumerate(cluster_keywords.items()):
            if not words_weights:
                continue

            words, weights = zip(*words_weights)

            axes[idx].barh(range(len(words)), weights, color=f'C{idx}', alpha=0.7)
            axes[idx].set_yticks(range(len(words)))
            axes[idx].set_yticklabels(words, fontsize=9)
            axes[idx].set_title(f'–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}')
            axes[idx].set_xlabel('TF-IDF –≤–µ—Å')

        plt.suptitle('–ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê –ö–õ–ê–°–¢–ï–†–û–í', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.show()

    def visualize_umap(self, labels, title="Spectral Clustering - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (UMAP)"):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ 2D —Å –ø–æ–º–æ—â—å—é UMAP
        """
        if not UMAP_AVAILABLE:
            print("‚ùå UMAP –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: pip install umap-learn")
            return None

        if self.X is None:
            self.fit_vectorizer()

        print("üîÑ –°—Ç—Ä–æ–∏–º UMAP –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é...")

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–ª–æ—Ç–Ω—ã–π –º–∞—Å—Å–∏–≤ –¥–ª—è UMAP
        X_dense = self.X.toarray()

        # –°–æ–∑–¥–∞–µ–º UMAP —Ä–µ–¥—É–∫—Ç–æ—Ä
        reducer = umap.UMAP(
            n_components=2,
            random_state=42,
            n_neighbors=15,
            min_dist=0.1,
            metric='cosine'
        )

        # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        embedding_2d = reducer.fit_transform(X_dense)

        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
        plt.figure(figsize=(12, 8))

        # –†–∞–∑–Ω—ã–µ —Ü–≤–µ—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        unique_labels = np.unique(labels)
        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))

        for i, cluster_id in enumerate(unique_labels):
            mask = labels == cluster_id
            plt.scatter(embedding_2d[mask, 0], embedding_2d[mask, 1],
                        c=[colors[i]], alpha=0.7, s=30, label=f'–ö–ª–∞—Å—Ç–µ—Ä {cluster_id} ({np.sum(mask)} —Ç–æ—á–µ–∫)')

        plt.title(title, fontsize=14, fontweight='bold')
        plt.xlabel('UMAP dimension 1')
        plt.ylabel('UMAP dimension 2')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

        return embedding_2d


def simple_spectral_cluster(texts, n_clusters=3, affinity='cosine', n_neighbors=10):
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è SpectralClustering —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π
    """
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä–∞
    interpreter = SimpleClusterInterpreter(texts)
    X = interpreter.fit_vectorizer()
    X_dense = X.toarray()

    print(f"üîÑ –ó–∞–ø—É—Å–∫ SpectralClustering —Å {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏...")
    print(f"‚öôÔ∏è  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: affinity={affinity}, n_neighbors={n_neighbors}")

    # SpectralClustering –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    if affinity == 'nearest_neighbors':
        spectral = SpectralClustering(
            n_clusters=n_clusters,
            affinity=affinity,
            n_neighbors=n_neighbors,
            random_state=42,
            n_init=5
        )
    else:
        spectral = SpectralClustering(
            n_clusters=n_clusters,
            affinity=affinity,
            random_state=42,
            n_init=5
        )

    # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö
    if affinity == 'cosine':
        X_used = X  # –î–ª—è cosine –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
    else:
        X_used = X_dense  # –î–ª—è –¥—Ä—É–≥–∏—Ö affinity –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–ª–æ—Ç–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É

    labels = spectral.fit_predict(X_used)

    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    from sklearn.metrics import silhouette_score
    silhouette = silhouette_score(X_dense, labels)

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\nüìä SPECTRAL CLUSTERING")
    print("=" * 50)
    print(f"‚öôÔ∏è  –ü–ê–†–ê–ú–ï–¢–†–´: n_clusters={n_clusters}, affinity={affinity}")
    if affinity == 'nearest_neighbors':
        print(f"                 n_neighbors={n_neighbors}")
    print(f"üéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(texts)}")
    print(f"   ‚Ä¢ Silhouette Score: {silhouette:.3f}")

    unique_labels, counts = np.unique(labels, return_counts=True)
    for cluster_id in unique_labels:
        count = counts[unique_labels == cluster_id][0]
        percentage = (count / len(texts)) * 100
        print(f"   ‚Ä¢ –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({percentage:.1f}%)")

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–∏–ø–∞—Ö —Å—Ö–æ–¥—Å—Ç–≤–∞
    print(f"\nüìã –¢–ò–ü–´ –°–•–û–î–°–¢–í–ê (AFFINITY):")
    affinity_info = {
        'rbf': "–†–∞–¥–∏–∞–ª—å–Ω–∞—è –±–∞–∑–∏—Å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è (–≥–∞—É—Å—Å–æ–≤–æ —è–¥—Ä–æ)",
        'nearest_neighbors': "–ì—Ä–∞—Ñ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π",
        'cosine': "–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (–ª—É—á—à–µ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤)"
    }
    print(f"   {affinity}: {affinity_info.get(affinity, '')}")

    # –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ Spectral Clustering
    print(f"\nüí° –û–°–û–ë–ï–ù–ù–û–°–¢–ò SPECTRAL CLUSTERING:")
    print("   ‚Ä¢ –†–∞–±–æ—Ç–∞–µ—Ç —Å –¥–∞–Ω–Ω—ã–º–∏ —Å–ª–æ–∂–Ω–æ–π –Ω–µ–≤—ã–ø—É–∫–ª–æ–π —Ñ–æ—Ä–º—ã")
    print("   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—É—é —Ç–µ–æ—Ä–∏—é –≥—Ä–∞—Ñ–æ–≤")
    print("   ‚Ä¢ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –∫–æ–≥–¥–∞ –∫–ª–∞—Å—Ç–µ—Ä—ã –Ω–µ —Ä–∞–∑–¥–µ–ª–∏–º—ã –ª–∏–Ω–µ–π–Ω–æ")
    print("   ‚Ä¢ –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –≤—ã–±–æ—Ä—É —Ñ—É–Ω–∫—Ü–∏–∏ —Å—Ö–æ–¥—Å—Ç–≤–∞")

    # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    print(f"\n{'=' * 50}")
    interpreter.print_cluster_info(labels)

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    print(f"\n{'=' * 50}")
    print("üìà –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í")
    interpreter.plot_keywords_barchart(labels)

    # UMAP –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    print(f"\n{'=' * 50}")
    print("üé® –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ö–õ–ê–°–¢–ï–†–û–í")
    interpreter.visualize_umap(labels, f"Spectral Clustering (k={n_clusters}, affinity={affinity})")

    return labels


def quick_spectral_analysis(texts, n_clusters_values=[2, 3, 4], affinity_types=['cosine', 'rbf']):
    """
    –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ SpectralClustering —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    """
    interpreter = SimpleClusterInterpreter(texts)
    X = interpreter.fit_vectorizer()
    X_dense = X.toarray()

    print("üöÄ –ë–´–°–¢–†–´–ô –ê–ù–ê–õ–ò–ó SPECTRAL CLUSTERING")
    print("=" * 70)
    print("k\tAffinity\tSilhouette\t–í—Ä–µ–º—è (—Å–µ–∫)")
    print("-" * 60)

    results = []

    for n_clusters in n_clusters_values:
        for affinity in affinity_types:
            try:
                # –ó–∞–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è
                import time
                start_time = time.time()

                # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                if affinity == 'nearest_neighbors':
                    spectral = SpectralClustering(
                        n_clusters=n_clusters,
                        affinity=affinity,
                        n_neighbors=10,
                        random_state=42,
                        n_init=5
                    )
                else:
                    spectral = SpectralClustering(
                        n_clusters=n_clusters,
                        affinity=affinity,
                        random_state=42,
                        n_init=5
                    )

                # –í—ã–±–∏—Ä–∞–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö
                if affinity == 'cosine':
                    X_used = X
                else:
                    X_used = X_dense

                labels = spectral.fit_predict(X_used)

                execution_time = time.time() - start_time

                # –í—ã—á–∏—Å–ª—è–µ–º silhouette score
                from sklearn.metrics import silhouette_score
                silhouette = silhouette_score(X_dense, labels)

                print(f"{n_clusters}\t{affinity}\t\t{silhouette:.3f}\t\t{execution_time:.2f}")

                results.append({
                    'n_clusters': n_clusters,
                    'affinity': affinity,
                    'silhouette': silhouette,
                    'time': execution_time,
                    'labels': labels
                })

            except Exception as e:
                print(f"{n_clusters}\t{affinity}\t\tERROR\t\t-")

    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    if results:
        best_result = max(results, key=lambda x: x['silhouette'])
        print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ï –ü–ê–†–ê–ú–ï–¢–†–´:")
        print(f"   n_clusters={best_result['n_clusters']}, affinity={best_result['affinity']}")
        print(f"   Silhouette Score: {best_result['silhouette']:.3f}")
        print(f"   –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {best_result['time']:.2f} —Å–µ–∫")

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        print(f"\n{'=' * 50}")
        print("üîç –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –õ–£–ß–®–ò–• –ö–õ–ê–°–¢–ï–†–û–í")
        interpreter.print_cluster_info(best_result['labels'])

        return best_result['labels']

    return None

def compare_affinity_types(texts, n_clusters=3):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è SpectralClustering
    """
    interpreter = SimpleClusterInterpreter(texts)
    X = interpreter.fit_vectorizer()
    X_dense = X.toarray()

    affinity_types = ['cosine', 'rbf', 'nearest_neighbors']
    affinity_info = {
        'cosine': "–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (–ª—É—á—à–µ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤)",
        'rbf': "–ì–∞—É—Å—Å–æ–≤–æ —è–¥—Ä–æ (–µ–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ)",
        'nearest_neighbors': "–ì—Ä–∞—Ñ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π"
    }

    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –¢–ò–ü–û–í –°–•–û–î–°–¢–í–ê (AFFINITY)")
    print("=" * 70)
    print("Affinity\t\t–û–ø–∏—Å–∞–Ω–∏–µ\t\t\tSilhouette")
    print("-" * 80)

    results = {}

    for affinity in affinity_types:
        try:
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            if affinity == 'nearest_neighbors':
                spectral = SpectralClustering(
                    n_clusters=n_clusters,
                    affinity=affinity,
                    n_neighbors=10,
                    random_state=42,
                    n_init=5
                )
            else:
                spectral = SpectralClustering(
                    n_clusters=n_clusters,
                    affinity=affinity,
                    random_state=42,
                    n_init=5
                )

            # –í—ã–±–∏—Ä–∞–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö
            if affinity == 'cosine':
                X_used = X
            else:
                X_used = X_dense

            labels = spectral.fit_predict(X_used)

            from sklearn.metrics import silhouette_score
            silhouette = silhouette_score(X_dense, labels)

            description = affinity_info.get(affinity, "")
            print(f"{affinity}\t\t{description[:25]}\t{silhouette:.3f}")

            results[affinity] = {
                'labels': labels,
                'silhouette': silhouette,
                'description': description
            }

        except Exception as e:
            print(f"{affinity}\t\t{affinity_info.get(affinity, '')[:25]}\tERROR")

    return results


def analyze_similarity_structure(texts):
    """
    –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
    """
    from sklearn.metrics.pairwise import cosine_similarity

    interpreter = SimpleClusterInterpreter(texts)
    X = interpreter.fit_vectorizer()
    X_dense = X.toarray()

    print("üîó –ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ –°–•–û–î–°–¢–í–ê –î–ê–ù–ù–´–•")
    print("=" * 50)

    # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
    similarities = cosine_similarity(X)
    np.fill_diagonal(similarities, 0)  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–∞–º–æ—Å—Ö–æ–¥—Å—Ç–≤–æ

    print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–û–°–ò–ù–£–°–ù–û–ì–û –°–•–û–î–°–¢–í–ê:")
    print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {np.mean(similarities):.4f}")
    print(f"   ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {np.max(similarities):.4f}")
    print(f"   ‚Ä¢ –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {np.min(similarities):.4f}")
    print(f"   ‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.std(similarities):.4f}")

    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞
    high_similarity = np.sum(similarities > 0.7) / (len(similarities) ** 2) * 100
    medium_similarity = np.sum((similarities > 0.3) & (similarities <= 0.7)) / (len(similarities) ** 2) * 100
    low_similarity = np.sum(similarities <= 0.3) / (len(similarities) ** 2) * 100

    print(f"\nüìà –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –°–•–û–î–°–¢–í–ê:")
    print(f"   ‚Ä¢ –í—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (>0.7): {high_similarity:.1f}% –ø–∞—Ä")
    print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (0.3-0.7): {medium_similarity:.1f}% –ø–∞—Ä")
    print(f"   ‚Ä¢ –ù–∏–∑–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (‚â§0.3): {low_similarity:.1f}% –ø–∞—Ä")


if __name__ == "__main__":
    texts = get_texts()
    true_labels = get_labels()

    print("üöÄ SPECTRAL CLUSTERING –î–õ–Ø –¢–ï–ö–°–¢–û–í –° –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–ï–ô")
    print("=" * 70)

    # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    print("üéØ –ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–•")
    analyze_similarity_structure(texts)

    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ü—Ä–æ—Å—Ç–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π
    print(f"\n{'=' * 70}")
    print("üéØ –í–ê–†–ò–ê–ù–¢ 1: –ü–†–û–°–¢–ê–Ø –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø –° –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–ï–ô")
    labels1 = simple_spectral_cluster(texts, n_clusters=3, affinity='cosine')

    # –í–∞—Ä–∏–∞–Ω—Ç 2: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —Å—Ö–æ–¥—Å—Ç–≤–∞
    print(f"\n{'=' * 70}")
    print("üéØ –í–ê–†–ò–ê–ù–¢ 2: –°–†–ê–í–ù–ï–ù–ò–ï –¢–ò–ü–û–í –°–•–û–î–°–¢–í–ê")
    affinity_results = compare_affinity_types(texts, n_clusters=3)

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ª—É—á—à–µ–≥–æ —Ç–∏–ø–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞
    if affinity_results:
        best_affinity = max(affinity_results.keys(),
                            key=lambda x: affinity_results[x]['silhouette']
                            if 'silhouette' in affinity_results[x] else -1)

        print(f"\nüé® –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –î–õ–Ø –õ–£–ß–®–ï–ì–û AFFINITY: {best_affinity}")
        interpreter = SimpleClusterInterpreter(texts)
        interpreter.fit_vectorizer()
        interpreter.visualize_umap(
            affinity_results[best_affinity]['labels'],
            f"Spectral Clustering (affinity={best_affinity})"
        )

    # –í–∞—Ä–∏–∞–Ω—Ç 3: –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —Å —Ä–∞–∑–Ω—ã–º–∏ k
    print(f"\n{'=' * 70}")
    print("üéØ –í–ê–†–ò–ê–ù–¢ 3: –ë–´–°–¢–†–´–ô –ê–ù–ê–õ–ò–ó –° –†–ê–ó–ù–´–ú–ò K")
    best_labels = quick_spectral_analysis(
        texts,
        n_clusters_values=[2, 3, 4],
        affinity_types=['cosine', 'rbf']
    )