from collections import Counter

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
import numpy as np

try:
    import umap

    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False
    print("‚ö†Ô∏è  UMAP –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install umap-learn")

from util.decribe import get_labels, get_texts


class SimpleClusterInterpreter:
    """
    –ü—Ä–æ—Å—Ç–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è TF-IDF
    """

    def __init__(self, texts, vectorizer=None):
        self.texts = texts
        self.vectorizer = vectorizer if vectorizer else TfidfVectorizer(max_features=500)
        self.X = None
        self.feature_names = None

    def fit_vectorizer(self):
        """–û–±—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞"""
        self.X = self.vectorizer.fit_transform(self.texts)
        self.feature_names = self.vectorizer.get_feature_names_out()
        return self.X

    def get_cluster_keywords(self, labels, n_words=10):
        """
        –¢–æ–ø-N –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø–æ TF-IDF
        """
        if self.X is None:
            self.fit_vectorizer()

        unique_labels = np.unique(labels)
        cluster_keywords = {}

        for cluster_id in unique_labels:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à—É–º–æ–≤—ã–µ —Ç–æ—á–∫–∏
            if cluster_id == -1:
                continue

            # –ò–Ω–¥–µ–∫—Å—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
            cluster_indices = np.where(labels == cluster_id)[0]

            if len(cluster_indices) == 0:
                cluster_keywords[cluster_id] = []
                continue

            # –°—Ä–µ–¥–Ω–∏–µ TF-IDF –≤–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞
            cluster_tfidf = self.X[cluster_indices].mean(axis=0)
            cluster_tfidf = np.array(cluster_tfidf).flatten()

            # –¢–æ–ø-N —Å–ª–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ –≤–µ—Å–∞–º–∏
            top_indices = np.argsort(cluster_tfidf)[::-1][:n_words]
            top_words = [(self.feature_names[i], cluster_tfidf[i])
                         for i in top_indices if cluster_tfidf[i] > 0]

            cluster_keywords[cluster_id] = top_words

        return cluster_keywords

    def get_most_frequent_words(self, labels, n_words=10):
        """
        –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
        """
        unique_labels = np.unique(labels)
        cluster_freq_words = {}

        for cluster_id in unique_labels:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à—É–º–æ–≤—ã–µ —Ç–æ—á–∫–∏
            if cluster_id == -1:
                continue

            cluster_indices = np.where(labels == cluster_id)[0]
            cluster_texts = [self.texts[i] for i in cluster_indices]

            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞
            all_words = []
            for text in cluster_texts:
                words = text.lower().split()
                words = [word for word in words if len(word) > 2]  # —É–±–∏—Ä–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                all_words.extend(words)

            # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
            word_counts = Counter(all_words)
            cluster_freq_words[cluster_id] = word_counts.most_common(n_words)

        return cluster_freq_words

    def print_cluster_info(self, labels, n_words=8):
        """
        –ö—Ä–∞—Å–∏–≤–∞—è –ø–µ—á–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
        """
        unique_labels, counts = np.unique(labels, return_counts=True)

        print("üìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ö–õ–ê–°–¢–ï–†–ê–•:")
        print("=" * 60)

        # –¢–æ–ø —Å–ª–æ–≤–∞ –ø–æ TF-IDF
        tfidf_keywords = self.get_cluster_keywords(labels, n_words)

        # –ß–∞—Å—Ç–æ—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
        freq_words = self.get_most_frequent_words(labels, n_words)

        # –°–Ω–∞—á–∞–ª–∞ –≤—ã–≤–æ–¥–∏–º —à—É–º
        noise_count = counts[unique_labels == -1][0] if -1 in unique_labels else 0
        if noise_count > 0:
            print(f"\nüî∏ –®–£–ú: {noise_count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({noise_count / len(labels) * 100:.1f}%)")

        # –ó–∞—Ç–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã
        for cluster_id in unique_labels:
            if cluster_id == -1:  # —à—É–º —É–∂–µ –≤—ã–≤–µ–ª–∏
                continue

            count = counts[unique_labels == cluster_id][0]
            percentage = (count / len(labels)) * 100

            print(f"\nüî∏ –ö–õ–ê–°–¢–ï–† {cluster_id} ({count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {percentage:.1f}%):")

            # TF-IDF –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if cluster_id in tfidf_keywords:
                tfidf_words = [word for word, weight in tfidf_keywords[cluster_id][:5]]
                print(f"   üìà –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (TF-IDF): {', '.join(tfidf_words)}")

            # –ß–∞—Å—Ç–æ—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
            if cluster_id in freq_words:
                freq_words_list = [word for word, count in freq_words[cluster_id][:5]]
                print(f"   üìä –ß–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞: {', '.join(freq_words_list)}")

            # –ü—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            cluster_indices = np.where(labels == cluster_id)[0]
            if len(cluster_indices) > 0:
                sample_text = self.texts[cluster_indices[0]]
                preview = sample_text[:100] + "..." if len(sample_text) > 100 else sample_text
                print(f"   üìÑ –ü—Ä–∏–º–µ—Ä: {preview}")

    def plot_keywords_barchart(self, labels, n_words=6):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        """
        cluster_keywords = self.get_cluster_keywords(labels, n_words)
        n_clusters = len(cluster_keywords)

        if n_clusters == 0:
            print("‚ùå –ù–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
            return

        fig, axes = plt.subplots(1, n_clusters, figsize=(4 * n_clusters, 5))
        if n_clusters == 1:
            axes = [axes]

        for idx, (cluster_id, words_weights) in enumerate(cluster_keywords.items()):
            if not words_weights:
                continue

            words, weights = zip(*words_weights)

            axes[idx].barh(range(len(words)), weights, color=f'C{idx}', alpha=0.7)
            axes[idx].set_yticks(range(len(words)))
            axes[idx].set_yticklabels(words, fontsize=9)
            axes[idx].set_title(f'–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}')
            axes[idx].set_xlabel('TF-IDF –≤–µ—Å')

        plt.suptitle('–ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê –ö–õ–ê–°–¢–ï–†–û–í', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.show()

    def visualize_umap(self, labels, title="DBSCAN - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (UMAP)"):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ 2D —Å –ø–æ–º–æ—â—å—é UMAP
        """
        if not UMAP_AVAILABLE:
            print("‚ùå UMAP –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: pip install umap-learn")
            return None

        if self.X is None:
            self.fit_vectorizer()

        print("üîÑ –°—Ç—Ä–æ–∏–º UMAP –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é...")

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–ª–æ—Ç–Ω—ã–π –º–∞—Å—Å–∏–≤ –¥–ª—è UMAP
        X_dense = self.X.toarray()

        # –°–æ–∑–¥–∞–µ–º UMAP —Ä–µ–¥—É–∫—Ç–æ—Ä
        reducer = umap.UMAP(
            n_components=2,
            random_state=42,
            n_neighbors=15,
            min_dist=0.1,
            metric='cosine'
        )

        # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        embedding_2d = reducer.fit_transform(X_dense)

        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
        plt.figure(figsize=(12, 8))

        # –†–∞–∑–Ω—ã–µ —Ü–≤–µ—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        unique_labels = np.unique(labels)
        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))

        for i, cluster_id in enumerate(unique_labels):
            if cluster_id == -1:
                # –®—É–º–æ–≤—ã–µ —Ç–æ—á–∫–∏ —Å–µ—Ä—ã–º —Ü–≤–µ—Ç–æ–º
                mask = labels == cluster_id
                plt.scatter(embedding_2d[mask, 0], embedding_2d[mask, 1],
                            c='gray', alpha=0.5, s=20, label=f'–®—É–º ({np.sum(mask)} —Ç–æ—á–µ–∫)')
            else:
                mask = labels == cluster_id
                plt.scatter(embedding_2d[mask, 0], embedding_2d[mask, 1],
                            c=[colors[i]], alpha=0.7, s=30, label=f'–ö–ª–∞—Å—Ç–µ—Ä {cluster_id} ({np.sum(mask)} —Ç–æ—á–µ–∫)')

        plt.title(title, fontsize=14, fontweight='bold')
        plt.xlabel('UMAP dimension 1')
        plt.ylabel('UMAP dimension 2')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

        return embedding_2d


def find_optimal_eps(texts, k=5):
    """
    –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ eps —Å –ø–æ–º–æ—â—å—é k-—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
    """
    vectorizer = TfidfVectorizer(max_features=500)
    X = vectorizer.fit_transform(texts)

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
    neighbors = NearestNeighbors(n_neighbors=k, metric='cosine')
    neighbors_fit = neighbors.fit(X)
    distances, indices = neighbors_fit.kneighbors(X)

    distances = np.sort(distances[:, k - 1], axis=0)

    plt.figure(figsize=(10, 6))
    plt.plot(distances)
    plt.xlabel('–¢–æ—á–∫–∏ (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é)')
    plt.ylabel(f'{k}-–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ')
    plt.title('–ú–ï–¢–û–î –ö–û–õ–ï–ù–ê –î–õ–Ø –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø EPS\n(–∏—â–µ–º —Ç–æ—á–∫—É –∏–∑–≥–∏–±–∞)')
    plt.grid(True, alpha=0.3)

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ—á–∫–∏ –∏–∑–≥–∏–±–∞
    gradients = np.gradient(distances)
    elbow_point = np.argmax(gradients) + 1

    plt.axvline(x=elbow_point, color='red', linestyle='--',
                label=f'–¢–æ—á–∫–∞ –∏–∑–≥–∏–±–∞ (eps ‚âà {distances[elbow_point]:.3f})')
    plt.legend()
    plt.show()

    recommended_eps = distances[elbow_point]
    print(f"üéØ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ô EPS: {recommended_eps:.3f}")

    return recommended_eps


def simple_dbscan_cluster(texts, eps=0.3, min_samples=3):
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è DBSCAN —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π
    """
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä–∞
    interpreter = SimpleClusterInterpreter(texts)
    X = interpreter.fit_vectorizer()

    # DBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    dbscan = DBSCAN(
        eps=eps,
        min_samples=min_samples,
        metric='cosine'
    )
    labels = dbscan.fit_predict(X)

    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    unique_labels = np.unique(labels)
    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
    n_noise = np.sum(labels == -1)

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"üìä DBSCAN –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø")
    print("=" * 50)
    print(f"‚öôÔ∏è  –ü–ê–†–ê–ú–ï–¢–†–´: eps={eps}, min_samples={min_samples}")
    print(f"üéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}")
    print(f"   ‚Ä¢ –®—É–º–æ–≤—ã—Ö —Ç–æ—á–µ–∫: {n_noise} ({n_noise / len(texts) * 100:.1f}%)")
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(texts)}")

    if n_clusters == 0:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å eps –∏–ª–∏ min_samples")
        return labels

    # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    print(f"\n{'=' * 50}")
    interpreter.print_cluster_info(labels)

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    print(f"\n{'=' * 50}")
    print("üìà –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í")
    interpreter.plot_keywords_barchart(labels)

    # UMAP –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    print(f"\n{'=' * 50}")
    print("üé® –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ö–õ–ê–°–¢–ï–†–û–í")
    interpreter.visualize_umap(labels, f"DBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (eps={eps}, min_samples={min_samples})")

    return labels


def quick_dbscan_analysis(texts, eps_values=[0.2, 0.3, 0.4], min_samples_values=[2, 3]):
    """
    –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ DBSCAN —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    """
    interpreter = SimpleClusterInterpreter(texts)
    X = interpreter.fit_vectorizer()

    print("üöÄ –ë–´–°–¢–†–´–ô –ê–ù–ê–õ–ò–ó DBSCAN –° –†–ê–ó–ù–´–ú–ò –ü–ê–†–ê–ú–ï–¢–†–ê–ú–ò")
    print("=" * 60)
    print("eps\tmin_sam\tClusters\tNoise\t% —à—É–º–∞")
    print("-" * 50)

    results = []

    for eps in eps_values:
        for min_samples in min_samples_values:
            dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
            labels = dbscan.fit_predict(X)

            unique_labels = np.unique(labels)
            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
            n_noise = np.sum(labels == -1)
            noise_percentage = n_noise / len(texts) * 100

            print(f"{eps}\t{min_samples}\t{n_clusters}\t\t{n_noise}\t{noise_percentage:.1f}%")

            results.append({
                'eps': eps,
                'min_samples': min_samples,
                'labels': labels,
                'n_clusters': n_clusters,
                'n_noise': n_noise
            })

    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é (–º–∏–Ω–∏–º—É–º —à—É–º–∞, —Ö–æ—Ç—è –±—ã 2 –∫–ª–∞—Å—Ç–µ—Ä–∞)
    valid_results = [r for r in results if r['n_clusters'] >= 2]
    if valid_results:
        best_result = min(valid_results, key=lambda x: x['n_noise'])
        print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ï –ü–ê–†–ê–ú–ï–¢–†–´:")
        print(f"   eps={best_result['eps']}, min_samples={best_result['min_samples']}")
        print(
            f"   –ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {best_result['n_clusters']}, –®—É–º: {best_result['n_noise']} ({best_result['n_noise'] / len(texts) * 100:.1f}%)")

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        print(f"\n{'=' * 50}")
        print("üîç –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –õ–£–ß–®–ò–• –ö–õ–ê–°–¢–ï–†–û–í")
        interpreter.print_cluster_info(best_result['labels'])

        return best_result['labels']

    return None


if __name__ == "__main__":
    texts = get_texts()
    true_labels = get_labels()

    print("üöÄ DBSCAN –î–õ–Ø –¢–ï–ö–°–¢–û–í –° –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–ï–ô")
    print("=" * 60)

    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä eps
    print("üéØ –í–ê–†–ò–ê–ù–¢ 1: –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ü–û–î–ë–û–† EPS")
    recommended_eps = find_optimal_eps(texts, k=5)

    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–º eps
    print(f"\nüéØ –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø –° EPS={recommended_eps:.3f}")
    labels1 = simple_dbscan_cluster(texts, eps=recommended_eps, min_samples=3)

    # –í–∞—Ä–∏–∞–Ω—Ç 2: –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    print(f"\n{'=' * 60}")
    print("üéØ –í–ê–†–ò–ê–ù–¢ 2: –ë–´–°–¢–†–´–ô –ê–ù–ê–õ–ò–ó –° –†–ê–ó–ù–´–ú–ò –ü–ê–†–ê–ú–ï–¢–†–ê–ú–ò")
    labels2 = quick_dbscan_analysis(texts, eps_values=[0.2, 0.3, 0.4, 0.5], min_samples_values=[2, 3])