from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import (silhouette_score, calinski_harabasz_score,
                             davies_bouldin_score, adjusted_rand_score,
                             normalized_mutual_info_score, v_measure_score,
                             homogeneity_score, completeness_score)
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from collections import Counter
import umap


class SimpleClusterInterpreter:
    """
    –ü—Ä–æ—Å—Ç–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è TF-IDF
    """

    def __init__(self, texts, vectorizer=None):
        self.texts = texts
        self.vectorizer = vectorizer if vectorizer else TfidfVectorizer(max_features=500)
        self.X = None
        self.feature_names = None

    def fit_vectorizer(self):
        """–û–±—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞"""
        self.X = self.vectorizer.fit_transform(self.texts)
        self.feature_names = self.vectorizer.get_feature_names_out()
        return self.X

    def get_cluster_keywords(self, labels, n_words=10):
        """
        –¢–æ–ø-N –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø–æ TF-IDF
        """
        if self.X is None:
            self.fit_vectorizer()

        unique_labels = np.unique(labels)
        cluster_keywords = {}

        for cluster_id in unique_labels:
            # –ò–Ω–¥–µ–∫—Å—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
            cluster_indices = np.where(labels == cluster_id)[0]

            if len(cluster_indices) == 0:
                cluster_keywords[cluster_id] = []
                continue

            # –°—Ä–µ–¥–Ω–∏–µ TF-IDF –≤–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞
            cluster_tfidf = self.X[cluster_indices].mean(axis=0)
            cluster_tfidf = np.array(cluster_tfidf).flatten()

            # –¢–æ–ø-N —Å–ª–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ –≤–µ—Å–∞–º–∏
            top_indices = np.argsort(cluster_tfidf)[::-1][:n_words]
            top_words = [(self.feature_names[i], cluster_tfidf[i])
                         for i in top_indices if cluster_tfidf[i] > 0]

            cluster_keywords[cluster_id] = top_words

        return cluster_keywords

    def get_most_frequent_words(self, labels, n_words=10):
        """
        –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
        """
        unique_labels = np.unique(labels)
        cluster_freq_words = {}

        for cluster_id in unique_labels:
            cluster_indices = np.where(labels == cluster_id)[0]
            cluster_texts = [self.texts[i] for i in cluster_indices]

            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞
            all_words = []
            for text in cluster_texts:
                words = text.lower().split()
                words = [word for word in words if len(word) > 2]  # —É–±–∏—Ä–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                all_words.extend(words)

            # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
            word_counts = Counter(all_words)
            cluster_freq_words[cluster_id] = word_counts.most_common(n_words)

        return cluster_freq_words

    def print_cluster_info(self, labels, n_words=8):
        """
        –ö—Ä–∞—Å–∏–≤–∞—è –ø–µ—á–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
        """
        unique_labels, counts = np.unique(labels, return_counts=True)

        print("üìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ö–õ–ê–°–¢–ï–†–ê–•:")
        print("=" * 60)

        # –¢–æ–ø —Å–ª–æ–≤–∞ –ø–æ TF-IDF
        tfidf_keywords = self.get_cluster_keywords(labels, n_words)

        # –ß–∞—Å—Ç–æ—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
        freq_words = self.get_most_frequent_words(labels, n_words)

        for cluster_id in unique_labels:
            if cluster_id == -1:  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —à—É–º
                continue

            count = counts[unique_labels == cluster_id][0]
            percentage = (count / len(labels)) * 100

            print(f"\nüî∏ –ö–õ–ê–°–¢–ï–† {cluster_id} ({count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {percentage:.1f}%):")

            # TF-IDF –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if cluster_id in tfidf_keywords:
                tfidf_words = [word for word, weight in tfidf_keywords[cluster_id][:5]]
                print(f"   üìà –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (TF-IDF): {', '.join(tfidf_words)}")

            # –ß–∞—Å—Ç–æ—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
            if cluster_id in freq_words:
                freq_words_list = [word for word, count in freq_words[cluster_id][:5]]
                print(f"   üìä –ß–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞: {', '.join(freq_words_list)}")

            # –ü—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            cluster_indices = np.where(labels == cluster_id)[0]
            if len(cluster_indices) > 0:
                sample_text = self.texts[cluster_indices[0]]
                preview = sample_text[:100] + "..." if len(sample_text) > 100 else sample_text
                print(f"   üìÑ –ü—Ä–∏–º–µ—Ä: {preview}")

    def plot_keywords_barchart(self, labels, n_words=6):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        """
        cluster_keywords = self.get_cluster_keywords(labels, n_words)
        n_clusters = len(cluster_keywords)

        fig, axes = plt.subplots(1, n_clusters, figsize=(4 * n_clusters, 5))
        if n_clusters == 1:
            axes = [axes]

        for idx, (cluster_id, words_weights) in enumerate(cluster_keywords.items()):
            if not words_weights:
                continue

            words, weights = zip(*words_weights)

            axes[idx].barh(range(len(words)), weights, color=f'C{idx}', alpha=0.7)
            axes[idx].set_yticks(range(len(words)))
            axes[idx].set_yticklabels(words, fontsize=9)
            axes[idx].set_title(f'–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}')
            axes[idx].set_xlabel('TF-IDF –≤–µ—Å')

        plt.suptitle('–ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê –ö–õ–ê–°–¢–ï–†–û–í', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.show()

    def visualize_umap(self, labels, title="–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (UMAP)"):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ 2D —Å –ø–æ–º–æ—â—å—é UMAP
        """

        if self.X is None:
            self.fit_vectorizer()

        print("üîÑ –°—Ç—Ä–æ–∏–º UMAP –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é...")

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–ª–æ—Ç–Ω—ã–π –º–∞—Å—Å–∏–≤ –¥–ª—è UMAP
        X_dense = self.X.toarray()

        # –°–æ–∑–¥–∞–µ–º UMAP —Ä–µ–¥—É–∫—Ç–æ—Ä
        reducer = umap.UMAP(
            n_components=2,
            random_state=42,
            n_neighbors=15,
            min_dist=0.1,
            metric='cosine'
        )

        # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        embedding_2d = reducer.fit_transform(X_dense)

        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
        plt.figure(figsize=(10, 8))

        # –†–∞–∑–Ω—ã–µ —Ü–≤–µ—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        unique_labels = np.unique(labels)
        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))

        for i, cluster_id in enumerate(unique_labels):
            if cluster_id == -1:
                # –®—É–º–æ–≤—ã–µ —Ç–æ—á–∫–∏ —Å–µ—Ä—ã–º —Ü–≤–µ—Ç–æ–º
                mask = labels == cluster_id
                plt.scatter(embedding_2d[mask, 0], embedding_2d[mask, 1],
                            c='gray', alpha=0.5, s=20, label=f'–®—É–º ({cluster_id})')
            else:
                mask = labels == cluster_id
                plt.scatter(embedding_2d[mask, 0], embedding_2d[mask, 1],
                            c=[colors[i]], alpha=0.7, s=30, label=f'–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}')

        plt.title(title, fontsize=14, fontweight='bold')
        plt.xlabel('UMAP dimension 1')
        plt.ylabel('UMAP dimension 2')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

        return embedding_2d

    def visualize_pca(self, labels, title="–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (PCA)"):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ 2D —Å –ø–æ–º–æ—â—å—é PCA (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ UMAP)
        """
        from sklearn.decomposition import PCA

        if self.X is None:
            self.fit_vectorizer()

        print("üîÑ –°—Ç—Ä–æ–∏–º PCA –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é...")

        X_dense = self.X.toarray()

        # –°–æ–∑–¥–∞–µ–º PCA —Ä–µ–¥—É–∫—Ç–æ—Ä
        pca = PCA(n_components=2, random_state=42)
        embedding_2d = pca.fit_transform(X_dense)

        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
        plt.figure(figsize=(10, 8))

        unique_labels = np.unique(labels)
        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))

        for i, cluster_id in enumerate(unique_labels):
            if cluster_id == -1:
                mask = labels == cluster_id
                plt.scatter(embedding_2d[mask, 0], embedding_2d[mask, 1],
                            c='gray', alpha=0.5, s=20, label=f'–®—É–º ({cluster_id})')
            else:
                mask = labels == cluster_id
                plt.scatter(embedding_2d[mask, 0], embedding_2d[mask, 1],
                            c=[colors[i]], alpha=0.7, s=30, label=f'–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}')

        plt.title(title, fontsize=14, fontweight='bold')
        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

        return embedding_2d


# –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π
def compare_cluster_sizes_with_interpretation(texts, true_labels, max_k=6):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π
    """
    interpreter = SimpleClusterInterpreter(texts)
    X = interpreter.fit_vectorizer().toarray()

    print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –†–ê–ó–õ–ò–ß–ù–´–• K (–í–ù–£–¢–†–ï–ù–ù–ò–ï + –í–ù–ï–®–ù–ò–ï –ú–ï–¢–†–ò–ö–ò):")
    print("k\tSilhouette\tCalinski\tDavies-B\tARI\t\tNMI\t\tV-measure")
    print("-" * 85)

    k_values = []
    internal_metrics = {'silhouette': [], 'calinski': [], 'davies': []}
    external_metrics = {'ari': [], 'nmi': [], 'v_measure': []}

    best_k_internal = 2
    best_k_external = 2
    best_silhouette = -1
    best_ari = -1
    best_labels = None

    for k in range(2, max_k + 1):
        kmeans = KMeans(n_clusters=k, random_state=42)
        labels = kmeans.fit_predict(X)

        # –ú–µ—Ç—Ä–∏–∫–∏
        silhouette = silhouette_score(X, labels)
        calinski = calinski_harabasz_score(X, labels)
        davies = davies_bouldin_score(X, labels)
        ari = adjusted_rand_score(true_labels, labels)
        nmi = normalized_mutual_info_score(true_labels, labels)
        v_measure = v_measure_score(true_labels, labels)

        print(f"{k}\t{silhouette:.3f}\t\t{calinski:.3f}\t\t{davies:.3f}\t\t"
              f"{ari:.3f}\t\t{nmi:.3f}\t\t{v_measure:.3f}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        k_values.append(k)
        internal_metrics['silhouette'].append(silhouette)
        internal_metrics['calinski'].append(calinski)
        internal_metrics['davies'].append(davies)
        external_metrics['ari'].append(ari)
        external_metrics['nmi'].append(nmi)
        external_metrics['v_measure'].append(v_measure)

        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–µ–µ k
        if silhouette > best_silhouette:
            best_silhouette = silhouette
            best_k_internal = k
            best_labels = labels

        if ari > best_ari:
            best_ari = ari
            best_k_external = k

    print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    print(f"   –ü–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –º–µ—Ç—Ä–∏–∫–∞–º: k={best_k_internal} (Silhouette: {best_silhouette:.3f})")
    print(f"   –ü–æ –≤–Ω–µ—à–Ω–∏–º –º–µ—Ç—Ä–∏–∫–∞–º: k={best_k_external} (ARI: {best_ari:.3f})")

    # –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –õ–£–ß–®–ò–• –ö–õ–ê–°–¢–ï–†–û–í
    if best_labels is not None:
        print(f"\n{'=' * 60}")
        print(f"üîç –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –ö–õ–ê–°–¢–ï–†–û–í (k={best_k_internal})")
        print(f"{'=' * 60}")

        interpreter.print_cluster_info(best_labels)
        interpreter.plot_keywords_barchart(best_labels)

        # –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø UMAP
        print(f"\n{'=' * 60}")
        print(f"üìä –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ö–õ–ê–°–¢–ï–†–û–í")
        print(f"{'=' * 60}")

        # –ü—Ä–æ–±—É–µ–º UMAP, –µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º PCA
        umap_embedding = interpreter.visualize_umap(
            best_labels,
            title=f"–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ (k={best_k_internal}) - UMAP"
        )

        if umap_embedding is None:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º PCA –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç
            interpreter.visualize_pca(
                best_labels,
                title=f"–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ (k={best_k_internal}) - PCA"
            )

    # –ì—Ä–∞—Ñ–∏–∫–∏ –º–µ—Ç—Ä–∏–∫
    _plot_all_metrics(k_values, internal_metrics, external_metrics,
                      best_k_internal, best_k_external, texts, true_labels)

    return best_k_internal, best_k_external, best_labels


def _plot_all_metrics(k_values, internal_metrics, external_metrics, best_k_int, best_k_ext, texts, true_labels):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫
    """
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))

    # –ì—Ä–∞—Ñ–∏–∫ 1: –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    ax1.plot(k_values, internal_metrics['silhouette'], 'bo-', label='Silhouette', linewidth=2)
    ax1.plot(k_values, _normalize(internal_metrics['calinski']), 'go-', label='Calinski (–Ω–æ—Ä–º.)', linewidth=2)
    ax1.plot(k_values, _normalize([1 / d for d in internal_metrics['davies']]), 'ro-',
             label='1/Davies (–Ω–æ—Ä–º.)', linewidth=2)
    ax1.axvline(x=best_k_int, color='blue', linestyle='--', alpha=0.7,
                label=f'–õ—É—á—à–µ–µ k={best_k_int} (–≤–Ω—É—Ç—Ä.)')
    ax1.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (k)')
    ax1.set_ylabel('–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
    ax1.set_title('–í–ù–£–¢–†–ï–ù–ù–ò–ï –ú–ï–¢–†–ò–ö–ò\n(‚Üë –ª—É—á—à–µ)')
    ax1.grid(True, alpha=0.3)
    ax1.legend()

    # –ì—Ä–∞—Ñ–∏–∫ 2: –í–Ω–µ—à–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    ax2.plot(k_values, external_metrics['ari'], 'bo-', label='ARI', linewidth=2)
    ax2.plot(k_values, external_metrics['nmi'], 'go-', label='NMI', linewidth=2)
    ax2.plot(k_values, external_metrics['v_measure'], 'ro-', label='V-measure', linewidth=2)
    ax2.axvline(x=best_k_ext, color='red', linestyle='--', alpha=0.7,
                label=f'–õ—É—á—à–µ–µ k={best_k_ext} (–≤–Ω–µ—à.)')
    ax2.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (k)')
    ax2.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫')
    ax2.set_title('–í–ù–ï–®–ù–ò–ï –ú–ï–¢–†–ò–ö–ò\n(‚Üë –ª—É—á—à–µ)')
    ax2.grid(True, alpha=0.3)
    ax2.legend()

    # –ì—Ä–∞—Ñ–∏–∫ 3: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∏ –≤–Ω–µ—à–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫
    ax3.plot(k_values, internal_metrics['silhouette'], 'b-', label='Silhouette', linewidth=2)
    ax3.plot(k_values, external_metrics['ari'], 'r-', label='ARI', linewidth=2)
    ax3.axvline(x=best_k_int, color='blue', linestyle='--', alpha=0.5,
                label=f'–õ—É—á—à–µ–µ k (–≤–Ω—É—Ç—Ä.)={best_k_int}')
    ax3.axvline(x=best_k_ext, color='red', linestyle='--', alpha=0.5,
                label=f'–õ—É—á—à–µ–µ k (–≤–Ω–µ—à.)={best_k_ext}')
    ax3.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (k)')
    ax3.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫')
    ax3.set_title('–°–†–ê–í–ù–ï–ù–ò–ï: Silhouette vs ARI')
    ax3.grid(True, alpha=0.3)
    ax3.legend()

    # –ì—Ä–∞—Ñ–∏–∫ 4: Homogeneity vs Completeness
    homogeneity_scores = []
    completeness_scores = []

    vectorizer = TfidfVectorizer(max_features=500)
    X = vectorizer.fit_transform(texts).toarray()

    for k in k_values:
        kmeans = KMeans(n_clusters=k, random_state=42)
        labels = kmeans.fit_predict(X)
        homogeneity_scores.append(homogeneity_score(true_labels, labels))
        completeness_scores.append(completeness_score(true_labels, labels))

    ax4.plot(k_values, homogeneity_scores, 'b-', label='Homogeneity', linewidth=2)
    ax4.plot(k_values, completeness_scores, 'g-', label='Completeness', linewidth=2)
    ax4.plot(k_values, external_metrics['v_measure'], 'r-', label='V-measure', linewidth=2)
    ax4.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (k)')
    ax4.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫')
    ax4.set_title('HOMOGENEITY, COMPLETENESS, V-MEASURE')
    ax4.grid(True, alpha=0.3)
    ax4.legend()

    plt.tight_layout()
    plt.show()


def _normalize(values):
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É [0, 1]"""
    min_val = min(values)
    max_val = max(values)
    if max_val == min_val:
        return [0.5] * len(values)
    return [(v - min_val) / (max_val - min_val) for v in values]


# –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π
def quick_interpret_clusters(texts, labels, n_words=8):
    """
    –ë—ã—Å—Ç—Ä–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π
    """
    interpreter = SimpleClusterInterpreter(texts)
    interpreter.fit_vectorizer()

    print("üìä –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –ö–õ–ê–°–¢–ï–†–û–í")
    print("=" * 50)
    interpreter.print_cluster_info(labels, n_words)
    interpreter.plot_keywords_barchart(labels, n_words)

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    print("\nüìà –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø")
    print("=" * 50)
    interpreter.visualize_umap(labels)
    interpreter.visualize_pca(labels)


if __name__ == "__main__":
    from util.decribe import get_texts, get_labels

    texts = get_texts()
    true_labels = get_labels()

    print("üöÄ –°–†–ê–í–ù–ï–ù–ò–ï –ö–õ–ê–°–¢–ï–†–û–í –° –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–ï–ô –ò –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ï–ô")
    print("=" * 70)

    best_k_int, best_k_ext, best_labels = compare_cluster_sizes_with_interpretation(
        texts, true_labels, max_k=5
    )